Directory structure:
└── maitreyam-clap-agents/
    ├── README.md
    ├── changelog.md
    ├── LICENSE
    ├── pyproject.toml
    ├── simple2_mcp.py
    ├── simple_mcp.py
    ├── examples/
    │   ├── all_func.py
    │   ├── CLAP-TEST-SUITE.py
    │   ├── document_ingestion.py
    │   ├── email_test.py
    │   ├── google_agent.py
    │   ├── google_mcp.py
    │   ├── google_team.py
    │   ├── huge_rag.py
    │   ├── local_rag.py
    │   ├── mcp_team_agent.py
    │   ├── mcp_test_agent.py
    │   ├── mcp_test_suite.py
    │   ├── ollama_test.py
    │   ├── qdrant_ingestion.py
    │   ├── rag_all_functionalities.py
    │   ├── rag_test.py
    │   ├── react_test.py
    │   ├── requirements.txt
    │   ├── scraping_test.py
    │   ├── simple_react_agent.py
    │   ├── team_test.py
    │   ├── test_agent.py
    │   └── tool_agent_rag.py
    └── src/
        └── clap/
            ├── __init__.py
            ├── embedding/
            │   ├── __init__.py
            │   ├── base_embedding.py
            │   ├── fastembed_embedding.py
            │   ├── ollama_embedding.py
            │   └── sentence_transformer_embedding.py
            ├── llm_services/
            │   ├── __init__.py
            │   ├── base.py
            │   ├── google_openai_compat_service.py
            │   ├── groq_service.py
            │   └── ollama_service.py
            ├── mcp_client/
            │   ├── __init__.py
            │   └── client.py
            ├── multiagent_pattern/
            │   ├── __init__.py
            │   ├── agent.py
            │   └── team.py
            ├── react_pattern/
            │   ├── __init__.py
            │   └── react_agent.py
            ├── tool_pattern/
            │   ├── __init__.py
            │   ├── tool.py
            │   └── tool_agent.py
            ├── tools/
            │   ├── __init__.py
            │   ├── email_tools.py
            │   ├── web_crawler.py
            │   └── web_search.py
            ├── utils/
            │   ├── __init__.py
            │   ├── completions.py
            │   ├── extraction.py
            │   ├── logging.py
            │   └── rag_utils.py
            └── vector_stores/
                ├── __init__.py
                ├── base.py
                ├── chroma_store.py
                └── qdrant_store.py

================================================
FILE: README.md
================================================
<p align="center">
  <img src="GITCLAP.png" alt="CLAP Logo" width="700" height="200"/>
</p>

# CLAP - Cognitive Layer Agent Package

[![PyPI version](https://img.shields.io/pypi/v/clap-agents.svg)](https://pypi.org/project/clap-agents/) 
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python Version](https://img.shields.io/pypi/pyversions/clap-agents.svg)](https://pypi.org/project/clap-agents/) 

**CLAP (Cognitive Layer Agent Package)** is a Python framework providing building blocks for creating sophisticated AI agents based on modern agentic patterns. It enables developers to easily construct agents capable of reasoning, planning, and interacting with external tools, systems, and knowledge bases.

Built with an asynchronous core (`asyncio`), CLAP offers flexibility and performance for complex agentic workflows.

<p align="center">
  <img src="PIP CLAP.png" alt="CLAP Pip Install" width="700" height="200"/> <!-- Updated alt text -->
</p>

## Key Features

*   **Modular Agent Patterns:**
    *   **ReAct Agent:** Implements the Reason-Act loop with robust thought-prompting and native tool calling. Ideal for complex reasoning and RAG.
    *   **Tool Agent:** A straightforward agent for single-step tool usage, including simple RAG.
    *   **Multi-Agent Teams:** Define teams of specialized agents with dependencies, enabling collaborative task execution (sequential or parallel).
*   **Advanced Tool Integration:**
    *   **Native LLM Tool Calling:** Leverages modern LLM APIs for reliable tool execution.
    *   **Local Tools:** Easily define and use local Python functions (both synchronous and asynchronous) as tools using the `@tool` decorator.
    *   **Remote Tools (MCP):** Integrates with [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) servers via the included `MCPClientManager`, allowing agents to discover and use tools exposed by external systems (currently supports SSE transport).
    *   **Robust Validation & Coercion:** Uses `jsonschema` for strict validation of tool arguments and attempts type coercion for common LLM outputs (e.g., string numbers to integers).
*   **Retrieval Augmented Generation (RAG) Capabilities:**
    *   **`VectorStoreInterface`:** An abstraction for interacting with various vector databases.
    *   **Supported Vector Stores:**
        *   **ChromaDB:** (`ChromaStore`) For local or self-hosted vector storage.
        *   **Qdrant:** (`QdrantStore`) For local (in-memory or file-based) vector storage.
    *   **`EmbeddingFunctionInterface`:** A protocol for consistent interaction with different embedding models.
    *   **Supported Embedding Function Wrappers:**
        *   `SentenceTransformerEmbeddings`: Uses models from the `sentence-transformers` library.
        *   `OllamaEmbeddings`: Generates embeddings using models running locally via Ollama.
        *   `FastEmbedEmbeddings`: Utilizes the `fastembed` library for CPU-optimized embeddings. (Note: Performance for very large batch ingestions via the async wrapper might vary based on CPU and may be slower than SentenceTransformers for initial bulk loads.)
    *   **RAG-Aware Agents:** Both `Agent` (via `ReactAgent`) and `ToolAgent` can be equipped with a `vector_store` to perform `vector_query` tool calls, enabling them to retrieve context before responding.
    *   **Utilities:** Includes basic PDF and CSV text loaders and chunking strategies in `clap.utils.rag_utils`.
*   **Pluggable LLM Backends:**
    *   Uses a **Strategy Pattern** (`LLMServiceInterface`) to abstract LLM interactions.
    *   Includes ready-to-use service implementations for:
        *   **Groq:** (`GroqService`)
        *   **Google Generative AI (Gemini):** (`GoogleOpenAICompatService` via OpenAI compatibility layer)
        *   **Ollama (Local LLMs):** (`OllamaOpenAICompatService` also known as `OllamaService` via OpenAI compatibility layer, allowing use of locally run models like Llama 3, Mistral, etc.)
    *   Easily extensible to support other LLM providers.
*   **Asynchronous Core:** Built entirely on `asyncio` for efficient I/O operations and potential concurrency.
*   **Structured Context Passing:** Enables clear and organized information flow between agents in a team.
*   **Built-in Tools:** Includes helpers for web search (`duckduckgo_search`). More available via optional dependencies.

## Installation

Ensure you have Python 3.10 or later installed.

```bash
pip install clap-agents
```

Ensure you have Python 3.10 or later installed.

```bash
pip install clap-agents


To use specific features, you might need to install optional dependencies:
# For Qdrant support (includes fastembed)
pip install "clap-agents[qdrant]"

# For ChromaDB support
pip install "clap-agents[chromadb]"

# For Ollama (LLM and/or Embeddings)
pip install "clap-agents[ollama]"

# For other tools like web crawling or visualization
pip install "clap-agents[standard_tools,viz]"

# To install all major optional dependencies
pip install "clap-agents[all]"
```


Check the pyproject.toml for the full list of [project.optional-dependencies]. You will also need to have external services like Ollama or Qdrant (if used locally) running.
Depending on the tools or LLM backends you intend to use, you might need additional dependencies listed in the pyproject.toml (e.g., groq, openai, mcp, jsonschema, requests, duckduckgo-search, graphviz). Check the [project.dependencies] and [project.optional-dependencies] sections.


## Quick Start: Simple Tool calling Agent with a Local Tool
This example demonstrates creating a Tool calling agent using the Groq backend and a local tool

```
from dotenv import load_dotenv
from clap import ToolAgent
from clap import duckduckgo_search

load_dotenv()

async def main():
    agent = ToolAgent(tools=duckduckgo_search, model="meta-llama/llama-4-scout-17b-16e-instruct")
    user_query = "Search the web for recent news about AI advancements."
    response = await agent.run(user_msg=user_query)
    print(f"Response:\n{response}")

asyncio.run(main())
```


## Quick Start: Simple ReAct Agent with a Local Tool
This example demonstrates creating a ReAct agent using the Groq backend and a local tool.

```
import asyncio
import os
from dotenv import load_dotenv
from clap import ReactAgent, tool, GroqService

load_dotenv() 
@tool
def get_word_length(word: str) -> int:
    """Calculates the length of a word."""
    print(f"[Local Tool] Calculating length of: {word}")
    return len(word)

async def main():
    groq_service = GroqService() # Your service of choice (either groq or Google)
    agent = ReactAgent(
        llm_service=groq_service,
        model="llama-3.3-70b-versatile", # Or another Groq model
        tools=[get_word_length], # Provide the local tool
        # system_prompt="You are a helpful assistant." # Optional base prompt
    )

    user_query = "How many letters are in the word 'framework'?"
    response = await agent.run(user_msg=user_query)
    
    print(response)
    
asyncio.run(main())
```

## Quick Start: Simple Tool-Calling Agent with Ollama
This example demonstrates a ToolAgent using a local Ollama model and a local tool.
Ensure Ollama is running and you have pulled the model (e.g., ollama pull llama3).

```
import asyncio
from dotenv import load_dotenv
from clap import ToolAgent, tool, OllamaService # Assuming OllamaService is your OllamaOpenAICompatService

load_dotenv()

@tool
def get_capital(country: str) -> str:
    """Returns the capital of a country."""
    if country.lower() == "france": return "Paris"
    return f"I don't know the capital of {country}."

async def main():
    # Initialize the Ollama service
    ollama_llm_service = OllamaService(default_model="llama3") # Specify your Ollama model

    agent = ToolAgent(
        llm_service=ollama_llm_service,
        model="llama3", # Model name for this agent
        tools=[get_capital]
    )
    user_query = "What is the capital of France?"
    response = await agent.run(user_msg=user_query)
    print(f"Query: {user_query}\nResponse:\n{response}")

    await ollama_llm_service.close() # Important for OllamaService

if __name__ == "__main__":
    asyncio.run(main())

```

## Quick Start: RAG Agent with Qdrant and Ollama Embeddings
This example shows an Agent performing RAG using Ollama for embeddings and Qdrant as the vector store.
Ensure Ollama is running (with nomic-embed-text and llama3 pulled) and Qdrant is running (e.g., via Docker).
```
import asyncio
import os
import shutil
from dotenv import load_dotenv
from clap import Agent, QdrantStore, OllamaEmbeddings, OllamaService
from clap.utils.rag_utils import chunk_text_by_fixed_size
from qdrant_client import models as qdrant_models # If needed for distance

load_dotenv()

OLLAMA_HOST = "http://localhost:11434"
EMBED_MODEL = "nomic-embed-text"
LLM_MODEL = "llama3"
DB_PATH = "./temp_rag_db_ollama_qdrant"
COLLECTION = "my_rag_docs"

async def main():
    if os.path.exists(DB_PATH): shutil.rmtree(DB_PATH)

    ollama_ef = OllamaEmbeddings(model_name=EMBED_MODEL, ollama_host=OLLAMA_HOST)
    vector_store = await QdrantStore.create(
        collection_name=COLLECTION,
        embedding_function=ollama_ef,
        path=DB_PATH, # For local file-based Qdrant
        recreate_collection_if_exists=True
    )

    sample_texts = ["The sky is blue due to Rayleigh scattering.", "Large language models are powerful."]
    chunks = [chunk for text in sample_texts for chunk in chunk_text_by_fixed_size(text, 100, 10)]
    ids = [str(i) for i in range(len(chunks))] # Qdrant needs UUIDs; QdrantStore handles this
    
    if chunks:
        await vector_store.add_documents(documents=chunks, ids=ids)
    print(f"Ingested {len(chunks)} chunks.")

    ollama_llm_service = OllamaService(default_model=LLM_MODEL, base_url=f"{OLLAMA_HOST}/v1")
    rag_agent = Agent(
        name="RAGMaster",
        backstory="I answer questions using provided documents.",
        task_description="Why is the sky blue according to the documents?", # This becomes the User Query
        llm_service=ollama_llm_service,
        model=LLM_MODEL,
        vector_store=vector_store
    )

    response = await rag_agent.run()
    print(f"Query: {rag_agent.task_description}\nResponse:\n{response.get('output')}")

    await vector_store.close()
    await ollama_llm_service.close()
    if os.path.exists(DB_PATH): shutil.rmtree(DB_PATH)

asyncio.run(main())
```

## Exploring Further


# Multi-Agent Teams: See examples/test_clap_comprehensive_suite.py and other team examples for setting up sequential or parallel agent workflows.

# MCP Integration: Check examples/test_clap_comprehensive_suite.py (ensure corresponding MCP servers from examples/simple_mcp.py etc. are running).

# Other LLM Services (Groq, Google Gemini , Ollama): Modify the Quick Starts to use GroqService or GoogleOpenAICompatService (ensure API keys are set).

# Different Vector Stores & Embedding Functions: Experiment with ChromaStore, QdrantStore, SentenceTransformerEmbeddings, FastEmbedEmbeddings, and OllamaEmbeddings as shown in the comprehensive test suite.

License
This project is licensed under the terms of the Apache License 2.0. See the LICENSE file for details.






================================================
FILE: changelog.md
================================================
# CHANGELOG.md

## [0.2.0] - 2025-05-16
### Added
- RAG (Retrieval Augmented Generation) capabilities for `Agent` and `ToolAgent`.
- `VectorStoreInterface` for abstracting vector database interactions.
- `ChromaStore` implementation for ChromaDB.
- `QdrantStore` implementation for Qdrant (local mode, supporting custom EFs and fastembed wrapper).
- `EmbeddingFunctionInterface` and concrete implementations:
    - `SentenceTransformerEmbeddings`
    - `OllamaEmbeddings`
    - `FastEmbedEmbeddings`
- `Ollama Compatibility` Now run Ollama models locally (both embedding and chat).
- PDF and CSV loading utilities in `rag_utils`.
- Argument type coercion in `Tool.run()` for robustness.

### Changed
- `ToolAgent` and `Agent` now require `llm_service` and `model` to be explicitly passed during initialization.
- Refined prompts for `ReactAgent` for better tool usage and RAG.
- Improved error handling and logging in various components.

### Fixed
- Resolved various import errors and `NameError` issues in examples and core files.
- Fixed argument type mismatches for LLM tool calls.

## [0.1.1] - 2025-04-19
- Initial release with core agent patterns.


================================================
FILE: LICENSE
================================================

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


================================================
FILE: pyproject.toml
================================================


[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "clap-agents"
version = "0.2.2"
description = "A Python framework for building cognitive agentic patterns including ReAct agents, Multi-Agent Teams, native tool calling, and MCP client integration."
readme = "README.md"
requires-python = ">=3.10"
license = { file = "LICENSE" }
authors = [
    { name = "Maitreya Mishra", email = "maitreyamishra04@gmail.com" },
    
]
keywords = [
    "ai",
    "agent",
    "llm",
    "agentic",
    "react",
    "multi-agent",
    "framework",
    "tool calling",
    "mcp",
    "agentic systems",
]
# Standard PyPI classifiers
classifiers = [
    "Development Status :: 3 - Alpha", # Or Beta, Production/Stable
    "Intended Audience :: Developers",
    "License :: OSI Approved :: Apache Software License", 
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries :: Application Frameworks",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]


dependencies = [
    "anyio>=4.5",
    "groq",
    "openai>=1.0.0",
    "httpx>=0.27",
    "httpx-sse>=0.4",
    "mcp>=1.2.0",  
    "jsonschema",
    "colorama",
    "requests",
    "duckduckgo-search",
    "pydantic>=2.7.2,<3.0.0",
]

[project.optional-dependencies]
sentence-transformers = ["sentence-transformers"] 
ollama = ["ollama>=0.2.0"] 
fastembed = ["fastembed>=0.2.0"] 
                               

chromadb = ["chromadb>=0.5.0"] 
qdrant = ["qdrant-client[fastembed]>=1.7.0"] 

pdf = ["pypdf"] 
pandas = ["pandas"] 

viz = ["graphviz"] 
standard_tools = ["crawl4ai"] 

rag = [
    "clap-agents[sentence-transformers]", 
    "clap-agents[ollama]",
    "clap-agents[fastembed]", 
    "clap-agents[chromadb]",
    "clap-agents[qdrant]",
    "clap-agents[pdf]",
]


all = [
    "clap-agents[rag]",
    "clap-agents[viz]",
    "clap-agents[standard_tools]",
    "clap-agents[pandas]", 
]

[project.urls]
Homepage = "https://github.com/MaitreyaM/CLAP-AGENTS.git" 
Repository = "https://github.com/MaitreyaM/CLAP-AGENTS.git"
Issues = "https://github.com/MaitreyaM/CLAP-AGENTS.git/issues"


[tool.hatch.build.targets.wheel]
packages = ["src/clap"]




================================================
FILE: simple2_mcp.py
================================================


from mcp.server.fastmcp import FastMCP

mcp = FastMCP()
mcp.settings.port=8001

@mcp.tool()
def sub(a: int, b: int) -> int:
    """Subtracts two integers."""
    print(f"[MCP Server] Received add request: {a} + {b}")
    result = a - b
    print(f"[MCP Server] Returning result: {result}")
    return result

if __name__ == "__main__":
    print("Starting minimal MCP server on http://localhost:8001/sse")
    
    mcp.run(transport='sse')




================================================
FILE: simple_mcp.py
================================================


from mcp.server.fastmcp import FastMCP

mcp = FastMCP()

@mcp.tool()
def add(a: int, b: int) -> int:
    """Adds two integers."""
    print(f"[MCP Server] Received add request: {a} + {b}")
    result = a + b
    print(f"[MCP Server] Returning result: {result}")
    return result

if __name__ == "__main__":
    print("Starting minimal MCP server on http://localhost:8000/sse")
    
    mcp.run(transport='sse')




================================================
FILE: examples/all_func.py
================================================
# --- START OF FILE examples/test_ollama_suite.py ---
import asyncio
import os
import shutil
import time
import json
from dotenv import load_dotenv
from typing import Any, Dict, List, Optional
import uuid 

# --- CLAP & Dependencies ---
from clap import Agent, Team, ToolAgent # Core CLAP
from clap.tool_pattern.tool import tool # For defining local tools
from clap.llm_services.ollama_service import OllamaOpenAICompatService
from clap.embedding.ollama_embedding import OllamaEmbeddings, KNOWN_OLLAMA_EMBEDDING_DIMENSIONS
from clap.vector_stores.qdrant_store import QdrantStore # Using Qdrant for RAG
from clap.utils.rag_utils import chunk_text_by_fixed_size # Simple chunker
from qdrant_client import models as qdrant_models # For Qdrant config

load_dotenv()

# --- Configuration ---
OLLAMA_HOST = "http://localhost:11434"
OLLAMA_LLM_FOR_AGENT = "llama3.2:latest"  # Model for agent reasoning (ensure pulled)
OLLAMA_MODEL_FOR_EMBEDDINGS = "nomic-embed-text" # Model for embeddings (ensure pulled)

RAG_DB_PATH = "./ollama_suite_qdrant_db"
RAG_COLLECTION_NAME = "ollama_suite_rag_collection"
RAG_CHUNK_SIZE, RAG_CHUNK_OVERLAP = 200, 20

SAMPLE_DOCS_FOR_RAG = [
    "Ollama allows running large language models locally, such as Llama 3, Mistral, and Gemma.",
    "Embedding models like nomic-embed-text can be used with Ollama to generate vector representations of text.",
    "The CLAP framework supports integration with local Ollama instances for both LLM and embedding tasks."
]

# --- Helper Functions ---
def print_section_header(title: str):
    print(f"\n\n{'='*20} {title.upper()} {'='*20}")

def print_test_case(name: str, query: str, response: Any):
    print(f"\n--- Test Case: {name} ---")
    print(f"Query/Task: {query}")
    final_output = response.get("output") if isinstance(response, dict) else response
    print(f"Response:\n{final_output}")
    print("--------------------")

# --- Local Tools for Testing ---
@tool
def get_city_population(city_name: str) -> str:
    """Returns a fictional population for a given city."""
    populations = {"paris": "2.1 million", "tokyo": "13.9 million", "berlin": "3.6 million"}
    return populations.get(city_name.lower(), f"Population data for {city_name} unknown.")

@tool
def square_number(number: float) -> float:
    """Calculates the square of a number."""
    return number * number

# --- Test Functions ---

async def test_tool_agent_direct_answer(ollama_service: OllamaOpenAICompatService):
    print_section_header("ToolAgent - Direct Answer (No Tools Used)")
    agent = ToolAgent(
        llm_service=ollama_service,
        model=OLLAMA_LLM_FOR_AGENT,
        tools=[] # No tools provided
    )
    query = "What is the main purpose of the Ollama software?"
    response = await agent.run(user_msg=query)
    print_test_case("ToolAgent Direct Answer", query, response)

async def test_tool_agent_local_tool(ollama_service: OllamaOpenAICompatService):
    print_section_header("ToolAgent - Local Tool Usage")
    agent = ToolAgent(
        llm_service=ollama_service,
        model=OLLAMA_LLM_FOR_AGENT,
        tools=[get_city_population]
    )
    query = "What is the population of Paris?"
    response = await agent.run(user_msg=query)
    print_test_case("ToolAgent Local Tool (Paris)", query, response)

    query_unknown = "What is the population of Atlantis?"
    response_unknown = await agent.run(user_msg=query_unknown)
    print_test_case("ToolAgent Local Tool (Atlantis)", query_unknown, response_unknown)


async def setup_rag_store_for_ollama() -> Optional[QdrantStore]:
    print_section_header("Setting up RAG Vector Store with Ollama Embeddings")
    if OLLAMA_MODEL_FOR_EMBEDDINGS not in KNOWN_OLLAMA_EMBEDDING_DIMENSIONS:
        print(f"ERROR: Dimension for Ollama embed model '{OLLAMA_MODEL_FOR_EMBEDDINGS}' unknown.")
        return None
    try:
        ollama_ef = OllamaEmbeddings(model_name=OLLAMA_MODEL_FOR_EMBEDDINGS, ollama_host=OLLAMA_HOST)
    except Exception as e:
        print(f"Failed to init OllamaEmbeddings: {e}"); return None

    if os.path.exists(RAG_DB_PATH): shutil.rmtree(RAG_DB_PATH)
    try:
        vector_store = await QdrantStore.create(
            collection_name=RAG_COLLECTION_NAME, embedding_function=ollama_ef,
            path=RAG_DB_PATH, recreate_collection_if_exists=True,
            distance_metric=qdrant_models.Distance.COSINE
        )
    except Exception as e:
        print(f"Failed to create QdrantStore: {e}"); return None

    chunks = []
    for doc in SAMPLE_DOCS_FOR_RAG:
        chunks.extend(chunk_text_by_fixed_size(doc, RAG_CHUNK_SIZE, RAG_CHUNK_OVERLAP))
    ids = [str(uuid.uuid4()) for _ in range(len(chunks))]
    metadatas = [{"source": "clap_ollama_test_data"} for _ in range(len(chunks))]

    if chunks: await vector_store.add_documents(documents=chunks, ids=ids, metadatas=metadatas)
    print(f"Ingested {len(chunks)} chunks into '{RAG_COLLECTION_NAME}'.")
    return vector_store


async def test_agent_rag_only(ollama_service: OllamaOpenAICompatService, vector_store: QdrantStore):
    print_section_header("Agent (ReAct) - RAG Only")
    agent = Agent(
        name="OllamaRAGer", backstory="You answer questions based on provided Ollama documentation.",
        task_description="What models can Ollama run locally?", # Will be set by query
        llm_service=ollama_service, model=OLLAMA_LLM_FOR_AGENT, vector_store=vector_store
    )
    agent.task_description = "What models can Ollama run locally?" # The actual query
    response = await agent.run()
    print_test_case("Agent RAG Only", agent.task_description, response)


async def test_agent_rag_with_local_tool(ollama_service: OllamaOpenAICompatService, vector_store: QdrantStore):
    print_section_header("Agent (ReAct) - RAG with Local Tool")
    agent = Agent(
        name="OllamaRAGToolUser",
        backstory="You use info from Ollama docs and can square numbers.",
        task_description="Based on Ollama documentation, what kind of models does it support? Then, what is 5 squared?",
        llm_service=ollama_service, model=OLLAMA_LLM_FOR_AGENT,
        vector_store=vector_store, tools=[square_number]
    )
    # The task_description is the multi-part query for this agent
    response = await agent.run()
    print_test_case("Agent RAG + Local Tool", agent.task_description, response)


async def main():
    overall_start_time = time.time()
    print("Ensure Ollama server is running and models are pulled:")
    print(f"  LLM for Agent: ollama pull {OLLAMA_LLM_FOR_AGENT}")
    print(f"  Model for Embeddings: ollama pull {OLLAMA_MODEL_FOR_EMBEDDINGS}")
    print("-" * 60)

    # Initialize Ollama LLM Service once
    ollama_llm_service = OllamaOpenAICompatService(
        default_model=OLLAMA_LLM_FOR_AGENT,
        base_url=f"{OLLAMA_HOST}/v1"
    )

    # --- Run ToolAgent Tests ---
    await test_tool_agent_direct_answer(ollama_llm_service)
    await test_tool_agent_local_tool(ollama_llm_service)

    # --- Setup RAG and Run Agent (ReAct) Tests ---
    rag_vector_store = await setup_rag_store_for_ollama()
    if rag_vector_store:
        await test_agent_rag_only(ollama_llm_service, rag_vector_store)
        await test_agent_rag_with_local_tool(ollama_llm_service, rag_vector_store)
        await rag_vector_store.close() # Close vector store connection
    else:
        print("Skipping RAG tests due to vector store setup failure.")

    await ollama_llm_service.close() # Close LLM service client

    print(f"\n\nTotal Ollama Test Suite Took: {time.time() - overall_start_time:.2f} seconds.")
    if os.path.exists(RAG_DB_PATH):
        print(f"Cleaning up test database: {RAG_DB_PATH}")
        shutil.rmtree(RAG_DB_PATH)

if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: examples/CLAP-TEST-SUITE.py
================================================
import asyncio
import os
import shutil
import time
import json
import uuid
from dotenv import load_dotenv
from typing import Any, Dict, List, Optional

from clap import (
    Agent, Team, ToolAgent, tool,
    LLMServiceInterface, GroqService, GoogleOpenAICompatService, 
    EmbeddingFunctionInterface, 
    VectorStoreInterface, QueryResult, 
    duckduckgo_search
)


from clap.llm_services.ollama_service import OllamaOpenAICompatService as OllamaService

from clap.embedding.sentence_transformer_embedding import SentenceTransformerEmbeddings
from clap.embedding.ollama_embedding import OllamaEmbeddings, KNOWN_OLLAMA_EMBEDDING_DIMENSIONS
from clap.embedding.fastembed_embedding import FastEmbedEmbeddings, KNOWN_FASTEMBED_DIMENSIONS as FE_KNOWN_DIMS

from clap.vector_stores.chroma_store import ChromaStore
from clap.vector_stores.qdrant_store import QdrantStore


from qdrant_client import models as qdrant_models
QDRANT_CLIENT_INSTALLED = True


try:
    CHROMA_CLIENT_INSTALLED = True 
    import chromadb 
except ImportError:
    CHROMA_CLIENT_INSTALLED = False



from clap import MCPClientManager, SseServerConfig
from pydantic import HttpUrl 

from clap.utils.rag_utils import load_pdf_file, chunk_text_by_fixed_size

load_dotenv()


PDF_PATH = "/Users/maitreyamishra/PROJECTS/Cognitive-Layer/examples/handsonml.pdf"
DB_BASE_PATH = "./clap_suite_dbs"

GROQ_LLM_MODEL = "llama-3.3-70b-versatile"
OLLAMA_LLM_MODEL = "llama3.2:latest" 
GOOGLE_LLM_MODEL = "gemini-2.5-flash-preview-04-17" 


ST_EMBED_MODEL = "all-MiniLM-L6-v2"
OLLAMA_EMBED_MODEL = "nomic-embed-text:latest" 
FASTEMBED_MODEL = "BAAI/bge-small-en-v1.5"



OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434") 


CHUNK_SIZE, CHUNK_OVERLAP = 400, 40


PDF_CONTENT_CACHE: Optional[str] = None
MCP_MANAGER_INSTANCE: Optional[MCPClientManager] = None
MCP_ENABLED = False


def print_section_header(title: str): print(f"\n\n{'='*25} {title.upper()} {'='*25}")
def print_test_case_header(name: str): print(f"\n--- Test Case: {name} ---")
def print_query(query:str): print(f"User Query/Task: {query}")
def print_agent_response(response: Any):
    final_output = response.get("output") if isinstance(response, dict) else response
    print(f"Agent Response:\n{final_output}")
    print("-" * 50)

async def get_pdf_content() -> str:
    global PDF_CONTENT_CACHE
    if PDF_CONTENT_CACHE is None:
        if not os.path.exists(PDF_PATH):
            print(f"ERROR: PDF file not found: '{PDF_PATH}'. RAG tests limited."); PDF_CONTENT_CACHE = ""; return ""
        print(f"Loading PDF '{PDF_PATH}' (once)..."); PDF_CONTENT_CACHE = load_pdf_file(PDF_PATH)
        if not PDF_CONTENT_CACHE: print(f"Failed to load content from '{PDF_PATH}'.")
    return PDF_CONTENT_CACHE

def get_mcp_manager() -> Optional[MCPClientManager]:
    global MCP_MANAGER_INSTANCE
    if MCP_MANAGER_INSTANCE is None:
        try:
            mcp_server_configs = {
                "adder_server": SseServerConfig(url=HttpUrl("http://localhost:8000")),
                "subtract_server": SseServerConfig(url=HttpUrl("http://localhost:8001"))
            }
            MCP_MANAGER_INSTANCE = MCPClientManager(mcp_server_configs); print("MCP Manager Initialized.")
        except Exception as e: print(f"Failed to init MCP Manager: {e}"); MCP_MANAGER_INSTANCE = None
    return MCP_MANAGER_INSTANCE

@tool
def simple_math(a: int, b: int, operation: str = "add") -> str:
    """Performs 'add' or 'subtract'. Args: a (int), b (int), operation (str)."""
    if operation == "add": return f"{a} + {b} = {a + b}"
    if operation == "subtract": return f"{a} - {b} = {a - b}"
    return "Unknown math operation."

async def setup_vector_store(
    store_type: str, db_name: str, collection_name: str, ef: Optional[EmbeddingFunctionInterface]
) -> Optional[VectorStoreInterface]:
    db_path = os.path.join(DB_BASE_PATH, db_name)
    if os.path.exists(db_path): shutil.rmtree(db_path)
    else: os.makedirs(db_path, exist_ok=True)
    print(f"Setting up {store_type} at '{db_path}' for collection '{collection_name}'...")
    store: Optional[VectorStoreInterface] = None
    try:
        if store_type == "ChromaStore" and CHROMA_CLIENT_INSTALLED:
            store = ChromaStore(path=db_path, collection_name=collection_name, embedding_function=ef)
        elif store_type == "QdrantStore" and QDRANT_CLIENT_INSTALLED:
            if ef is None:
                print(f"ERROR: QdrantStore requires an explicit embedding function for setup. Skipping {collection_name}.")
                return None
            store = await QdrantStore.create(
                collection_name=collection_name, embedding_function=ef, path=db_path,
                recreate_collection_if_exists=True, distance_metric=qdrant_models.Distance.COSINE )
        else: print(f"Unsupported store_type '{store_type}' or client not installed."); return None
        pdf_content = await get_pdf_content()
        if pdf_content and store:
            chunks = chunk_text_by_fixed_size(pdf_content, CHUNK_SIZE, CHUNK_OVERLAP)
            ids = [str(uuid.uuid4()) for _ in range(len(chunks))]
            metadatas = [{"source": os.path.basename(PDF_PATH), "chunk_index": i} for i, _ in enumerate(chunks)]
            if chunks: await store.add_documents(documents=chunks, ids=ids, metadatas=metadatas)
            print(f"Ingested {len(chunks)} chunks into {collection_name}.")
        elif not pdf_content: print("No PDF content to ingest for RAG.")
        return store
    except Exception as e: print(f"ERROR setting up {store_type} '{collection_name}': {e}"); return None


async def cleanup_vector_store(store: Optional[VectorStoreInterface], db_name: str):
    if store and hasattr(store, 'close') and callable(store.close): await store.close()

async def run_llm_service_tests(llm_service: LLMServiceInterface, llm_model: str, service_name: str):
    print_section_header(f"{service_name} LLM Tests (ToolAgent & ReactAgent Basic)")
    agent1 = ToolAgent(llm_service=llm_service, model=llm_model, tools=[])
    query1 = "What is the capital of Japan?"
    print_test_case_header(f"{service_name} ToolAgent DirectQ"); print_query(query1); response1 = await agent1.run(user_msg=query1); print_agent_response(response1)
    agent2 = ToolAgent(llm_service=llm_service, model=llm_model, tools=[simple_math])
    query2 = "What is 150 plus 75?"
    print_test_case_header(f"{service_name} ToolAgent LocalTool"); print_query(query2); response2 = await agent2.run(user_msg=query2); print_agent_response(response2)
    agent3 = Agent(name=f"{service_name}Thinker", backstory="I break problems down.",
                   task_description="Explain pros/cons of remote work, 3 points each.",
                   llm_service=llm_service, model=llm_model)
    print_test_case_header(f"{service_name} ReactAgent Thought"); print_query(agent3.task_description); response3 = await agent3.run(); print_agent_response(response3)

async def run_rag_tests_for_service(
    llm_service: LLMServiceInterface, llm_model: str, service_name: str,
    ef: EmbeddingFunctionInterface, ef_name: str,
    store_type: str, store_db_name_suffix: str ):
    collection_name = f"rag_{service_name.lower()}_{ef_name.lower()}_{store_db_name_suffix}"
    db_name = f"{service_name.lower()}_{ef_name.lower()}_{store_db_name_suffix}"
    print_section_header(f"{service_name} RAG ({ef_name} on {store_type})")
    vector_store = await setup_vector_store(store_type, db_name, collection_name, ef)
    if not vector_store: print(f"Skipping RAG for {service_name}/{ef_name}/{store_type} due to VS setup error."); return
    agent_rag = Agent( name=f"{service_name}{ef_name.replace('_', '')}RAGer", backstory="I answer from the ML book.",
        task_description="Explain what a confusion matrix is, based on the book.",
        llm_service=llm_service, model=llm_model, vector_store=vector_store )
    print_test_case_header(f"{service_name} ReactAgent RAG"); print_query(agent_rag.task_description); response_rag = await agent_rag.run(); print_agent_response(response_rag)
    agent_tool_rag = ToolAgent( llm_service=llm_service, model=llm_model, vector_store=vector_store )
    query_tool_rag = "Typical steps in an ML project according to the book?"
    print_test_case_header(f"{service_name} ToolAgent RAG"); print_query(query_tool_rag); response_tool_rag = await agent_tool_rag.run(user_msg=query_tool_rag); print_agent_response(response_tool_rag)
    await cleanup_vector_store(vector_store, db_name)

async def run_rag_tests_chroma_default_ef(llm_service: LLMServiceInterface, llm_model: str, service_name: str):
    collection_name = f"rag_{service_name.lower()}_chroma_default"
    db_name = f"{service_name.lower()}_chroma_default_db"
    print_section_header(f"{service_name} RAG (ChromaDB with Default EF)")
    vector_store = await setup_vector_store("ChromaStore", db_name, collection_name, ef=None)
    if not vector_store: print(f"Skipping RAG for {service_name}/ChromaDefault due to VS error."); return
    agent_rag = Agent(name=f"{service_name}ChromaDefRAG", backstory="ML book expert via Chroma default EF.",
        task_description="What does the book say about model evaluation techniques?",
        llm_service=llm_service, model=llm_model, vector_store=vector_store)
    print_test_case_header(f"{service_name} ReactAgent RAG (Chroma Default)"); print_query(agent_rag.task_description); response_rag = await agent_rag.run(); print_agent_response(response_rag)
    await cleanup_vector_store(vector_store, db_name)

async def run_team_tests_for_service(llm_service: LLMServiceInterface, llm_model: str, service_name: str, mcp_manager: Optional[MCPClientManager]):
    print_section_header(f"{service_name} Team Test (RAG + Local + MCP)")
    st_ef = SentenceTransformerEmbeddings(model_name=ST_EMBED_MODEL)
    rag_db_name = f"team_rag_{service_name.lower()}"; rag_collection_name = "team_rag_coll"
    vector_store = await setup_vector_store("ChromaStore", rag_db_name, rag_collection_name, st_ef)
    if not vector_store: print(f"Skipping Team RAG for {service_name} due to VS setup error."); return
    with Team() as team:
        researcher = Agent( name=f"{service_name}Researcher", backstory="I find info.",
            task_description="Define 'validation set' using the book and web search.",
            llm_service=llm_service, model=llm_model, tools=[duckduckgo_search], vector_store=vector_store)
        calculator = Agent( name=f"{service_name}Calculator", backstory="I do math.",
            task_description="If researcher found 3 points, add 5 to it using 'add' tool, then multiply the sum by 2 using 'simple_math'.",
            llm_service=llm_service, model=llm_model, tools=[simple_math],
            mcp_manager=mcp_manager if MCP_ENABLED else None, mcp_server_names=["adder_server"] if MCP_ENABLED else None)
        reporter = Agent( name=f"{service_name}Reporter", backstory="I summarize.",
            task_description="Combine researcher and calculator outputs into a report.",
            llm_service=llm_service, model=llm_model)
        researcher >> calculator >> reporter
        await team.run()
    print_test_case_header(f"{service_name} Team Report"); print_query("Team Task: Validation sets, calc, report.")
    print_agent_response(team.results.get(f"{service_name}Reporter", {}))
    await cleanup_vector_store(vector_store, rag_db_name)

async def main():
    global MCP_ENABLED
    overall_start_time = time.time()
    if not os.path.exists(DB_BASE_PATH): os.makedirs(DB_BASE_PATH)
    await get_pdf_content()

    
    ollama_service: Optional[OllamaService] = None
    if os.getenv("RUN_OLLAMA_TESTS", "true").lower() == "true":
        try:
            ollama_service = OllamaService(
                default_model=OLLAMA_LLM_MODEL,
                base_url=f"{OLLAMA_HOST}/v1" # Use the global OLLAMA_HOST
            )
            print("Ollama Service Initialized.")
        except Exception as e: print(f"Failed to init Ollama Service: {e}")

    # ... (GroqService and GoogleOpenAICompatService initialization remains the same) ...
    groq_service: Optional[GroqService] = None
    if os.getenv("GROQ_API_KEY"):
        try: groq_service = GroqService(); print("Groq Service Initialized.")
        except Exception as e: print(f"Failed to init Groq Service: {e}")

    google_service: Optional[GoogleOpenAICompatService] = None
    if os.getenv("GOOGLE_API_KEY"):
        try: google_service = GoogleOpenAICompatService(); print("Google Service Initialized.")
        except Exception as e: print(f"Failed to init Google Service: {e}")


    st_ef = SentenceTransformerEmbeddings(model_name=ST_EMBED_MODEL)
    
    ollama_ef: Optional[OllamaEmbeddings] = None
    if ollama_service:
       
        if OLLAMA_EMBED_MODEL in KNOWN_OLLAMA_EMBEDDING_DIMENSIONS:
            try:
                ollama_ef = OllamaEmbeddings(
                    model_name=OLLAMA_EMBED_MODEL,
                    ollama_host=OLLAMA_HOST 
                )
            except Exception as e: print(f"Could not initialize OllamaEmbeddings: {e}")
        else:
            print(f"Warning: Ollama embed model '{OLLAMA_EMBED_MODEL}' not in KNOWN_OLLAMA_EMBEDDING_DIMENSIONS. Ollama RAG tests will be skipped.")
    
    fast_ef: Optional[FastEmbedEmbeddings] = None
    # FE_KNOWN_DIMS is imported at the top
    try:
        if FASTEMBED_MODEL in FE_KNOWN_DIMS:
            fast_ef = FastEmbedEmbeddings(model_name=FASTEMBED_MODEL)
        else:
            print(f"Warning: FastEmbed model '{FASTEMBED_MODEL}' not in FE_KNOWN_DIMS. FastEmbed RAG will be skipped.")
    except NameError: # If FastEmbedEmbeddings or FE_KNOWN_DIMS not defined due to import error
        print("FastEmbedEmbeddings or its KNOWN_DIMS not available.")
    except Exception as e:
        print(f"Could not initialize FastEmbedEmbeddings: {e}")
    if fast_ef: print("FastEmbedEmbeddings initialized (RAG tests may be commented out).")
    # print("INFO: FastEmbed RAG tests are currently commented out to save time.")


    mcp_manager = get_mcp_manager()
    MCP_ENABLED = mcp_manager is not None
    if not MCP_ENABLED: print("MCP Manager not available. MCP tool tests in Team will be skipped.")

    services_to_test = []
    if ollama_service: services_to_test.append({"service": ollama_service, "model": OLLAMA_LLM_MODEL, "name": "Ollama"})
    if groq_service: services_to_test.append({"service": groq_service, "model": GROQ_LLM_MODEL, "name": "Groq"})
    if google_service: services_to_test.append({"service": google_service, "model": GOOGLE_LLM_MODEL, "name": "Google"})

    for test_config in services_to_test:
        svc, model, name = test_config["service"], test_config["model"], test_config["name"]
        await run_llm_service_tests(svc, model, name)

        if name == "Ollama":
            if ollama_ef:
                if CHROMA_CLIENT_INSTALLED: await run_rag_tests_for_service(svc, model, name, ollama_ef, "OllamaEF", "ChromaStore", f"{name.lower()}_chroma_ollama")
                if QDRANT_CLIENT_INSTALLED: await run_rag_tests_for_service(svc, model, name, ollama_ef, "OllamaEF", "QdrantStore", f"{name.lower()}_qdrant_ollama")
        else: # For Groq and Google, use ST_EF as the primary EF for RAG
            if CHROMA_CLIENT_INSTALLED: await run_rag_tests_for_service(svc, model, name, st_ef, "ST_EF", "ChromaStore", f"{name.lower()}_chroma_st")
            if QDRANT_CLIENT_INSTALLED: await run_rag_tests_for_service(svc, model, name, st_ef, "ST_EF", "QdrantStore", f"{name.lower()}_qdrant_st")
            # FastEmbed RAG tests are commented out here
            # if fast_ef:
            #     if CHROMA_CLIENT_INSTALLED: await run_rag_tests_for_service(svc, model, name, fast_ef, "FastEF", "ChromaStore", f"{name.lower()}_chroma_fe")
            #     if QDRANT_CLIENT_INSTALLED: await run_rag_tests_for_service(svc, model, name, fast_ef, "FastEF", "QdrantStore", f"{name.lower()}_qdrant_fe")

        # Chroma Default EF test for all services
        if CHROMA_CLIENT_INSTALLED: await run_rag_tests_chroma_default_ef(svc, model, name)
        
        await run_team_tests_for_service(svc, model, name, mcp_manager)
        if hasattr(svc, 'close') and callable(svc.close): await svc.close()

    if mcp_manager: await mcp_manager.disconnect_all()
    print(f"\n\nTotal Comprehensive Test Suite Took: {time.time() - overall_start_time:.2f} seconds.")

if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: examples/document_ingestion.py
================================================
# --- START OF FILE examples/document_ingestion_test.py ---

import asyncio
import os
import shutil
import json
import uuid # For generating unique IDs
from dotenv import load_dotenv

# --- CLAP Imports ---
from clap.vector_stores.chroma_store import ChromaStore
from clap.utils.rag_utils import (
    load_text_file,
    load_pdf_file,
    load_csv_file,
    chunk_text_by_fixed_size # Use the fixed size chunker
)

# --- Embedding Function Imports ---
try:
    from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
    DEFAULT_EF = SentenceTransformerEmbeddingFunction()
except ImportError:
    print("Warning: sentence-transformers not installed.")
    from chromadb.utils.embedding_functions import DefaultEmbeddingFunction
    DEFAULT_EF = DefaultEmbeddingFunction()

# --- Config ---
load_dotenv()
CHROMA_DB_PATH = "./ingestion_test_chroma_db"
COLLECTION_NAME = "clap_ingestion_demo"

# --- File Paths ---
TXT_FILE = "examples/sample.txt"
PDF_FILE = "examples/sample.pdf"
CSV_FILE = "examples/sample.csv"

# --- Chunking Config ---
CHUNK_SIZE = 150 # User-defined chunk size
CHUNK_OVERLAP = 20 # User-defined overlap

async def ingest_and_query():
    """Loads, chunks, ingests, and queries documents."""
    print(f"Setting up ChromaDB at {CHROMA_DB_PATH}...")
    if os.path.exists(CHROMA_DB_PATH):
        shutil.rmtree(CHROMA_DB_PATH)

    vector_store = ChromaStore(
        path=CHROMA_DB_PATH,
        collection_name=COLLECTION_NAME,
        embedding_function=DEFAULT_EF
    )

    all_docs_to_add: List[str] = []
    all_ids_to_add: List[str] = []
    all_metadatas_to_add: List[Dict[str, Any]] = []

    # --- 1. Process TXT File ---
    print(f"\n--- Processing TXT: {TXT_FILE} ---")
    txt_content = load_text_file(TXT_FILE)
    if txt_content:
        txt_chunks = chunk_text_by_fixed_size(
            txt_content,
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP
        )
        print(f"Chunked TXT into {len(txt_chunks)} chunks (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}).")
        for i, chunk in enumerate(txt_chunks):
            chunk_id = f"txt_{os.path.basename(TXT_FILE)}_{i}"
            all_docs_to_add.append(chunk)
            all_ids_to_add.append(chunk_id)
            all_metadatas_to_add.append({"source": TXT_FILE, "chunk_num": i})

    # --- 2. Process PDF File ---
    print(f"\n--- Processing PDF: {PDF_FILE} ---")
    pdf_content = load_pdf_file(PDF_FILE)
    if pdf_content:
        pdf_chunks = chunk_text_by_fixed_size(
            pdf_content,
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP
        )
        print(f"Chunked PDF into {len(pdf_chunks)} chunks (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}).")
        for i, chunk in enumerate(pdf_chunks):
            chunk_id = f"pdf_{os.path.basename(PDF_FILE)}_{i}"
            all_docs_to_add.append(chunk)
            all_ids_to_add.append(chunk_id)
            all_metadatas_to_add.append({"source": PDF_FILE, "chunk_num": i})

    # --- 3. Process CSV File ---
    print(f"\n--- Processing CSV: {CSV_FILE} ---")
    # Treat each row's 'Content' as a document, use 'Title', 'Category', 'Year' as metadata
    csv_data = load_csv_file(
        CSV_FILE,
        content_column="Content",
        metadata_columns=["Title", "Category", "Year"] # Specify metadata columns by name
        # Alternatively use indices: content_column=2, metadata_columns=[1, 3, 4]
    )
    if csv_data:
        print(f"Loaded {len(csv_data)} rows from CSV.")
        for i, (content, metadata) in enumerate(csv_data):
            # Option 1: Add each row as one "document" (no further chunking here)
            row_id = f"csv_{os.path.basename(CSV_FILE)}_row{metadata.get('source_row', i)}"
            all_docs_to_add.append(content)
            all_ids_to_add.append(row_id)
            # Add original source file to metadata
            metadata["source"] = CSV_FILE
            all_metadatas_to_add.append(metadata)

            # Option 2 (Alternative): Chunk the content of *each* CSV row if needed
            # csv_chunks = chunk_text_by_fixed_size(content, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
            # for j, chunk in enumerate(csv_chunks):
            #     chunk_id = f"csv_{os.path.basename(CSV_FILE)}_row{metadata.get('source_row', i)}_chunk{j}"
            #     all_docs_to_add.append(chunk)
            #     all_ids_to_add.append(chunk_id)
            #     chunk_meta = metadata.copy()
            #     chunk_meta["source"] = CSV_FILE
            #     chunk_meta["chunk_num"] = j
            #     all_metadatas_to_add.append(chunk_meta)


    # --- 4. Add all prepared documents to Chroma ---
    print(f"\n--- Adding {len(all_docs_to_add)} total processed chunks/rows to ChromaDB ---")
    if all_docs_to_add:
        # Consider batching adds if the list becomes very large
        await vector_store.aadd_documents(
            documents=all_docs_to_add,
            ids=all_ids_to_add,
            metadatas=all_metadatas_to_add
        )
        print("Ingestion complete.")
    else:
        print("No documents processed to add.")

    # --- 5. Query the ingested data ---
    print("\n--- Querying Ingested Data ---")
    queries = [
        "What is ChromaDB?",
        "Tell me about Machine Learning",
        "What is the first sentence about?" # Should match TXT
    ]
    for query in queries:
        print(f"\nQuerying for: '{query}' (Top 2 results)")
        results = await vector_store.aquery(
            query_texts=[query],
            n_results=2,
            include=["metadatas", "documents", "distances"]
        )
        print(json.dumps(results, indent=2, ensure_ascii=False))

    # --- Cleanup ---
    if os.path.exists(CHROMA_DB_PATH):
         print(f"\nCleaning up test database: {CHROMA_DB_PATH}")
         # shutil.rmtree(CHROMA_DB_PATH) # Uncomment to auto-delete DB

# --- Main Execution ---
if __name__ == "__main__":
    asyncio.run(ingest_and_query())


================================================
FILE: examples/email_test.py
================================================
import asyncio
import os
from dotenv import load_dotenv
# --- START DEBUG BLOCK ---
print(f"Attempting to load .env file.")
print(f"Current Working Directory: {os.getcwd()}") # Check if this is Cognitive-Layer/
env_path = os.path.join(os.getcwd(), '.env')
print(f"Expected .env path: {env_path}")
print(f"Does .env exist at expected path? {os.path.exists(env_path)}")
# Load the .env file
loaded_dotenv = load_dotenv()
print(f"load_dotenv() returned: {loaded_dotenv}") # Should be True if file was found
# Explicitly check the environment variables *after* loading
loaded_user = os.getenv("SMTP_USERNAME")
loaded_pass_exists = bool(os.getenv("SMTP_PASSWORD")) # Don't print the password itself
print(f"os.getenv('SMTP_USERNAME') after load: {loaded_user}")
print(f"os.getenv('SMTP_PASSWORD') is set after load: {loaded_pass_exists}")
print("--- END DEBUG BLOCK ---")
# Check if variables are still None/empty here
if not loaded_user or not loaded_pass_exists:
    print("\n*** ERROR: Environment variables NOT correctly loaded! Check .env file location and content. ***\n")
# --- Original imports ---
from clap import ToolAgent
from clap import send_email, fetch_recent_emails

load_dotenv()

async def main():

    agent = ToolAgent(
        tools=[send_email, fetch_recent_emails], 
        model="llama-3.3-70b-versatile"
    )

   
    query1 = "Check my INBOX and tell me the last 2 emails."
    response1 = await agent.run(user_msg=query1)
    print(response1)

    await asyncio.sleep(1) # Small delay

    test_recipient = "maitreyamishra04@gmail.com" 
    
    query2 = f"Draft and send an email to {test_recipient}. Subject should be 'Agent Test' and body should be 'Hello from the CLAP Framework! and write a 30 words message thanking them for using our frame work'"
    response2 = await agent.run(user_msg=query2)


asyncio.run(main())


================================================
FILE: examples/google_agent.py
================================================
import os
from dotenv import load_dotenv
from clap import ReactAgent
from clap import GoogleOpenAICompatService
from clap import tool
import asyncio

load_dotenv()

@tool
def get_circle_area(radius: float) -> float:
    """Calculates the area of a circle given its radius."""
    print(f"[Local Tool] Calculating area for radius: {radius}")
    return 3.14159 * (radius ** 2)

async def main():
    google_llm_service = GoogleOpenAICompatService()

    agent = ReactAgent(
        llm_service=google_llm_service, # Pass the correct service instance
        model="gemini-2.5-pro-exp-03-25", # Specify a Gemini model name
        tools=[get_circle_area],
    )

    user_query = "What is the area of a circle with a radius of 5?"
    response = await agent.run(user_msg=user_query)
    print(response)
    
asyncio.run(main())


================================================
FILE: examples/google_mcp.py
================================================

import asyncio
import os
from dotenv import load_dotenv
from pydantic import HttpUrl

from clap import ReactAgent
from clap import GoogleOpenAICompatService
from clap import MCPClientManager, SseServerConfig
from clap import tool

load_dotenv()

@tool
def multiply(a: int, b: int) -> int:
    """Multiplies integer a by integer b."""
    print(f"[Local Tool] Multiplying: {a} * {b}")
    return a * b

async def main():
    
    server_configs = {
        "adder_server": SseServerConfig(url=HttpUrl("http://localhost:8000")),
        "subtract_server": SseServerConfig(url=HttpUrl("http://localhost:8001"))
    }
    manager = MCPClientManager(server_configs)

    # 3. Instantiate the Google LLM Service
    google_llm_service = GoogleOpenAICompatService()
    gemini_model = "gemini-2.5-pro-exp-03-25" # Or your preferred compatible model

    agent = ReactAgent(
        llm_service=google_llm_service,
        model=gemini_model,
        tools=[multiply],
        mcp_manager=manager, 
        mcp_server_names=["adder_server","subtract_server"] 
    )

    user_query = "Calculate (10 + 5) * 3"

    response = await agent.run(user_msg=user_query)

    await manager.disconnect_all()


asyncio.run(main())




================================================
FILE: examples/google_team.py
================================================

import asyncio
import os
from dotenv import load_dotenv

from clap import Team
from clap import Agent
from clap import GoogleOpenAICompatService

load_dotenv()

async def run_google_team():
    google_llm_service = GoogleOpenAICompatService()
    gemini_model = "gemini-2.5-pro-exp-03-25" 
    topic = "the benefits of using asynchronous programming in Python"
    with Team() as team:
        planner = Agent(
            name="Topic_Planner",
            backstory="Expert in outlining content.",
            task_description=f"Create a short, 3-bullet point outline for explaining '{topic}'.",
            task_expected_output="A 3-item bullet list.",
            llm_service=google_llm_service,
            model=gemini_model,
            
        )

        writer = Agent(
            name="Content_Writer",
            backstory="Skilled technical writer.",
            task_description="Take the outline provided in the context and write a concise paragraph explaining the topic.",
            task_expected_output="One paragraph based on the outline.",
            llm_service=google_llm_service,
            model=gemini_model,
           
        )

        # Define dependency
        planner >> writer

        
        await team.run()

asyncio.run(run_google_team())



================================================
FILE: examples/huge_rag.py
================================================
import asyncio
import os
import shutil
import time
from dotenv import load_dotenv
from clap import Agent                                  
from clap.vector_stores.chroma_store import ChromaStore 
from clap.utils.rag_utils import (
    load_pdf_file,
    chunk_text_by_fixed_size
)                                                       
from clap.llm_services.groq_service import GroqService  

from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
DEFAULT_EF = SentenceTransformerEmbeddingFunction()

load_dotenv()
PDF_PATH = "/Users/maitreyamishra/PROJECTS/Cognitive-Layer/examples/handsonml.pdf"
CHROMA_DB_PATH = "./large_pdf_chroma_db" 
COLLECTION_NAME = "ml_book_rag"
CHUNK_SIZE = 500    
CHUNK_OVERLAP = 50    
LLM_MODEL = "llama-3.3-70b-versatile" 

async def run_minimal_rag():
    start_time = time.time()

    if os.path.exists(CHROMA_DB_PATH):
        print(f"Removing existing DB at {CHROMA_DB_PATH}...")
        shutil.rmtree(CHROMA_DB_PATH)

    vector_store = ChromaStore(
        path=CHROMA_DB_PATH,
        collection_name=COLLECTION_NAME,
        embedding_function=DEFAULT_EF
    )

    pdf_content = load_pdf_file(PDF_PATH)

    chunks = chunk_text_by_fixed_size(pdf_content, CHUNK_SIZE, CHUNK_OVERLAP)
    print(f"Generated {len(chunks)} chunks.")

    ids = [f"chunk_{i}" for i in range(len(chunks))]
    metadatas = [{"source": PDF_PATH, "chunk_index": i} for i in range(len(chunks))]

    print("Adding chunks to vector store (embedding process)...")
    if chunks:
        await vector_store.add_documents(documents=chunks, ids=ids, metadatas=metadatas)
        print("Ingestion complete.")
    else:
        print("No chunks generated to add.")

    ingestion_time = time.time() - start_time
    print(f"Ingestion took {ingestion_time:.2f} seconds.")

    llm_service = GroqService() # Or GoogleOpenAICompatService() etc.
    rag_agent = Agent(
        name="Book_Expert",
        backstory="Assistant answering questions based *only* on the provided Machine Learning book context.",
        task_description="Placeholder Query",
        task_expected_output="A concise answer derived solely from the retrieved book context.",
        llm_service=llm_service,
        model=LLM_MODEL,
        vector_store=vector_store 
    )

    queries = [
    # Core Concepts & Comparisons
    "Compare Random Forests and Gradient Boosting machines, highlighting the key differences in how they build ensembles of decision trees.",
    "Describe the concept of the 'kernel trick' as used in Support Vector Machines (SVMs) and explain its primary benefit.",

    # Algorithms & Techniques
    "Explain the objective of Principal Component Analysis (PCA) and outline the main steps involved in its calculation according to the text.",
    "What is the fundamental purpose of the backpropagation algorithm in training artificial neural networks?",
    "Discuss the purpose of regularization in linear models, mentioning techniques like Ridge or Lasso regression as explained in the book.",
    "What are some limitations or disadvantages of the K-Means clustering algorithm mentioned in the text?",

    # Deep Learning Specifics
    # "Explain the role of activation functions in neural networks, perhaps using the ReLU function as an example described in the text.",
    # "What is a convolutional layer and what specific type of patterns or features is it designed to detect in Convolutional Neural Networks (CNNs)?",
    # "Describe the concept of transfer learning in the context of deep learning models and its main advantage.",
    # "What are Recurrent Neural Networks (RNNs) typically used for, according to the book, and what is a common challenge when training them?",

    # # Data & Evaluation
    # "According to the book, why is feature scaling often a necessary preprocessing step for many machine learning algorithms?",
    # "Explain the purpose of cross-validation in model evaluation and briefly describe the K-Fold strategy.",
    "What are precision and recall, and why might you prioritize one over the other in different classification scenarios discussed in the text?"]


    for q in queries:
        rag_agent.task_description = q

        result = await rag_agent.run()

        print(result.get("output", "Agent failed to produce an answer."))

        end_time = time.time()
        print(f"\nTotal process took {(end_time - start_time):.2f} seconds.")


asyncio.run(run_minimal_rag())


================================================
FILE: examples/local_rag.py
================================================
import asyncio
import os
import shutil
import time
from dotenv import load_dotenv
import uuid
from clap import Agent
from clap.vector_stores.qdrant_store import QdrantStore
from clap.utils.rag_utils import load_pdf_file, chunk_text_by_fixed_size
from clap.llm_services.ollama_service import OllamaOpenAICompatService
from clap.embedding.ollama_embedding import OllamaEmbeddings, KNOWN_OLLAMA_EMBEDDING_DIMENSIONS
from qdrant_client import models as qdrant_models

load_dotenv()

PDF_PATH = "/Users/maitreyamishra/PROJECTS/Cognitive-Layer/examples/handsonml.pdf" 
DB_PATH = "./ollama_rag_qdrant_minimal_db"
COLLECTION_NAME = "ml_book_ollama_minimal"
CHUNK_SIZE, CHUNK_OVERLAP = 500, 50
OLLAMA_HOST = "http://localhost:11434"
OLLAMA_LLM_MODEL = "llama3.2:latest" 
OLLAMA_EMBED_MODEL = "nomic-embed-text"

async def run_minimal_ollama_rag():
    start_time = time.time()
    if OLLAMA_EMBED_MODEL not in KNOWN_OLLAMA_EMBEDDING_DIMENSIONS:
        print(f"ERROR: Dimension for '{OLLAMA_EMBED_MODEL}' unknown."); return

    print(f"Minimal Ollama RAG Test (LLM: {OLLAMA_LLM_MODEL}, Embed: {OLLAMA_EMBED_MODEL})")

    if os.path.exists(DB_PATH): shutil.rmtree(DB_PATH)

    ollama_ef = OllamaEmbeddings(model_name=OLLAMA_EMBED_MODEL, ollama_host=OLLAMA_HOST)
    vector_store = await QdrantStore.create(
        collection_name=COLLECTION_NAME, embedding_function=ollama_ef, path=DB_PATH,
        recreate_collection_if_exists=True, distance_metric=qdrant_models.Distance.COSINE
    )

    print("Loading & Chunking PDF...")
    pdf_content = load_pdf_file(PDF_PATH)
    if not pdf_content: await vector_store.close(); return
    chunks = chunk_text_by_fixed_size(pdf_content, CHUNK_SIZE, CHUNK_OVERLAP)
    ids = [str(uuid.uuid4()) for _ in range(len(chunks))] 
    metadatas = [{"src": os.path.basename(PDF_PATH)} for _ in range(len(chunks))]

    print(f"Ingesting {len(chunks)} chunks...")
    ingest_start = time.time()
    if chunks: await vector_store.add_documents(documents=chunks, ids=ids, metadatas=metadatas)
    print(f"Ingestion Took: {time.time() - ingest_start:.2f}s")

    ollama_llm_service = OllamaOpenAICompatService(default_model=OLLAMA_LLM_MODEL, base_url=f"{OLLAMA_HOST}/v1")
    rag_agent = Agent(
        name="OllamaMinimalExpert", backstory="Answer from book context.",
        task_description="What are Generative Adversarial Networks?", # can be overwritten
        llm_service=ollama_llm_service, model=OLLAMA_LLM_MODEL, vector_store=vector_store
    )

    user_query = "Explain Generative Adversarial Networks (GANs) based on the book."
    rag_agent.task_description = user_query
    
    query_start = time.time()
    result = await rag_agent.run()
    print(f"Query Took: {time.time() - query_start:.2f}s")
    print(result.get("output", "No answer."))

    await vector_store.close()
    await ollama_llm_service.close()
    print(f"\nTotal Test Took: {time.time() - start_time:.2f}s")


asyncio.run(run_minimal_ollama_rag())



================================================
FILE: examples/mcp_team_agent.py
================================================

import os
import asyncio
from dotenv import load_dotenv
from pydantic import HttpUrl
from clap import Team
from clap import Agent
from clap import duckduckgo_search 
from clap import tool 
from clap import MCPClientManager, SseServerConfig
load_dotenv()

@tool
def count_words(text: str) -> int:
    """Counts the number of words in a given text."""
    return len(text.split())

async def run_agile_team_with_mcp():
    """Defines and runs the Agile benefits team, potentially using MCP tools."""
    mcp_server_configs = {
        "adder_server": SseServerConfig(url=HttpUrl("http://localhost:8000"))
       
    }
    mcp_manager = MCPClientManager(mcp_server_configs)

    topic = "the benefits of Agile methodology in software development"
    
    with Team() as team:
        researcher = Agent(
            name="Web_Researcher",
            backstory="You are an expert web researcher. You can use local search tools or potentially remote MCP tools.",
            task_description=f"Search the web for information on '{topic}'. Also calculate 5 + 3 using the 'add' tool.", # Modified task to use 'add'
            task_expected_output="Raw search results AND the result of 5 + 3.",
            tools=[duckduckgo_search],
            mcp_manager=mcp_manager, # Pass the shared manager
            mcp_server_names=["adder_server"] # Tell it which server(s) to use
        )

        summarizer = Agent(
            name="Content_Summarizer",
            backstory="You are an expert analyst. You can count words locally.",
            task_description="Analyze the provided context (search results and addition result) and extract the main benefits. Also count the words in the addition result.",
            task_expected_output="A concise bullet-point list summarizing key benefits, and the word count.",
            tools=[count_words],
        )
        reporter = Agent(
            name="Report_Writer",
            backstory="You are a skilled writer.",
            task_description="Take the summarized key points and word count, and write a short paragraph.",
            task_expected_output="A single paragraph summarizing the benefits and mentioning the word count."
        )

        researcher >> summarizer >> reporter
        await team.run()
        await mcp_manager.disconnect_all()

asyncio.run(run_agile_team_with_mcp())


================================================
FILE: examples/mcp_test_agent.py
================================================

import os
import asyncio
from dotenv import load_dotenv
from pydantic import HttpUrl
from clap import ToolAgent
from clap import MCPClientManager, SseServerConfig

load_dotenv()

async def main():
    server_configs = {
        server_name: SseServerConfig(url=HttpUrl("http://localhost:8000"))
    }
    manager = MCPClientManager(server_configs)
    agent = ToolAgent(
        mcp_manager=manager,
        mcp_server_names=["adder_server"],
        model="meta-llama/llama-4-scout-17b-16e-instruct" 
    )

    user_query = "What is 123 plus 456?"

    response = await agent.run(user_msg=user_query)
    await manager.disconnect_all()


asyncio.run(main())



================================================
FILE: examples/mcp_test_suite.py
================================================
import asyncio
import os
from dotenv import load_dotenv
from typing import Optional

from pydantic import HttpUrl


from clap import MCPClientManager, SseServerConfig

load_dotenv()

def print_header(title: str):
    print(f"\n{'='*10} {title.upper()} {'='*10}")

async def run_mcp_test_cycle(
    manager: MCPClientManager,
    server_name: str,
    tool_to_call: Optional[str] = None,
    tool_args: Optional[dict] = None
):
    print_header(f"TEST CYCLE FOR {server_name.upper()}")
    try:
        print(f"Attempting to list tools from '{server_name}'...")
        tools = await manager.list_remote_tools(server_name)
        print(f"Found tools on '{server_name}': {[t.name for t in tools] if tools else 'None'}")

        if tool_to_call and tool_args:
            if any(t.name == tool_to_call for t in tools):
                print(f"Attempting to call tool '{tool_to_call}' on '{server_name}' with args: {tool_args}...")
                result = await manager.call_remote_tool(server_name, tool_to_call, tool_args)
                print(f"Result from '{tool_to_call}' on '{server_name}': {result}")
            else:
                print(f"Tool '{tool_to_call}' not found on server '{server_name}'.")

    except RuntimeError as e:
        print(f"RuntimeError during {server_name} cycle: {e}")
    except Exception as e:
        print(f"Unexpected error during {server_name} cycle: {type(e).__name__} - {e}")


async def main():
    print("MCP Robustness Test: Starting...")
    print("Ensure MCP servers ('adder_server' on 8000, 'subtract_server' on 8001) are running.")

    mcp_server_configs = {
        "adder_server": SseServerConfig(url=HttpUrl("http://localhost:8000")),
        "subtract_server": SseServerConfig(url=HttpUrl("http://localhost:8001")),
        "non_existent_server": SseServerConfig(url=HttpUrl("http://localhost:8009")) # To test connection error
    }
    manager: Optional[MCPClientManager] = None

    try:
        manager = MCPClientManager(server_configs=mcp_server_configs)
        print("MCPClientManager initialized.")

        await run_mcp_test_cycle(manager, "adder_server", tool_to_call="add", tool_args={"a": 10, "b": 5})

        print_header("SIMULATING AN EXTERNAL ERROR")
        try:
            
            print("Raising a simulated application error...")
            raise ValueError("Simulated application error after first MCP interaction.")
        except ValueError as app_error:
            print(f"Caught simulated application error: {app_error}")
            print("Proceeding to test other MCP operations and cleanup...")

        await run_mcp_test_cycle(manager, "subtract_server", tool_to_call="sub", tool_args={"a": 20, "b": 3})

        await run_mcp_test_cycle(manager, "non_existent_server")


    except Exception as e:
        print(f"Critical error in main test execution: {type(e).__name__} - {e}")
    finally:
        if manager:
            print_header("FINAL MCP CLEANUP")
            print("Calling manager.disconnect_all()...")
            await manager.disconnect_all()
            print("manager.disconnect_all() completed.")
        else:
            print("MCPClientManager was not initialized.")

    print("\nMCP Robustness Test: Finished.")

if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: examples/ollama_test.py
================================================
import asyncio
import os
from dotenv import load_dotenv
from clap import Agent, Team, ToolAgent # Core CLAP
from clap.tool_pattern.tool import tool # For defining local tools

# --- LLM Services ---
from clap.llm_services.ollama_llm_service import OllamaOpenAICompatService
from clap.llm_services.groq_service import GroqService
from clap.llm_services.google_openai_compat_service import GoogleOpenAICompatService

# --- Embedding Functions ---
from clap.embedding.sentence_transformer_embedding import SentenceTransformerEmbeddings
from clap.embedding.ollama_embedding import OllamaEmbeddings
from clap.embedding.fastembed_embedding import FastEmbedEmbeddings # If you keep this

# --- Vector Stores ---
from clap.vector_stores.chroma_store import ChromaStore
from clap.vector_stores.qdrant_store import QdrantStore

# --- Pre-built Tools ---
from clap.tools.web_search import duckduckgo_search
from clap.tools.web_crawler import scrape_url 

load_dotenv()

OLLAMA_LLM_MODEL = "llama3.2:latest" # Or your preferred Ollama model
OLLAMA_EMBED_MODEL = "nomic-embed-text" # Or your preferred Ollama embedding model
GROQ_LLM_MODEL = "llama-3.3-70b-versatile"
OLLAMA_HOST = "http://localhost:11434"

@tool
def get_capital(country: str) -> str:
    """Returns the capital of a given country."""
    capitals = {"france": "Paris", "germany": "Berlin", "japan": "Tokyo"}
    return capitals.get(country.lower(), f"Sorry, I don't know the capital of {country}.")

@tool
def get_weather(city: str) -> str:
    """Gets the current weather for a city."""
    if city.lower() == "london": return "Weather in London is 15°C and cloudy."
    return f"Weather for {city} is sunny."

# async def main():
#     ollama_service = OllamaOpenAICompatService(default_model=OLLAMA_LLM_MODEL, base_url=f"{OLLAMA_HOST}/v1")
    
#     agent = ToolAgent(llm_service=ollama_service, model=OLLAMA_LLM_MODEL, tools=[])
    
#     query = "Hello, how are you today?"
#     response = await agent.run(user_msg=query)

#     await ollama_service.close()

async def main():
    print(f"Using Ollama model: {OLLAMA_LLM_MODEL} on {OLLAMA_HOST}")
    ollama_service = OllamaOpenAICompatService(default_model=OLLAMA_LLM_MODEL, base_url=f"{OLLAMA_HOST}/v1")
    
    # ToolAgent initialized with the get_capital tool
    agent = ToolAgent(llm_service=ollama_service, model=OLLAMA_LLM_MODEL, tools=[get_capital])
    
    query = "What is the capital of France?"
    response = await agent.run(user_msg=query)
    print("Ollama ToolAgent (Local Tool)", query, response)

    query_unknown = "What is the capital of Wonderland?"
    response_unknown = await agent.run(user_msg=query_unknown)
    print("Ollama ToolAgent (Local Tool - Unknown)", query_unknown, response_unknown)

    await ollama_service.close()

asyncio.run(main())


================================================
FILE: examples/qdrant_ingestion.py
================================================
# --- START OF FILE examples/qdrant_ingestion_test.py ---
import asyncio
import os
import shutil
import time
import json
import uuid
from dotenv import load_dotenv
from typing import Optional

from clap import Agent
from clap.vector_stores.qdrant_store import QdrantStore
from clap.utils.rag_utils import (
    load_pdf_file,
    chunk_text_by_fixed_size
)
from clap.llm_services.groq_service import GroqService 
from clap.embedding.sentence_transformer_embedding import SentenceTransformerEmbeddings
from clap.embedding.fastembed_embedding import FastEmbedEmbeddings


# Qdrant models for distance
try:
    from qdrant_client import models as qdrant_models
except ImportError:
    print("ERROR: qdrant-client not found. Run: pip install 'qdrant-client[fastembed]'")
    exit(1)


# --- Configuration ---
load_dotenv()
# !!! IMPORTANT: Update this path to your large PDF file !!!
PDF_PATH = "/Users/maitreyamishra/PROJECTS/Cognitive-Layer/examples/Hands_On_ML.pdf" # Or the actual path to your PDF
# ---
DB_BASE_PATH = "./qdrant_test_dbs" # Base directory for Qdrant DBs
COLLECTION_NAME_CUSTOM_EF = "ml_book_custom_ef"
COLLECTION_NAME_FASTEMBED = "ml_book_fastembed"

CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
LLM_MODEL = "llama-3.3-70b-versatile" # Or your preferred model

# Qdrant's default fastembed model and its known dimension
# (Ensure this matches the actual default or the one you want to test)
QDRANT_INTERNAL_FASTEMBED_MODEL_NAME = "BAAI/bge-small-en-v1.5"


async def perform_rag_cycle(
    db_path: str,
    collection_name: str,
    embedding_function_instance: Optional[SentenceTransformerEmbeddings], # Explicitly typed for this test
    use_internal_fastembed_flag: bool,
    fastembed_model_to_use: Optional[str]
):
    """Performs a full RAG cycle: setup, ingest, query."""
    cycle_start_time = time.time()
    if embedding_function_instance:
        print(f"Embedding: Custom SentenceTransformerEmbeddings")
    elif use_internal_fastembed_flag and fastembed_model_to_use:
        print(f"Embedding: Qdrant Internal FastEmbed (model: {fastembed_model_to_use})")
    else:
        print("Error: Invalid embedding configuration for cycle.")
        return

        # --- Inside perform_rag_cycle in the example script ---
    # Replace the old initialization block with this:
    try:
        qdrant_store = await QdrantStore.create(
            collection_name=collection_name,
            embedding_function=embedding_function_instance,
            use_internal_fastembed=use_internal_fastembed_flag,
            fastembed_model_name=fastembed_model_to_use, # type: ignore
            path=db_path,
            recreate_collection_if_exists=True,
            distance_metric=qdrant_models.Distance.COSINE
        )
    except Exception as e:
        print(f"FATAL: Failed to create QdrantStore: {e}")
        return # Stop this cycle if store creation fails
    # --- Continue with the rest of the cycle ---


    # --- 2. Load and Chunk PDF ---
    pdf_content = load_pdf_file(PDF_PATH)
    if not pdf_content:
        print(f"Failed to load PDF content from {PDF_PATH}.")
        await qdrant_store.close()
        return

    print("Chunking PDF content...")
    chunks = chunk_text_by_fixed_size(pdf_content, CHUNK_SIZE, CHUNK_OVERLAP)
    print(f"Generated {len(chunks)} chunks.")

    # --- 3. Prepare and Add Data ---
    ids = [str(uuid.uuid4()) for _ in range(len(chunks))]
    metadatas = [{"source": os.path.basename(PDF_PATH), "chunk_idx": i} for i in range(len(chunks))]

    print("Adding chunks to vector store (embedding process)...")
    ingestion_start_time = time.time()
    if chunks:
        await qdrant_store.add_documents(documents=chunks, ids=ids, metadatas=metadatas)
        print("Ingestion complete.")
    else:
        print("No chunks generated to add.")
    ingestion_duration = time.time() - ingestion_start_time
    print(f"Data Ingestion Took: {ingestion_duration:.2f} seconds.")

    llm_service = GroqService()
    rag_agent = Agent(
        name=f"BookExpert_{collection_name}",
        backstory="Assistant answering questions based *only* on the provided Machine Learning book context.",
        task_description="Placeholder - will be set by query",
        llm_service=llm_service,
        model=LLM_MODEL,
        vector_store=qdrant_store
    )

    # --- 5. Ask a Question ---
    user_query = "Compare and contrast decision trees and support vector machines as explained in the book."
    print(f"\n--- RAG Query for '{collection_name}' ---")
    print(f"User Query: {user_query}")

    rag_agent.task_description = user_query # Set the actual query
    query_start_time = time.time()
    result = await rag_agent.run()
    query_duration = time.time() - query_start_time
    print(f"RAG Query Took: {query_duration:.2f} seconds.")

    # --- 6. Display Result ---
    print("\n--- Final Agent Answer ---")
    print(result.get("output", "Agent failed to produce an answer."))

    # --- 7. Cleanup ---
    await qdrant_store.close() # Important to close Qdrant, especially file-based
    cycle_duration = time.time() - cycle_start_time
    print(f"--- RAG Cycle for {collection_name} Took: {cycle_duration:.2f} seconds ---")


async def main():
    overall_start_time = time.time()

    if not os.path.exists(PDF_PATH):
        print(f"ERROR: PDF file not found at '{PDF_PATH}'. Please update the PDF_PATH variable.")
        return
    if not os.getenv("GROQ_API_KEY"): # Add check for GOOGLE_API_KEY if using Google LLM
         print("Warning: LLM API Key (e.g., GROQ_API_KEY) not found in environment variables.")
         # return # Optionally exit if key is strictly needed

    # --- Test Cycle 1: Custom EmbeddingFunctionInterface ---
    print("\n\n========== TEST 1: QDRANT WITH CUSTOM SentenceTransformerEmbeddings ==========")
    st_embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2") # Default model
    db_path_custom_ef = os.path.join(DB_BASE_PATH, "custom_ef_db")
    await perform_rag_cycle(
        db_path=db_path_custom_ef,
        collection_name=COLLECTION_NAME_CUSTOM_EF,
        embedding_function_instance=st_embeddings,
        use_internal_fastembed_flag=False,
        fastembed_model_to_use=None
    )

    await asyncio.sleep(2) # Small pause

    # --- Test Cycle 2: Qdrant WITH FastEmbed Wrapper ---
    print("\n\n========== TEST 2: QDRANT WITH FastEmbedEmbeddings WRAPPER ==========")
    try:
        # Instantiate the wrapper
        fast_embed_ef = FastEmbedEmbeddings(model_name=QDRANT_INTERNAL_FASTEMBED_MODEL_NAME)

        db_path_fastembed = os.path.join(DB_BASE_PATH, "fastembed_via_wrapper_db")
        await perform_rag_cycle(
            db_path=db_path_fastembed,
            collection_name=COLLECTION_NAME_FASTEMBED, # Can reuse name or make unique
            # Pass the wrapper instance as the embedding_function
            embedding_function_instance=fast_embed_ef,
            # These are now irrelevant as we provide the EF
            use_internal_fastembed_flag=False,
            fastembed_model_to_use=None
        )
    except ImportError as e:
        print(f"\nSkipping FastEmbed test cycle: {e}")
    except Exception as e:
         print(f"\nError during FastEmbed test cycle: {e}")

# --- Main Execution ---
if __name__ == "__main__":
    asyncio.run(main())

# --- END OF FILE examples/qdrant_ingestion_test.py ---


================================================
FILE: examples/rag_all_functionalities.py
================================================
# --- START OF FILE examples/complex_rag_mcp_team_test.py ---

import asyncio
import os
import shutil
import json
from dotenv import load_dotenv
from pydantic import HttpUrl # For MCP ServerConfig

# --- CLAP Imports ---
from clap import Agent, Team
from clap.vector_stores.chroma_store import ChromaStore
from clap.utils.rag_utils import chunk_text_by_separator
from clap.llm_services.groq_service import GroqService # Or GoogleOpenAICompatService
from clap.tool_pattern.tool import tool # For local tools
from clap.mcp_client.client import MCPClientManager, SseServerConfig # MCP Client
from clap.tools.web_search import duckduckgo_search # Pre-built local tool

# --- Embedding Function Imports ---
try:
    # Requires: pip install sentence-transformers
    from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
    DEFAULT_EF = SentenceTransformerEmbeddingFunction()
except ImportError:
    print("Warning: sentence-transformers not installed. ChromaDB might use its default EF.")
    print("Install with: pip install sentence-transformers")
    from chromadb.utils.embedding_functions import DefaultEmbeddingFunction
    DEFAULT_EF = DefaultEmbeddingFunction()

# --- Config ---
load_dotenv()
CHROMA_DB_PATH = "./complex_test_chroma_db"
COLLECTION_NAME = "clap_complex_demo"

# --- Sample Data for RAG ---
SAMPLE_RAG_DOCUMENTS = [
    """Agent performance metrics are crucial for evaluation. Key factors include task completion rate, latency, and cost per task. Tool usage accuracy is also vital, especially in complex workflows involving external APIs or databases.""",
    """Integrating multiple AI systems often presents challenges. Ensuring reliable data flow, handling asynchronous operations gracefully, and maintaining context across different agents requires careful architectural design. Frameworks aim to simplify this.""",
    """User experience in agentic systems depends heavily on responsiveness and accuracy. Long delays caused by inefficient tool calls or complex reasoning loops can frustrate users. Optimizing the ReAct cycle is important.""",
    """Security considerations for AI agents involve managing API keys securely, validating tool inputs and outputs, and preventing prompt injection attacks. Access control for tools and data sources (like vector stores or MCP endpoints) is necessary.""",
]

# --- Local Tool Definition ---
@tool
def multiply(a: int, b: int) -> int:
    """Calculates the product of two integers."""
    print(f"[Local Tool Executing] multiply({a}, {b})")
    return a * b

# --- Vector Store Setup ---
async def setup_vector_store():
    """Sets up the ChromaDB store and adds sample data."""
    print(f"Setting up ChromaDB at {CHROMA_DB_PATH}...")
    if os.path.exists(CHROMA_DB_PATH):
        shutil.rmtree(CHROMA_DB_PATH)

    vector_store = ChromaStore(
        path=CHROMA_DB_PATH,
        collection_name=COLLECTION_NAME,
        embedding_function=DEFAULT_EF
    )

    all_chunks = []
    all_ids = []
    all_metadatas = []
    doc_id_counter = 1
    chunk_id_counter = 1
    for doc in SAMPLE_RAG_DOCUMENTS:
        chunks = chunk_text_by_separator(doc, separator=". ")
        for chunk in chunks:
            if not chunk.strip(): continue
            chunk_id = f"rag_doc{doc_id_counter}_chunk{chunk_id_counter}"
            all_chunks.append(chunk.strip() + ".")
            all_ids.append(chunk_id)
            all_metadatas.append({"source_doc": f"rag_doc{doc_id_counter}", "type": "internal_note"})
            chunk_id_counter += 1
        doc_id_counter += 1
        chunk_id_counter = 1

    print(f"Adding {len(all_chunks)} chunks to ChromaDB...")
    if all_chunks:
        await vector_store.add_documents(
            documents=all_chunks,
            ids=all_ids,
            metadatas=all_metadatas
        )
    else:
        print("No chunks to add.")

    print("Vector store setup complete.")
    return vector_store

# --- Main Test Function ---
async def run_complex_team():
    """Initializes and runs the complex agent team."""
    print("\n--- Running Complex RAG + MCP + Tools Team ---")

    # Initialize LLM Service (Choose one)
    llm_service = GroqService()
    # llm_service = GoogleOpenAICompatService() # Uncomment if using Google

    # Configure MCP Client
    mcp_server_configs = {
        "adder_server": SseServerConfig(url=HttpUrl("http://localhost:8000")),
        "subtract_server": SseServerConfig(url=HttpUrl("http://localhost:8001"))
    }
    mcp_manager = MCPClientManager(mcp_server_configs)

    # Setup Vector Store
    vector_store = await setup_vector_store()

    # Define the team
    try:
        with Team() as team:
            # Agent 1: Research + RAG + MCP Add
            researcher = Agent(
                name="Topic_Researcher",
                backstory="Expert researcher skilled in web search, internal knowledge retrieval (vector search), and basic calculations.",
                task_description="1. Search the web for 'challenges in multi-agent system design'. "
                                 "2. Query the internal vector store for information related to 'agent performance'. "
                                 "3. Use the remote 'add' tool to calculate 25 + 17.",
                task_expected_output="A combined summary of web search results, vector store findings on performance, and the result of the addition.",
                llm_service=llm_service,
                model="llama-3.3-70b-versatile", # Or your preferred model
                tools=[duckduckgo_search],       # Local tool
                vector_store=vector_store,       # RAG capability
                mcp_manager=mcp_manager,         # MCP capability
                mcp_server_names=["adder_server"] # Specific server for 'add'
            )

            # Agent 2: Calculations (Local + MCP Sub)
            calculator = Agent(
                name="Number_Cruncher",
                backstory="Specialist in performing calculations based on provided inputs.",
                task_description="1. Take the addition result from the Researcher (should be 42). "
                                 "2. Use the remote 'sub' tool to calculate Result - 12. "
                                 "3. Use the local 'multiply' tool to calculate the original numbers from the addition task (25 * 17).",
                task_expected_output="The result of the subtraction and the result of the multiplication.",
                llm_service=llm_service,
                model="llama-3.3-70b-versatile",
                tools=[multiply],                  # Local tool
                mcp_manager=mcp_manager,           # MCP capability
                mcp_server_names=["subtract_server"] # Specific server for 'sub'
            )

            # Agent 3: Reporting
            reporter = Agent(
                name="Report_Synthesizer",
                backstory="Skilled writer adept at summarizing complex information from multiple sources.",
                task_description="Compile all the information provided by the Researcher (web search, RAG findings, addition result) and the Calculator (subtraction result, multiplication result) into a single, concise final report paragraph.",
                task_expected_output="One paragraph summarizing all findings and calculations.",
                llm_service=llm_service,
                model="llama-3.3-70b-versatile"
                # No tools needed for this agent
            )

            # Define Dependencies
            researcher >> calculator >> reporter

            # Run the team
            await team.run()

            # Print final results from the team object
            print(json.dumps(team.results, indent=2, ensure_ascii=False))

    except Exception as e:
        print(f"\n--- An error occurred during team execution: {e} ---")
        import traceback
        traceback.print_exc()
    finally:
        # Ensure MCP connections are closed
        print("\n--- Cleaning up MCP connections ---")
        await mcp_manager.disconnect_all()
        # Clean up DB after run
        if os.path.exists(CHROMA_DB_PATH):
            print(f"\nCleaning up test database: {CHROMA_DB_PATH}")
            # shutil.rmtree(CHROMA_DB_PATH) # Uncomment to auto-delete DB

if __name__ == "__main__":
   
    asyncio.run(run_complex_team())




================================================
FILE: examples/rag_test.py
================================================

import asyncio
import os
import shutil
from dotenv import load_dotenv

# --- CLAP Imports ---
from clap import Agent, Team # Assuming Agent is modified as above
from clap.vector_stores.chroma_store import ChromaStore
from clap.utils.rag_utils import chunk_text_by_separator # Example chunker
from clap.llm_services.groq_service import GroqService 
from clap import GoogleOpenAICompatService


# Requires: pip install sentence-transformers
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
DEFAULT_EF = SentenceTransformerEmbeddingFunction()



# --- Config ---
load_dotenv()
CHROMA_DB_PATH = "./rag_test_chroma_db"
COLLECTION_NAME = "clap_rag_demo"

# --- Sample Data ---
SAMPLE_DOCUMENTS = [
    """Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that, through cellular respiration, can later be released to fuel the organisms' activities. This chemical energy is stored in carbohydrate molecules, such as sugars and starches, which are synthesized from carbon dioxide and water – hence the name photosynthesis, from the Greek φῶς, phos, "light", and σύνθεσις, synthesis, "putting together".""",
    """The Formula One World Championship, commonly known as Formula 1 or F1, is the highest class of international racing for open-wheel single-seater formula racing cars sanctioned by the Fédération Internationale de l'Automobile (FIA). The World Drivers' Championship, which became the FIA Formula One World Championship in 1981, has been one of the premier forms of racing around the world since its inaugural season in 1950.""",
    """The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Locally nicknamed "La dame de fer" (French for "Iron Lady"), it was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair."""
]

async def setup_vector_store():
    """Sets up the ChromaDB store and adds sample data."""
    print(f"Setting up ChromaDB at {CHROMA_DB_PATH}...")
    # Clean up previous run if exists
    if os.path.exists(CHROMA_DB_PATH):
        shutil.rmtree(CHROMA_DB_PATH)

    # Initialize ChromaStore
    vector_store = ChromaStore(
        path=CHROMA_DB_PATH,
        collection_name=COLLECTION_NAME,
        embedding_function=DEFAULT_EF
    )

    # Prepare data (Chunking is simple here, could use rag_utils)
    all_chunks = []
    all_ids = []
    all_metadatas = []
    doc_id_counter = 1
    chunk_id_counter = 1

    for doc in SAMPLE_DOCUMENTS:
        # Example: Simple chunking (could use rag_utils.chunk_text_...)
        chunks = chunk_text_by_separator(doc, separator=". ") # Split by sentence
        for chunk in chunks:
            if not chunk.strip(): continue
            chunk_id = f"doc{doc_id_counter}_chunk{chunk_id_counter}"
            all_chunks.append(chunk.strip() + ".") # Add back period for context
            all_ids.append(chunk_id)
            all_metadatas.append({"source_doc_id": f"doc{doc_id_counter}"})
            chunk_id_counter += 1
        doc_id_counter += 1
        chunk_id_counter = 1 # Reset chunk counter for next doc

    if all_chunks:
        await vector_store.add_documents(
            documents=all_chunks,
            ids=all_ids,
            metadatas=all_metadatas
        )
    else:
        print("No chunks to add.")

    print("Vector store setup complete.")
    return vector_store

async def run_rag_agent(vector_store: ChromaStore):
    # ... (agent initialization remains the same, BUT the initial task_description doesn't matter much now)
    llm_service = GroqService() # Use Gemini
    rag_agent = Agent(
        name="RAG_Expert",
        backstory="You are an AI assistant that answers questions based on provided context...",
        task_description="Placeholder - will be overwritten in loop", # Initial value doesn't matter
        task_expected_output="A concise answer based on retrieved context...",
        llm_service=llm_service,
        model="llama-3.3-70b-versatile", # Use a highly capable model
        vector_store=vector_store
    )

    queries = [
        "What is photosynthesis?",
        "Who designed the Eiffel Tower?",
        "What is the capital of Germany?",
        "Tell me about Formula 1 racing."
    ]

    # Using Team context manager even for a single agent is fine
    with Team() as team:
        for query in queries:
            print(f"\n--- User Query: {query} ---")
            rag_agent.received_context = {} # Reset context

            # --- THE FIX ---
            # Update the agent's task description to the *current* query
            rag_agent.task_description = query
            # --- END FIX ---

            # Now, when agent.run() calls create_prompt(), self.task_description will be correct
            result = await rag_agent.run()
            output_content = result.get("output", "Agent did not produce 'output' key.")
            print(output_content)
            await asyncio.sleep(1)

async def main():
    vector_store = await setup_vector_store()
    await run_rag_agent(vector_store)

    # Clean up DB after run
    if os.path.exists(CHROMA_DB_PATH):
         print(f"\nCleaning up test database: {CHROMA_DB_PATH}")
         shutil.rmtree(CHROMA_DB_PATH) # Uncomment to auto-delete DB

asyncio.run(main())




================================================
FILE: examples/react_test.py
================================================

import asyncio
import os
from dotenv import load_dotenv
from pydantic import HttpUrl 
from clap import ReactAgent
from clap import MCPClientManager, SseServerConfig
from clap import tool
from clap import GroqService

@tool
def multiply(a: int, b: int) -> int:
    """Subtracts integer b from integer a."""
    print(f"[Local Tool] Multiplying: {a} * {b}")
    return a * b

async def main():
    load_dotenv() 
    groq_llm_service = GroqService()

    server_configs = {
        "adder_server": SseServerConfig(url=HttpUrl("http://localhost:8000")),
        "subtract_server": SseServerConfig(url=HttpUrl("http://localhost:8001"))
    }
    manager = MCPClientManager(server_configs)

    agent = ReactAgent(
        llm_service=groq_llm_service,
        model="llama3-70b-8192",
        tools=[multiply], 
        mcp_manager=manager,
        mcp_server_names=["adder_server","subtract_server"] 
    )

    user_query = "Calculate ((15 + 7) - 5) * 2"
    response = await agent.run(user_msg=user_query)
    print(response)
    await manager.disconnect_all()

asyncio.run(main())


================================================
FILE: examples/requirements.txt
================================================
# Core asynchronous library
anyio>=4.5

colorama

python-dotenv

graphviz

groq
openai>=1.0.0 

httpx>=0.27
httpx-sse>=0.4

mcp>=1.2.0

# JSON Schema validation for tools
jsonschema

requests

crawl4ai

duckduckgo-search
pydantic>=2.7.2,<3.0.0
pydantic-settings>=2.5.2 




================================================
FILE: examples/scraping_test.py
================================================

import asyncio
import os
from dotenv import load_dotenv
import crawl4ai
from clap import ToolAgent
from clap.tools import scrape_url, extract_text_by_query
from clap.llm_services import GroqService

load_dotenv()

async def main():
    agent = ToolAgent(
        llm_service=GroqService(),
        tools=[scrape_url, extract_text_by_query], 
        model="llama-3.3-70b-versatile" 
    )
    query1 = "Can you scrape the content of https://docs.agno.com/introduction for me?"
    response1 = await agent.run(user_msg=query1)
    
    print(response1)
    

    await asyncio.sleep(1)

    query2 = "Can you look at https://docs.agno.com/introduction and tell me what it says about 'Agent Teams'?"
    response2 = await agent.run(user_msg=query2)
    print(response2)
    

    

asyncio.run(main())



================================================
FILE: examples/simple_react_agent.py
================================================
import asyncio
import os
from dotenv import load_dotenv
from clap import ReactAgent, tool, GroqService

load_dotenv() 
@tool
def get_word_length(word: str) -> int:
    """Calculates the length of a word."""
    print(f"[Local Tool] Calculating length of: {word}")
    return len(word)

async def main():
    groq_service = GroqService() 
    agent = ReactAgent(
        llm_service=groq_service,
        model="llama-3.3-70b-versatile", 
        tools=[get_word_length], 
        system_prompt="You are a helpful assistant." # Optional 
    )

    user_query = "How many letters are in the word 'framework'?"
    response = await agent.run(user_msg=user_query)
    
    print(response)
    
asyncio.run(main())


================================================
FILE: examples/team_test.py
================================================

import os
import asyncio # Import asyncio
from dotenv import load_dotenv
from clap import Team
from clap import Agent
from clap import duckduckgo_search

load_dotenv()

topic = "the benefits of Agile methodology in software development"

async def run_team():
    with Team() as team:
        researcher = Agent(
            name="Web_Researcher",
            backstory="You are an expert web researcher.",
            task_description=f"Search the web for information on '{topic}'.",
            task_expected_output="Raw search results.",
            tools=[duckduckgo_search]
        )

        summarizer = Agent(
            name="Content_Summarizer",
            backstory="You are an expert analyst.",
            task_description="Analyze the provided context (search results) and extract the main benefits.",
            task_expected_output="A concise bullet-point list summarizing key benefits."
        )

        reporter = Agent(
            name="Report_Writer",
            backstory="You are a skilled writer.",
            task_description="Take the summarized key points and write a short paragraph.",
            task_expected_output="A single paragraph summarizing the benefits."
        )

        researcher >> summarizer >> reporter

        await team.run()



asyncio.run(run_team())




================================================
FILE: examples/test_agent.py
================================================
import os
import asyncio 
from dotenv import load_dotenv
from clap import ToolAgent
from clap import duckduckgo_search
from clap import GroqService
from clap.utils.completions import GroqClient

load_dotenv()

async def main():
    agent = ToolAgent(llm_service=GroqService(),tools=duckduckgo_search, model="meta-llama/llama-4-maverick-17b-128e-instruct")
    user_query = "Search the web for recent news about AI advancements."
    response = await agent.run(user_msg=user_query)
    print(f"Response:\n{response}")

asyncio.run(main())


================================================
FILE: examples/tool_agent_rag.py
================================================
# --- START OF MODIFIED examples/tool_agent_rag.py ---
import asyncio
import os
import shutil
import time
import json
import uuid # <--- IMPORT THE uuid MODULE
from dotenv import load_dotenv

# ... (other imports like ToolAgent, OllamaOpenAICompatService, OllamaEmbeddings, QdrantStore, etc.)
from clap import ToolAgent
from clap.llm_services.ollama_service import OllamaOpenAICompatService
from clap.embedding.ollama_embedding import OllamaEmbeddings, KNOWN_OLLAMA_EMBEDDING_DIMENSIONS
from clap.vector_stores.qdrant_store import QdrantStore
from clap.utils.rag_utils import chunk_text_by_fixed_size
from qdrant_client import models as qdrant_models


load_dotenv()

# ... (Configuration constants remain the same) ...
OLLAMA_HOST = "http://localhost:11434"
OLLAMA_LLM_FOR_AGENT = "llama3.2:latest"
OLLAMA_MODEL_FOR_EMBEDDINGS = "nomic-embed-text"
RAG_DB_PATH = "./ollama_toolagent_rag_db"
RAG_COLLECTION_NAME = "toolagent_rag_docs"
RAG_CHUNK_SIZE, RAG_CHUNK_OVERLAP = 200, 20
SAMPLE_DOCS = [
    "The ToolAgent can now perform RAG by querying a vector store.",
    "Vector query results are passed as observations back to the LLM for a final answer.",
    "This allows simpler agents to access knowledge bases without a full ReAct loop."
]



async def main():
    print_section_header("ToolAgent - RAG Test")
    if OLLAMA_MODEL_FOR_EMBEDDINGS not in KNOWN_OLLAMA_EMBEDDING_DIMENSIONS:
        print(f"ERROR: Dimension for Ollama embed model '{OLLAMA_MODEL_FOR_EMBEDDINGS}' unknown."); return

    ollama_ef = OllamaEmbeddings(model_name=OLLAMA_MODEL_FOR_EMBEDDINGS, ollama_host=OLLAMA_HOST)
    if os.path.exists(RAG_DB_PATH): shutil.rmtree(RAG_DB_PATH)
    vector_store = await QdrantStore.create(
        collection_name=RAG_COLLECTION_NAME, embedding_function=ollama_ef,
        path=RAG_DB_PATH, recreate_collection_if_exists=True,
        distance_metric=qdrant_models.Distance.COSINE
    )
    chunks = []; # ids = []; # No longer needed here if generating UUIDs per chunk
    all_ids = [] # Use a new list for UUIDs
    metadatas = []

    doc_counter = 0
    for i, doc_text_content in enumerate(SAMPLE_DOCS): # Renamed 'doc' to avoid conflict if you use 'doc' later
        doc_chunks = chunk_text_by_fixed_size(doc_text_content, RAG_CHUNK_SIZE, RAG_CHUNK_OVERLAP)
        for j, chunk in enumerate(doc_chunks):
            chunks.append(chunk)
            # --- MODIFIED ID GENERATION ---
            all_ids.append(str(uuid.uuid4())) # Generate a new UUID for each chunk
            # --- END MODIFICATION ---
            metadatas.append({"source": f"sample_doc_{i}", "original_chunk_id": f"doc{i}_chunk{j}"})
        doc_counter +=1

    if chunks: await vector_store.add_documents(documents=chunks, ids=all_ids, metadatas=metadatas)
    print(f"Ingested {len(chunks)} chunks.")

    ollama_llm_service = OllamaOpenAICompatService(default_model=OLLAMA_LLM_FOR_AGENT, base_url=f"{OLLAMA_HOST}/v1")
    
    agent = ToolAgent(
        llm_service=ollama_llm_service,
        model=OLLAMA_LLM_FOR_AGENT,
        vector_store=vector_store
    )
    
    query = "How can ToolAgent perform RAG?"
    response = await agent.run(user_msg=query)
    print("ToolAgent with RAG", query, response)

    await vector_store.close()
    await ollama_llm_service.close()
    if os.path.exists(RAG_DB_PATH): shutil.rmtree(RAG_DB_PATH)

def print_section_header(title): print(f"\n{'='*20} {title.upper()} {'='*20}")

if __name__ == "__main__":
    print(f"Ensure Ollama (models: {OLLAMA_LLM_FOR_AGENT}, {OLLAMA_MODEL_FOR_EMBEDDINGS}) is running.")
    asyncio.run(main())

# --- END OF MODIFIED examples/tool_agent_rag.py ---


================================================
FILE: src/clap/__init__.py
================================================
from .multiagent_pattern.agent import Agent
from .multiagent_pattern.team import Team
from .react_pattern.react_agent import ReactAgent
from .tool_pattern.tool_agent import ToolAgent
from .tool_pattern.tool import tool, Tool

from .llm_services.base import LLMServiceInterface, StandardizedLLMResponse, LLMToolCall
from .llm_services.groq_service import GroqService
from .llm_services.google_openai_compat_service import GoogleOpenAICompatService

from .embedding.base_embedding import EmbeddingFunctionInterface

from .vector_stores.base import VectorStoreInterface, QueryResult

from .mcp_client.client import MCPClientManager, SseServerConfig

from .tools.web_search import duckduckgo_search


__all__ = [
    "Agent", "Team", "ReactAgent", "ToolAgent", "Tool", "tool",
    "LLMServiceInterface", "StandardizedLLMResponse", "LLMToolCall",
    "GroqService", "GoogleOpenAICompatService",
    "EmbeddingFunctionInterface", "SentenceTransformerEmbeddings",
    "VectorStoreInterface", "QueryResult",
    "MCPClientManager", "SseServerConfig",
    "duckduckgo_search",
]


================================================
FILE: src/clap/embedding/__init__.py
================================================
from .base_embedding import EmbeddingFunctionInterface

__all__ = ["EmbeddingFunctionInterface"]

try:
    from .sentence_transformer_embedding import SentenceTransformerEmbeddings
    __all__.append("SentenceTransformerEmbeddings")
except ImportError:
    pass

try:
    from .fastembed_embedding import FastEmbedEmbeddings
    __all__.append("FastEmbedEmbeddings")
except ImportError:
    pass

try:
    from .ollama_embedding import OllamaEmbeddings
    __all__.append("OllamaEmbeddings")
except ImportError:
    pass


================================================
FILE: src/clap/embedding/base_embedding.py
================================================
import abc
from typing import List, Protocol

class EmbeddingFunctionInterface(Protocol):
    """
    A protocol for embedding functions to ensure they can provide
    their output dimensionality and embed documents.
    """

    @abc.abstractmethod
    def __call__(self, input: List[str]) -> List[List[float]]:
        """
        Embeds a list of texts.

        Args:
            input: A list of document texts.

        Returns:
            A list of embeddings (list of floats).
        """
        ...

    @abc.abstractmethod
    def get_embedding_dimension(self) -> int:
        """
        Returns the dimensionality of the embeddings produced by this function.
        """
        ...


================================================
FILE: src/clap/embedding/fastembed_embedding.py
================================================
import asyncio
import functools
from typing import List, Optional, Any, cast

import anyio

from .base_embedding import EmbeddingFunctionInterface

_FASTEMBED_LIB_AVAILABLE = False
_FastEmbed_TextEmbedding_Placeholder_Type = Any 

try:
    from fastembed import TextEmbedding as ActualTextEmbedding
    _FastEmbed_TextEmbedding_Placeholder_Type = ActualTextEmbedding
    _FASTEMBED_LIB_AVAILABLE = True
except ImportError:
    pass

KNOWN_FASTEMBED_DIMENSIONS = {
    "BAAI/bge-small-en-v1.5": 384,
    "sentence-transformers/all-MiniLM-L6-v2": 384,
}
DEFAULT_FASTEMBED_MODEL = "BAAI/bge-small-en-v1.5"

class FastEmbedEmbeddings(EmbeddingFunctionInterface):
    _model: _FastEmbed_TextEmbedding_Placeholder_Type
    _dimension: int
    DEFAULT_EMBED_BATCH_SIZE = 256

    def __init__(self,
                 model_name: str = DEFAULT_FASTEMBED_MODEL,
                 dimension: Optional[int] = None,
                 embed_batch_size: int = DEFAULT_EMBED_BATCH_SIZE,
                 **kwargs: Any
                ):
        if not _FASTEMBED_LIB_AVAILABLE:
            raise ImportError(
                "The 'fastembed' library is required to use FastEmbedEmbeddings. "
                "Install with 'pip install fastembed' or 'pip install \"clap-agents[qdrant]\"' (if qdrant includes it as an extra)."
            )

        self.model_name = model_name
        self.embed_batch_size = embed_batch_size

        if dimension is not None:
            self._dimension = dimension
        elif model_name in KNOWN_FASTEMBED_DIMENSIONS:
            self._dimension = KNOWN_FASTEMBED_DIMENSIONS[model_name]
        else:
            raise ValueError(
                f"Dimension for fastembed model '{self.model_name}' is unknown. "
                "Provide 'dimension' parameter or update KNOWN_FASTEMBED_DIMENSIONS."
            )
        
        try:
            self._model = _FastEmbed_TextEmbedding_Placeholder_Type(model_name=self.model_name, **kwargs)
        except Exception as e:
            raise RuntimeError(f"Failed to initialize fastembed model '{self.model_name}': {e}")

    async def __call__(self, input: List[str]) -> List[List[float]]: # Changed to 'input'
        if not input: return []
        if not _FASTEMBED_LIB_AVAILABLE: raise RuntimeError("FastEmbed library not available.")
        
        all_embeddings_list: List[List[float]] = []
        for i in range(0, len(input), self.embed_batch_size):
            batch_texts = input[i:i + self.embed_batch_size]
            if not batch_texts: continue
            try:
                embeddings_iterable = await anyio.to_thread.run_sync(self._model.embed, list(batch_texts))
                for emb_np in embeddings_iterable: all_embeddings_list.append(emb_np.tolist())
            except Exception as e: print(f"Error embedding batch with fastembed: {e}"); raise
        return all_embeddings_list

    def get_embedding_dimension(self) -> int:
        return self._dimension



================================================
FILE: src/clap/embedding/ollama_embedding.py
================================================
import asyncio
import functools
from typing import List, Optional, Any, cast

import anyio

from .base_embedding import EmbeddingFunctionInterface

_OLLAMA_LIB_AVAILABLE = False
_OllamaAsyncClient_Placeholder_Type = Any
_OllamaResponseError_Placeholder_Type = type(Exception)

try:
    from ollama import AsyncClient as ImportedOllamaAsyncClient
    from ollama import ResponseError as ImportedOllamaResponseError
    _OllamaAsyncClient_Placeholder_Type = ImportedOllamaAsyncClient
    _OllamaResponseError_Placeholder_Type = ImportedOllamaResponseError
    _OLLAMA_LIB_AVAILABLE = True
except ImportError:
    pass

KNOWN_OLLAMA_EMBEDDING_DIMENSIONS = {
    "nomic-embed-text": 768, "mxbai-embed-large": 1024, "all-minilm": 384,
    "llama3": 4096, "llama3.2:latest": 4096, "nomic-embed-text:latest": 768,
}
DEFAULT_OLLAMA_EMBED_MODEL = "nomic-embed-text"

class OllamaEmbeddings(EmbeddingFunctionInterface):
    _client: _OllamaAsyncClient_Placeholder_Type
    _model_name: str
    _dimension: int

    def __init__(self,
                 model_name: str = DEFAULT_OLLAMA_EMBED_MODEL,
                 dimension: Optional[int] = None,
                 ollama_host: str = "http://localhost:11434",
                 **kwargs: Any):
        if not _OLLAMA_LIB_AVAILABLE:
            raise ImportError("The 'ollama' Python library is required. Install with: pip install 'clap-agents[ollama]'")

        self.model_name = model_name
        self._client = _OllamaAsyncClient_Placeholder_Type(host=ollama_host, **kwargs)

        if dimension is not None: self._dimension = dimension
        elif model_name in KNOWN_OLLAMA_EMBEDDING_DIMENSIONS:
            self._dimension = KNOWN_OLLAMA_EMBEDDING_DIMENSIONS[model_name]
        else:
            raise ValueError(f"Dimension for Ollama model '{model_name}' unknown. Provide 'dimension' or update KNOWN_OLLAMA_EMBEDDING_DIMENSIONS.")
        print(f"Initialized OllamaEmbeddings for model '{self.model_name}' (dim: {self._dimension}).")

    async def __call__(self, input: List[str]) -> List[List[float]]: 
        if not input: return []
        if not _OLLAMA_LIB_AVAILABLE: raise RuntimeError("Ollama library not available.")
        try:
            response = await self._client.embed(model=self.model_name, input=input) 
            embeddings_data = response.get("embeddings")
            if embeddings_data is None and len(input) == 1 and response.get("embedding"):
                single_embedding = response.get("embedding")
                if isinstance(single_embedding, list) and all(isinstance(x, (int, float)) for x in single_embedding):
                    embeddings_data = [single_embedding]
            if not isinstance(embeddings_data, list) or not all(isinstance(e, list) for e in embeddings_data):
                raise TypeError(f"Ollama embed returned unexpected format. Expected List[List[float]]. Resp: {response}")
            return cast(List[List[float]], embeddings_data)
        except _OllamaResponseError_Placeholder_Type as e:
            print(f"Ollama API error: {getattr(e, 'error', str(e))} (Status: {getattr(e, 'status_code', 'N/A')})")
            raise
        except Exception as e: print(f"Unexpected Ollama embedding error: {e}"); raise

    def get_embedding_dimension(self) -> int: return self._dimension

    async def close(self):
        if _OLLAMA_LIB_AVAILABLE:
            if hasattr(self._client, "_client") and hasattr(self._client._client, "is_closed"):
                if not self._client._client.is_closed: await self._client._client.aclose() 
            elif hasattr(self._client, 'aclose'): await self._client.aclose() 
        print(f"OllamaEmbeddings: Closed client for {self.model_name}.")



================================================
FILE: src/clap/embedding/sentence_transformer_embedding.py
================================================
from typing import List, Optional, Any 
from .base_embedding import EmbeddingFunctionInterface

_ST_LIB_AVAILABLE = False
_SentenceTransformer_Placeholder_Type = Any 

try:
    from sentence_transformers import SentenceTransformer as ImportedSentenceTransformer
    _SentenceTransformer_Placeholder_Type = ImportedSentenceTransformer
    _ST_LIB_AVAILABLE = True
except ImportError:
    pass

class SentenceTransformerEmbeddings(EmbeddingFunctionInterface):
    model: _SentenceTransformer_Placeholder_Type
    _dimension: int

    def __init__(self, model_name: str = "all-MiniLM-L6-v2", device: Optional[str] = None):
        if not _ST_LIB_AVAILABLE:
            raise ImportError(
                "The 'sentence-transformers' library is required to use SentenceTransformerEmbeddings. "
                "Install with 'pip install sentence-transformers' or 'pip install \"clap-agents[sentence-transformers]\"'."
            )
        
        try:
            self.model = _SentenceTransformer_Placeholder_Type(model_name, device=device)
            dim = self.model.get_sentence_embedding_dimension() 
            if dim is None: 
                dummy_embedding = self.model.encode("test") 
                dim = len(dummy_embedding)
            self._dimension = dim
        except Exception as e:
            raise RuntimeError(f"Failed to initialize SentenceTransformer model '{model_name}': {e}")



    def __call__(self, input: List[str]) -> List[List[float]]: 
        if not _ST_LIB_AVAILABLE: 
            raise RuntimeError("SentenceTransformers library not available for embedding operation.")
        embeddings_np = self.model.encode(input, convert_to_numpy=True) 
        return embeddings_np.tolist()

    def get_embedding_dimension(self) -> int:
        return self._dimension



================================================
FILE: src/clap/llm_services/__init__.py
================================================
# src/clap/llm_services/__init__.py
from .base import LLMServiceInterface, StandardizedLLMResponse, LLMToolCall
from .groq_service import GroqService
from .google_openai_compat_service import GoogleOpenAICompatService

__all__ = [
    "LLMServiceInterface", "StandardizedLLMResponse", "LLMToolCall",
    "GroqService", "GoogleOpenAICompatService",
]

try:
    from .ollama_service import OllamaOpenAICompatService as OllamaService # Assuming file is ollama_service.py
    __all__.append("OllamaService")
except ImportError:
    pass


================================================
FILE: src/clap/llm_services/base.py
================================================

import abc
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union



@dataclass
class LLMToolCall:
    """Represents a tool call requested by the LLM."""
    id: str 
    function_name: str
    function_arguments_json_str: str

@dataclass
class StandardizedLLMResponse:
    """A consistent format for LLM responses passed back to the agent."""
    text_content: Optional[str] = None
    tool_calls: List[LLMToolCall] = field(default_factory=list)


class LLMServiceInterface(abc.ABC):
    """
    Abstract Base Class defining the interface for interacting with different LLM backends.
    Concrete implementations (e.g., for Groq, Google GenAI) will inherit from this.
    """

    @abc.abstractmethod
    async def get_llm_response(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: str = "auto",
        # Optional: 
        # temperature: Optional[float] = None,
        # max_tokens: Optional[int] = None,
    ) -> StandardizedLLMResponse:
        """
        Sends messages to the configured LLM backend and returns a standardized response.

        Args:
            model: The specific model identifier for the backend.
            messages: The chat history in a list of dictionaries format
                      (e.g., [{'role': 'user', 'content': 'Hello'}]).
                      Implementations will need to translate this if their
                      native API uses a different format.
            tools: A list of tool schemas available for the LLM to use,
                   formatted according to the OpenAI/Groq standard
                   `{"type": "function", "function": {...}}`.
                   Implementations will need to translate this if their
                   native API uses a different format.
            tool_choice: How the LLM should use tools (e.g., "auto", "none").

        Returns:
            A StandardizedLLMResponse object containing the text content and/or
            tool calls requested by the LLM.

        Raises:
            Exception: Can raise exceptions if the API call fails.
        """
        pass

   




================================================
FILE: src/clap/llm_services/google_openai_compat_service.py
================================================
# --- START OF agentic_patterns/llm_services/google_openai_compat_service.py ---

import os
import json
import uuid 
from typing import Any, Dict, List, Optional

try:
    from openai import AsyncOpenAI, OpenAIError
except ImportError:
    raise ImportError("OpenAI SDK not found. Please install it using: pip install openai")

from colorama import Fore

from .base import LLMServiceInterface, StandardizedLLMResponse, LLMToolCall

GOOGLE_COMPAT_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"

class GoogleOpenAICompatService(LLMServiceInterface):
    """
    LLM Service implementation using the OpenAI SDK configured for Google's
    Generative Language API (Gemini models via compatibility layer).
    """

    def __init__(self, api_key: Optional[str] = None, base_url: str = GOOGLE_COMPAT_BASE_URL):
        """
        Initializes the service using the OpenAI client pointed at Google's endpoint.

        Args:
            api_key: Optional Google API key. If None, uses GOOGLE_API_KEY env var.
            base_url: The base URL for the Google compatibility endpoint.
        """
        effective_key = api_key or os.getenv("GOOGLE_API_KEY")
        if not effective_key:
            raise ValueError("Google API Key not provided or found in environment variables (GOOGLE_API_KEY).")

        try:
            self.client = AsyncOpenAI(
                api_key=effective_key,
                base_url=base_url,
            )
        except Exception as e:
            print(f"{Fore.RED}Failed to initialize OpenAI client for Google: {e}{Fore.RESET}")
            raise

    async def get_llm_response(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: str = "auto",
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -> StandardizedLLMResponse:
        """
        Sends messages via the OpenAI SDK (to Google's endpoint) and returns a standardized response.

        Args:
            model: The Google model identifier (e.g., "gemini-1.5-flash").
            messages: Chat history in the OpenAI dictionary format.
            tools: Tool schemas in the OpenAI function format.
            tool_choice: Tool choice setting ("auto", "none", etc.).
            temperature: Sampling temperature.
            max_tokens: Max output tokens.

        Returns:
            A StandardizedLLMResponse object.

        Raises:
            OpenAIError: If the API call fails.
            Exception: For other unexpected errors.
        """
        try:
            api_kwargs = {
                "messages": messages,
                "model": model,
                "tool_choice": tool_choice if tools else None,
                "tools": tools if tools else None,
            }
            if temperature is not None: api_kwargs["temperature"] = temperature
            if max_tokens is not None: api_kwargs["max_tokens"] = max_tokens
            api_kwargs = {k: v for k, v in api_kwargs.items() if v is not None}


            response = await self.client.chat.completions.create(**api_kwargs)

            message = response.choices[0].message
            text_content = message.content
            tool_calls: List[LLMToolCall] = []

            if message.tool_calls:
                for tc in message.tool_calls:
                    tool_call_id = getattr(tc, 'id', None)
                    if not tool_call_id:
                        tool_call_id = f"compat_call_{uuid.uuid4().hex[:6]}" 
                        print(f"{Fore.YELLOW}Warning: Tool call from Google compat layer missing ID. Generated fallback: {tool_call_id}{Fore.RESET}")

                    if tc.function:
                        tool_calls.append(
                            LLMToolCall(
                                id=tool_call_id,
                                function_name=tc.function.name,
                                function_arguments_json_str=tc.function.arguments
                            )
                        )

            return StandardizedLLMResponse(
                text_content=text_content,
                tool_calls=tool_calls
            )

        except OpenAIError as e:
            print(f"{Fore.RED}Google (via OpenAI Compat Layer) API Error: {e}{Fore.RESET}")
            raise
        except Exception as e:
            print(f"{Fore.RED}Error calling Google (via OpenAI Compat Layer) LLM API: {e}{Fore.RESET}")
            raise




================================================
FILE: src/clap/llm_services/groq_service.py
================================================

from typing import Any, Dict, List, Optional

from groq import AsyncGroq, GroqError 
from colorama import Fore 

from .base import LLMServiceInterface, StandardizedLLMResponse, LLMToolCall

class GroqService(LLMServiceInterface):
    """LLM Service implementation for the Groq API."""

    def __init__(self, client: Optional[AsyncGroq] = None):
        """
        Initializes the Groq service.

        Args:
            client: An optional pre-configured AsyncGroq client.
                    If None, a new client will be created using environment variables.
        """
        self.client = client or AsyncGroq()
        # Add any other Groq-specific initialization here if needed

    async def get_llm_response(
        self,
        model: str,
        messages: List[Dict[str, Any]],
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: str = "auto",
        # Add other relevant Groq parameters if desired, e.g., temperature, max_tokens
        # temperature: Optional[float] = None,
        # max_tokens: Optional[int] = None,
    ) -> StandardizedLLMResponse:
        """
        Sends messages to the Groq API and returns a standardized response.

        Args:
            model: The Groq model identifier (e.g., "llama-3.3-70b-versatile").
            messages: Chat history in the OpenAI/Groq dictionary format.
            tools: Tool schemas in the OpenAI/Groq function format.
            tool_choice: Tool choice setting ("auto", "none", etc.).

        Returns:
            A StandardizedLLMResponse object.

        Raises:
            GroqError: If the API call fails.
            Exception: For other unexpected errors.
        """
        try:
            api_kwargs = {
                "messages": messages,
                "model": model,
                # Pass other parameters if added to method signature
                # "temperature": temperature,
                # "max_tokens": max_tokens,
            }
            if tools:
                api_kwargs["tools"] = tools
                api_kwargs["tool_choice"] = tool_choice

            response = await self.client.chat.completions.create(**api_kwargs)

            message = response.choices[0].message
            text_content = message.content
            tool_calls: List[LLMToolCall] = []

            if message.tool_calls:
                for tc in message.tool_calls:
                    if tc.function: 
                        tool_calls.append(
                            LLMToolCall(
                                id=tc.id,
                                function_name=tc.function.name,
                                function_arguments_json_str=tc.function.arguments
                            )
                        )

            # Return the standardized response
            return StandardizedLLMResponse(
                text_content=text_content,
                tool_calls=tool_calls
            )

        except GroqError as e:
            print(f"{Fore.RED}Groq API Error: {e}{Fore.RESET}")
            
            raise
        except Exception as e:
            print(f"{Fore.RED}Error calling Groq LLM API: {e}{Fore.RESET}")
            
            raise




================================================
FILE: src/clap/llm_services/ollama_service.py
================================================
import os
import json
import uuid
from typing import Any, Dict, List, Optional

_OPENAI_LIB_AVAILABLE = False
_AsyncOpenAI_Placeholder_Type = Any
_OpenAIError_Placeholder_Type = type(Exception)

try:
    from openai import AsyncOpenAI as ImportedAsyncOpenAI, OpenAIError as ImportedOpenAIError
    _AsyncOpenAI_Placeholder_Type = ImportedAsyncOpenAI
    _OpenAIError_Placeholder_Type = ImportedOpenAIError
    _OPENAI_LIB_AVAILABLE = True
except ImportError:
    pass

from colorama import Fore
from .base import LLMServiceInterface, StandardizedLLMResponse, LLMToolCall

OLLAMA_OPENAI_COMPAT_BASE_URL = "http://localhost:11434/v1"

class OllamaOpenAICompatService(LLMServiceInterface): 
    """
    LLM Service implementation using the OpenAI SDK configured for a
    local Ollama instance's OpenAI-compatible API.
    """
    _client: _AsyncOpenAI_Placeholder_Type

    def __init__(
        self,
        base_url: str = OLLAMA_OPENAI_COMPAT_BASE_URL,
        api_key: str = "ollama", # Dummy
        default_model: Optional[str] = None
    ):

        """
        Initializes the service using the OpenAI client pointed at Ollama.

        Args:
            base_url: The base URL for the Ollama OpenAI compatibility endpoint.
            api_key: Dummy API key for the OpenAI client (Ollama ignores it).
            default_model: Optional default Ollama model name to use if not specified in calls.
        """
        if not _OPENAI_LIB_AVAILABLE:
            raise ImportError(
                "The 'openai' Python library is required to use OllamaOpenAICompatService. "
                "Install with 'pip install openai' or 'pip install \"clap-agents[ollama]\"' (if ollama extra includes openai)."
            )
        self.default_model = default_model
        try:
            self._client = _AsyncOpenAI_Placeholder_Type(base_url=base_url, api_key=api_key)
            # print(f"OllamaService: Initialized OpenAI client for Ollama at {base_url}")
        except Exception as e:
            print(f"{Fore.RED}Failed to initialize OpenAI client for Ollama: {e}{Fore.RESET}"); raise

    async def get_llm_response(self, model: str, messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None, tool_choice: str = "auto", temperature: Optional[float] = None, max_tokens: Optional[int] = None) -> StandardizedLLMResponse:
        """
        Sends messages via the OpenAI SDK (to Ollama's endpoint)
        and returns a standardized response.
        """
        if not _OPENAI_LIB_AVAILABLE: raise RuntimeError("OpenAI library not available for Ollama service.")
        request_model = model or self.default_model
        if not request_model: raise ValueError("Ollama model name not specified.")
        try:
            api_kwargs: Dict[str, Any] = {"messages": messages, "model": request_model}
            if tools and tool_choice != "none":
                api_kwargs["tools"] = tools
                if isinstance(tool_choice, dict) or tool_choice in ["auto", "required", "none"]: api_kwargs["tool_choice"] = tool_choice
            else: api_kwargs["tools"] = None; api_kwargs["tool_choice"] = None
            if temperature is not None: api_kwargs["temperature"] = temperature
            if max_tokens is not None: api_kwargs["max_tokens"] = max_tokens
            api_kwargs = {k: v for k, v in api_kwargs.items() if v is not None}
            # print(f"OllamaService: Sending request to model '{request_model}'")
            response = await self._client.chat.completions.create(**api_kwargs)
            message = response.choices[0].message
            text_content = message.content
            tool_calls_std: List[LLMToolCall] = []
            if message.tool_calls:
                for tc in message.tool_calls:
                    if tc.id and tc.function and tc.function.name and tc.function.arguments is not None:
                        tool_calls_std.append(LLMToolCall(id=tc.id, function_name=tc.function.name, function_arguments_json_str=tc.function.arguments))
                    else: print(f"{Fore.YELLOW}Warning: Incomplete tool_call from Ollama: {tc}{Fore.RESET}")
            return StandardizedLLMResponse(text_content=text_content, tool_calls=tool_calls_std)
        except _OpenAIError_Placeholder_Type as e: # Use placeholder
            err_msg = f"Ollama (OpenAI Compat) API Error: {e}"
            if hasattr(e, 'response') and e.response and hasattr(e.response, 'text'): err_msg += f" - Details: {e.response.text}"
            print(f"{Fore.RED}{err_msg}{Fore.RESET}")
            return StandardizedLLMResponse(text_content=err_msg)
        except Exception as e:
            print(f"{Fore.RED}Unexpected error with Ollama (OpenAI Compat): {e}{Fore.RESET}")
            return StandardizedLLMResponse(text_content=f"Ollama Unexpected Error: {e}")


    async def close(self):
        if _OPENAI_LIB_AVAILABLE and hasattr(self, '_client') and self._client:
            if hasattr(self._client, "close"): await self._client.close() # For openai >1.0
            elif hasattr(self._client, "_client") and hasattr(self._client._client, "is_closed"): # For httpx client in openai <1.0
                 if not self._client._client.is_closed: await self._client._client.aclose() # type: ignore
        # print("OllamaService: Client closed.")
# --- END OF FILE ---


================================================
FILE: src/clap/mcp_client/__init__.py
================================================



================================================
FILE: src/clap/mcp_client/client.py
================================================

import asyncio
import json
from contextlib import AsyncExitStack
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, HttpUrl 

from mcp import ClientSession, types
from mcp.client.sse import sse_client
from colorama import Fore

class SseServerConfig(BaseModel):
    """Configuration for connecting to an MCP server via SSE."""
    url: HttpUrl = Field(description="The base URL of the MCP SSE server.")
    headers: Optional[Dict[str, str]] = Field(default=None, description="Optional headers for the connection.")

class MCPClientManager:
    """
    Manages connections and interactions with multiple MCP servers via SSE.

    Handles connecting, disconnecting, listing tools, and calling tools on
    configured MCP servers accessible over HTTP/S.
    """

    def __init__(self, server_configs: Dict[str, SseServerConfig]):
        """
        Initializes the manager with SSE server configurations.

        Args:
            server_configs: A dictionary where keys are logical server names
                            and values are SseServerConfig objects.
        """
        if not isinstance(server_configs, dict):
             raise TypeError("server_configs must be a dictionary.")
        self.server_configs = server_configs
        self.sessions: Dict[str, ClientSession] = {}
        self.exit_stacks: Dict[str, AsyncExitStack] = {}
        self._connect_locks: Dict[str, asyncio.Lock] = {
             name: asyncio.Lock() for name in server_configs
        }
        self._manager_lock = asyncio.Lock() # General lock for manager state

    async def _ensure_connected(self, server_name: str):
        """
        Ensures a connection via SSE to the specified server is active.

        Args:
            server_name: The logical name of the server to connect to.

        Raises:
            ValueError: If the server configuration is not found or URL is invalid.
            RuntimeError: If connection or initialization fails.
        """
        if server_name in self.sessions:
            return

        connect_lock = self._connect_locks.get(server_name)
        if not connect_lock:
             raise ValueError(f"Configuration or lock for server '{server_name}' not found.")

        async with connect_lock:
            if server_name in self.sessions:
                return

            config = self.server_configs.get(server_name)
            if not config:
                raise ValueError(f"Configuration for server '{server_name}' not found.")

            print(f"{Fore.YELLOW}Attempting to connect to MCP server via SSE: {server_name} at {config.url}{Fore.RESET}")

            # Construct the specific SSE endpoint URL (often /sse)
            sse_url = str(config.url).rstrip('/') + "/sse" 

            exit_stack = AsyncExitStack()
            try:
                
                sse_transport = await exit_stack.enter_async_context(
                    sse_client(url=sse_url, headers=config.headers)
                )
                read_stream, write_stream = sse_transport

                
                session = await exit_stack.enter_async_context(
                    ClientSession(read_stream, write_stream)
                )

                
                await session.initialize()

                async with self._manager_lock:
                    self.sessions[server_name] = session
                    self.exit_stacks[server_name] = exit_stack
                print(f"{Fore.GREEN}Successfully connected to MCP server via SSE: {server_name}{Fore.RESET}")

            except Exception as e:
                await exit_stack.aclose()
                print(f"{Fore.RED}Failed to connect to MCP server '{server_name}' via SSE: {e}{Fore.RESET}")
                raise RuntimeError(f"SSE connection to '{server_name}' failed.") from e

    async def disconnect(self, server_name: str):
        """
        Disconnects from a specific server and cleans up resources.

        Args:
            server_name: The logical name of the server to disconnect from.
        """
        async with self._manager_lock:
             if server_name in self.sessions:
                 print(f"{Fore.YELLOW}Disconnecting from MCP server: {server_name}...{Fore.RESET}")
                 exit_stack = self.exit_stacks.pop(server_name)
                 del self.sessions[server_name]
                 await exit_stack.aclose()
                 print(f"{Fore.GREEN}Disconnected from MCP server: {server_name}{Fore.RESET}")

        
    async def disconnect_all(self):
        server_names = list(self.sessions.keys())
        print(f"{Fore.YELLOW}MCPClientManager: Disconnecting from all servers ({len(server_names)})...{Fore.RESET}")
        for name in server_names:
            try:
                await self.disconnect(name)
            except Exception as e:
                print(f"{Fore.RED}MCPClientManager: Error during disconnect of '{name}': {e}{Fore.RESET}")
        print(f"{Fore.GREEN}MCPClientManager: Finished disconnecting all servers.{Fore.RESET}")

    async def list_remote_tools(self, server_name: str) -> List[types.Tool]:
        """
        Lists tools available on a specific connected SSE server.

        Args:
            server_name: The logical name of the server.

        Returns:
            A list of mcp.types.Tool objects provided by the server.
        """
        await self._ensure_connected(server_name)
        session = self.sessions.get(server_name)
        if not session:
             raise RuntimeError(f"Failed to get session for '{server_name}' after ensuring connection.")

        try:
            print(f"{Fore.CYAN}Listing tools for server: {server_name}...{Fore.RESET}")
            tool_list_result = await session.list_tools()
            print(f"{Fore.CYAN}Found {len(tool_list_result.tools)} tools on {server_name}.{Fore.RESET}")
            return tool_list_result.tools
        except Exception as e:
            print(f"{Fore.RED}Error listing tools for server '{server_name}': {e}{Fore.RESET}")
            raise RuntimeError(f"Failed to list tools for '{server_name}'.") from e

    async def call_remote_tool(
        self, server_name: str, tool_name: str, arguments: Dict[str, Any]
    ) -> str:
        """
        Calls a tool on a specific connected SSE server.

        Args:
            server_name: The logical name of the server.
            tool_name: The name of the tool to call.
            arguments: A dictionary of arguments for the tool.

        Returns:
            A string representation of the tool's result content.
        """
        await self._ensure_connected(server_name)
        session = self.sessions.get(server_name)
        if not session:
             raise RuntimeError(f"Failed to get session for '{server_name}' after ensuring connection.")

        print(f"{Fore.CYAN}Calling remote tool '{tool_name}' on server '{server_name}' with args: {arguments}{Fore.RESET}")
        try:
             result: types.CallToolResult = await session.call_tool(tool_name, arguments)

             if result.isError:
                 error_content = result.content[0] if result.content else None
                 error_text = getattr(error_content, 'text', 'Unknown tool error')
                 print(f"{Fore.RED}MCP Tool '{tool_name}' on server '{server_name}' returned an error: {error_text}{Fore.RESET}")
                 raise RuntimeError(f"Tool call error on {server_name}.{tool_name}: {error_text}")
             else:
                  response_parts = []
                  for content_item in result.content:
                      if isinstance(content_item, types.TextContent):
                          response_parts.append(content_item.text)
                      elif isinstance(content_item, types.ImageContent):
                           response_parts.append(f"[Image Content Received: {content_item.mimeType}]")
                      elif isinstance(content_item, types.EmbeddedResource):
                           response_parts.append(f"[Embedded Resource Received: {content_item.resource.uri}]")
                      else:
                           response_parts.append(f"[Unsupported content type: {getattr(content_item, 'type', 'unknown')}]")
                  combined_response = "\n".join(response_parts)
                  print(f"{Fore.GREEN}Tool '{tool_name}' result from '{server_name}': {combined_response[:100]}...{Fore.RESET}")
                  return combined_response

        except Exception as e:
             print(f"{Fore.RED}Error calling tool '{tool_name}' on server '{server_name}': {e}{Fore.RESET}")
             raise RuntimeError(f"Failed to call tool '{tool_name}' on '{server_name}'.") from e



================================================
FILE: src/clap/multiagent_pattern/__init__.py
================================================



================================================
FILE: src/clap/multiagent_pattern/agent.py
================================================

import asyncio
import json
from textwrap import dedent
from typing import Any, List, Optional, Dict 

from clap.llm_services.base import LLMServiceInterface
from clap.llm_services.groq_service import GroqService 

from clap.mcp_client.client import MCPClientManager

from clap.tool_pattern.tool import Tool

from clap.vector_stores.base import VectorStoreInterface




VECTOR_QUERY_TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "vector_query",
        "description": "Queries the configured vector store for relevant information based on the input query text. Use this to find context from stored documents before answering complex questions or summarizing information.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The natural language query text to search for relevant documents."
                },
                "top_k": {
                    "type": "integer",
                    "description": "The maximum number of relevant document chunks to retrieve. Defaults to 3.",
                    "default": 3
                },
                
            },
            "required": ["query"]
        }
    }
}


class Agent:
    """
    Represents an AI agent using a configurable LLM Service.
    Can work in a team, use local/remote MCP tools, and optionally a vector store.

    Args:
        name (str): Agent name.
        backstory (str): Agent background/persona.
        task_description (str): Description of the agent's specific task.
        task_expected_output (str): Expected output format.
        tools (Optional[List[Tool]]): Local tools for the agent.
        model (str): Model identifier string (passed to llm_service).
        llm_service (Optional[LLMServiceInterface]): Service for LLM calls (defaults to GroqService).
        mcp_manager (Optional[MCPClientManager]): Shared MCP client manager.
        mcp_server_names (Optional[List[str]]): MCP servers this agent uses.
        vector_store (Optional[VectorStoreInterface]): Vector store instance for RAG. 
        # embedding_function(Optional[EmbeddingFunction]): EF if needed by agent. 

    """
    def __init__(
        self,
        name: str,
        backstory: str,
        task_description: str = "No specific task assigned; await runtime user message.",
        task_expected_output: str = "",
        tools: Optional[List[Tool]] = None,
        model: str = "llama-3.3-70b-versatile", 
        llm_service: Optional[LLMServiceInterface] = None,
        mcp_manager: Optional[MCPClientManager] = None,
        mcp_server_names: Optional[List[str]] = None,
        vector_store: Optional[VectorStoreInterface] = None, 
        # embedding_function: Optional[EmbeddingFunction] = None,

    ):
        self.name = name
        self.backstory = backstory
        self.task_description = task_description
        self.task_expected_output = task_expected_output
        self.mcp_manager = mcp_manager
        self.mcp_server_names = mcp_server_names or []
        self.local_tools = tools or [] 

        self.vector_store = vector_store
        # self.embedding_function = embedding_function 

        llm_service_instance = llm_service or GroqService()

       
        from clap.react_pattern.react_agent import ReactAgent 
        self.react_agent = ReactAgent(
            agent_name=self.name, 
            llm_service=llm_service_instance,
            model=model,
            system_prompt=self.backstory,
            tools=self.local_tools, 
            mcp_manager=self.mcp_manager,
            mcp_server_names=self.mcp_server_names,
            vector_store=self.vector_store 
        )

        self.dependencies: List['Agent'] = []
        self.dependents: List['Agent'] = []
        self.received_context: dict[str, Any] = {}

        from clap.multiagent_pattern.team import Team 
        Team.register_agent(self)


    def __repr__(self): return f"{self.name}"

    def __rshift__(self, other: 'Agent') -> 'Agent': self.add_dependent(other); return other
    def __lshift__(self, other: 'Agent') -> 'Agent': self.add_dependency(other); return other
    def __rrshift__(self, other: List['Agent'] | 'Agent'): self.add_dependency(other); return self
    def __rlshift__(self, other: List['Agent'] | 'Agent'): self.add_dependent(other); return self

    def add_dependency(self, other: 'Agent' | List['Agent']):
        AgentClass = type(self)
        if isinstance(other, AgentClass):
            if other not in self.dependencies: self.dependencies.append(other)
            if self not in other.dependents: other.dependents.append(self)
        elif isinstance(other, list) and all(isinstance(item, AgentClass) for item in other):
            for item in other:
                 if item not in self.dependencies: self.dependencies.append(item)
                 if self not in item.dependents: item.dependents.append(self)
        else: raise TypeError("The dependency must be an instance or list of Agent.")
    def add_dependent(self, other: 'Agent' | List['Agent']):
        AgentClass = type(self)
        if isinstance(other, AgentClass):
            if self not in other.dependencies: other.dependencies.append(self)
            if other not in self.dependents: self.dependents.append(other)
        elif isinstance(other, list) and all(isinstance(item, AgentClass) for item in other):
            for item in other:
                if self not in item.dependencies: item.dependencies.append(self)
                if item not in self.dependents: self.dependents.append(item)
        else: raise TypeError("The dependent must be an instance or list of Agent.")

    def receive_context(self, sender_name: str, input_data: Any):
        self.received_context[sender_name] = input_data

    def create_prompt(self) -> str:
        """Creates the initial prompt for the agent's task execution."""
        context_str = "\n---\n".join(
            f"Context from {name}:\n{json.dumps(data, indent=2, ensure_ascii=False) if isinstance(data, dict) else str(data)}"
            for name, data in self.received_context.items()
        )
        if not context_str:
            context_str = "No context received from other agents."

        vector_store_info = ""
        user_query = self.task_description 

        if self.vector_store:
            vector_store_info = "\nVector Store Available: Use the 'vector_query' tool with the User Query below to find relevant context before answering factual questions."

        task_info = f"""
        User Query: {user_query}
        Task: Answer the User Query. {vector_store_info or ''} Use context from other agents if provided.
        Expected Output: {self.task_expected_output or 'Produce a meaningful response to complete the task.'}
        """.strip()

        prompt = dedent(f"""
        Agent: {self.name}
        Persona: {self.backstory}
        Team Context: {context_str}

        {task_info}

        Execute now following the ReAct pattern. If using 'vector_query', use the User Query text as the 'query' argument.
        """).strip()
        return prompt

    async def run(self, user_msg: Optional[str] = None) -> dict[str, Any]: 
        """Runs the agent's task using its configured ReactAgent.
        """
        print(f"Agent {self.name}: Preparing to run...")

        current_task = user_msg if user_msg is not None else self.task_description
        if not user_msg and self.task_description == "No specific task assigned; await runtime user message.":
             print(f"Agent {self.name}: Warning - Running without a specific user_msg or a meaningful pre-set task_description.")

        
        original_task_description = self.task_description
        self.task_description = current_task 

        msg = self.create_prompt()
        
        self.task_description = original_task_description 

        print(f"Agent {self.name}: Running ReactAgent...")
        raw_output = await self.react_agent.run(user_msg=msg) 
        output_data = {"output": raw_output}

        print(f"Agent {self.name}: Passing context to {len(self.dependents)} dependents...")
        for dependent in self.dependents:
            dependent.receive_context(self.name, output_data)

        return output_data




================================================
FILE: src/clap/multiagent_pattern/team.py
================================================

import asyncio
from collections import deque
from typing import Any, Dict, List , Optional
from colorama import Fore
from clap.utils.logging import fancy_print
from clap.multiagent_pattern.agent import Agent

_GRAPHVIZ_AVAILABLE = False
_Digraph_Placeholder_Type = Any 

try:
    from graphviz import Digraph as ImportedDigraph
    from graphviz.backend import ExecutableNotFound as GraphvizExecutableNotFound 
    _Digraph_Placeholder_Type = ImportedDigraph
    _GRAPHVIZ_AVAILABLE = True
except ImportError:
    pass

class Team:
    """
    A class representing a team of agents working together asynchronously.
    Supports parallel execution of agents where dependencies allow.

    Attributes:
        current_team (Team | None): Class-level variable to track the active Team context. None if no team context is active.
        agents (list[Agent]): A list of agents in the team.
        results (dict[str, Any]): Stores the final results of each agent run.
    """
    current_team = None

    def __init__(self):
        self.agents: List[Agent] = []
        self.results: dict[str, Any] = {}

    def __enter__(self): Team.current_team = self; return self
    def __exit__(self, exc_type, exc_val, exc_tb): Team.current_team = None
    def add_agent(self, agent: Agent):
        if agent not in self.agents: self.agents.append(agent)
    @staticmethod
    def register_agent(agent: Agent):
        if Team.current_team is not None: Team.current_team.add_agent(agent)

    
    def topological_sort(self) -> List[Agent]:
        in_degree: Dict[Agent, int] = {agent: 0 for agent in self.agents}
        adj: Dict[Agent, List[Agent]] = {agent: [] for agent in self.agents}
        agent_map: Dict[str, Agent] = {agent.name: agent for agent in self.agents}
        for agent in self.agents:
            valid_dependencies = [dep for dep in agent.dependencies if dep in self.agents]
            agent.dependencies = valid_dependencies
            for dependency in agent.dependencies:
                if dependency in agent_map.values():
                    adj[dependency].append(agent)
                    in_degree[agent] += 1
        queue: deque[Agent] = deque([agent for agent in self.agents if in_degree[agent] == 0])
        sorted_agents: List[Agent] = []
        processed_edges = 0
        while queue:
            current_agent = queue.popleft()
            sorted_agents.append(current_agent)
            for potential_dependent in self.agents:
                 if current_agent in potential_dependent.dependencies:
                      in_degree[potential_dependent] -= 1
                      processed_edges +=1
                      if in_degree[potential_dependent] == 0:
                           queue.append(potential_dependent)
        if len(sorted_agents) != len(self.agents):
             detected_agents = {a.name for a in sorted_agents}
             missing_agents = {a.name for a in self.agents} - detected_agents
             remaining_degrees = {a.name: in_degree[a] for a in self.agents if a not in sorted_agents}
             raise ValueError(
                 "Circular dependencies detected. Cannot perform topological sort. "
                 f"Agents processed: {list(detected_agents)}. "
                 f"Agents potentially in cycle: {list(missing_agents)}. "
                 f"Remaining degrees: {remaining_degrees}"
             )
        return sorted_agents

    def plot(self) -> Optional[_Digraph_Placeholder_Type]:
        """
        Generates a visual representation of the agent team's dependency graph.
        Requires the 'graphviz' Python library and the Graphviz system software to be installed.
        If Graphviz is not available, prints a warning and returns None.

        Returns:
            Optional[graphviz.Digraph]: A Digraph object if successful, else None.
        """
        if not _GRAPHVIZ_AVAILABLE:
            print(
                f"{Fore.YELLOW}CLAP Warning: The 'graphviz' Python library is not installed. "
                f"To use the plot() feature, please install it (e.g., pip install \"clap-agents[viz]\").{Fore.RESET}"
            )
            return None
        
        dot: Optional[_Digraph_Placeholder_Type] = None
        try:
            dot = _Digraph_Placeholder_Type(format="png") 
            for agent in self.agents:
                dot.node(agent.name) 
            for agent in self.agents:
                 for dependent in agent.dependents: 
                      if dependent in self.agents: 
                          dot.edge(agent.name, dependent.name) 
            print(f"{Fore.GREEN}Team dependency graph created. To render, call .render() or .view() on the returned object.{Fore.RESET}")
            return dot
        except NameError: 
            print(f"{Fore.YELLOW}CLAP Warning: Graphviz Digraph class not found (likely import issue). Plotting unavailable.{Fore.RESET}")
            return None
        except GraphvizExecutableNotFound:
            print(
                f"{Fore.RED}CLAP Error: Graphviz system software (dot executable) not found in PATH. "
                f"Team.plot() requires it to generate images. Please install Graphviz for your OS and ensure 'dot' is in your PATH.{Fore.RESET}"
            )
            return None
        except Exception as e:
            print(f"{Fore.RED}CLAP Error: An unexpected error occurred during graph plotting: {e}{Fore.RESET}")
            return None

    async def run(self):
        """
        Runs all agents in the team asynchronously, executing them in parallel
        when their dependencies are met. Compatible with Python 3.10+.
        """
        try:
            sorted_agents = self.topological_sort()
        except ValueError as e:
            print(f"{Fore.RED}Error during team setup: {e}{Fore.RESET}")
            return

        self.results = {}
        agent_tasks: Dict[Agent, asyncio.Task] = {}
        try:
            tasks_to_gather = []
            for agent in sorted_agents:
                task = asyncio.create_task(self._run_agent_task(agent, agent_tasks))
                agent_tasks[agent] = task
                tasks_to_gather.append(task)

            await asyncio.gather(*tasks_to_gather)
            print(f"{Fore.BLUE}--- All agent tasks finished ---{Fore.RESET}")

        except Exception as e:
            print(f"{Fore.RED}One or more agents failed during execution:{Fore.RESET}")
            print(f"{Fore.RED}- Error: {e}{Fore.RESET}")
            


    async def _run_agent_task(self, agent: Agent, all_tasks: Dict[Agent, asyncio.Task]):
        """
        An internal async function that wraps the execution of a single agent.
        It waits for dependencies to complete before running the agent.
        """
        dependency_tasks = [
            all_tasks[dep] for dep in agent.dependencies if dep in all_tasks
        ]
        if dependency_tasks:
            print(f"{Fore.YELLOW}Agent {agent.name} waiting for dependencies: {[dep.name for dep in agent.dependencies if dep in all_tasks]}...{Fore.RESET}")
            
            await asyncio.gather(*dependency_tasks)
            print(f"{Fore.GREEN}Agent {agent.name} dependencies met.{Fore.RESET}")

        fancy_print(f"STARTING AGENT: {agent.name}")
        try:
            agent_result = await agent.run()
            self.results[agent.name] = agent_result

            if isinstance(agent_result, dict) and 'output' in agent_result:
                 print(f"{Fore.GREEN}Agent {agent.name} Result:\n{agent_result['output']}{Fore.RESET}")
            else:
                 print(f"{Fore.YELLOW}Agent {agent.name} Result (raw):\n{str(agent_result)}{Fore.RESET}")
            fancy_print(f"FINISHED AGENT: {agent.name}")

        except Exception as e:
            fancy_print(f"ERROR IN AGENT: {agent.name}")
            print(f"{Fore.RED}Agent {agent.name} failed: {e}{Fore.RESET}")
            self.results[agent.name] = {"error": str(e)}
            raise




================================================
FILE: src/clap/react_pattern/__init__.py
================================================



================================================
FILE: src/clap/react_pattern/react_agent.py
================================================

import json
import re
from typing import List, Dict, Any, Optional
import asyncio

from colorama import Fore
from dotenv import load_dotenv


from clap.llm_services.base import LLMServiceInterface, StandardizedLLMResponse, LLMToolCall
from clap.tool_pattern.tool import Tool
from clap.mcp_client.client import MCPClientManager
# from clap.mcp_client.client import MCPClientManager, SseServerConfig 
from clap.utils.completions import build_prompt_structure, ChatHistory, update_chat_history
from clap.vector_stores.base import VectorStoreInterface, QueryResult
from clap.multiagent_pattern.agent import VECTOR_QUERY_TOOL_SCHEMA

try:
    from mcp import types as mcp_types
except ImportError:
    mcp_types = None 


load_dotenv()

CORE_SYSTEM_PROMPT = """
You are an AI assistant using the ReAct (Reason->Act) process with tools (local, remote MCP, `vector_query`).

**ReAct Loop:**
1.  **Thought:** REQUIRED start. Analyze the query/situation and plan next action. Start response ONLY with "Thought:".
2.  **Action Decision:** Decide if a tool is needed. **If using `vector_query`, the `query` argument MUST be the 'User Query:' from the main prompt.** Determine arguments for other tools based on your reasoning.
3.  **Tool Call / Next Step:** Use standard tool call format if applicable. If no tool call, proceed to step 5 (or 6 if done).
4.  **Observation:** (System provides tool results if a tool was called).
5.  **Thought:** Analyze observation (if any) and decide next step (another tool or final response).
6.  **Final Response:** REQUIRED end for the final answer. Must immediately follow the last Thought.

**Output Format:** Always start responses with "Thought:". Use "Final Response:" ONLY for the final answer, directly after the concluding Thought. No extra text before these prefixes. Be precise with tool arguments.
"""

class ReactAgent:
    """
    Async ReAct agent supporting local tools, remote MCP tools, and vector store queries,
    using a configurable LLM service.
    """

    def __init__(
        self,
        llm_service: LLMServiceInterface,
        model: str,
        agent_name: str = "ReactAgent", 
        tools: Optional[List[Tool]] = None,
        mcp_manager: Optional[MCPClientManager] = None, 
        mcp_server_names: Optional[List[str]] = None,  
        vector_store: Optional[VectorStoreInterface] = None, 
        system_prompt: str = "",
    ) -> None:
        self.llm_service = llm_service
        self.model = model
        self.agent_name = agent_name
        self.system_prompt = (system_prompt + "\n\n" + CORE_SYSTEM_PROMPT).strip()

        
        self.local_tools = tools if tools else []
        self.local_tools_dict = {tool.name: tool for tool in self.local_tools}
        self.local_tool_schemas = [tool.fn_schema for tool in self.local_tools]

        
        self.mcp_manager = mcp_manager
        self.mcp_server_names = mcp_server_names or []
        self.remote_tools_dict: Dict[str, Any] = {} 
        self.remote_tool_server_map: Dict[str, str] = {}

        
        self.vector_store = vector_store


    async def _get_combined_tool_schemas(self) -> List[Dict[str, Any]]:
        """Combines schemas for local tools, remote MCP tools, and vector store query tool."""
        all_schemas = list(self.local_tool_schemas) 

        
        if self.vector_store:
            all_schemas.append(VECTOR_QUERY_TOOL_SCHEMA)
            print(f"{Fore.BLUE}[{self.agent_name}] Vector query tool is available.{Fore.RESET}")

        
        self.remote_tools_dict = {}
        self.remote_tool_server_map = {}
        if self.mcp_manager and self.mcp_server_names and mcp_types:
            fetch_tasks = [self.mcp_manager.list_remote_tools(name) for name in self.mcp_server_names]
            results = await asyncio.gather(*fetch_tasks, return_exceptions=True)
            for server_name, result in zip(self.mcp_server_names, results):
                if isinstance(result, Exception):
                    print(f"{Fore.RED}[{self.agent_name}] Error listing tools from MCP server '{server_name}': {result}{Fore.RESET}")
                    continue
                if isinstance(result, list):
                    for tool in result:
                         
                        if isinstance(tool, mcp_types.Tool):
                            if tool.name in self.local_tools_dict or tool.name == VECTOR_QUERY_TOOL_SCHEMA["function"]["name"]:
                                print(f"{Fore.YELLOW}Warning: Remote MCP tool '{tool.name}' conflicts with a local/vector tool. Skipping.{Fore.RESET}")
                                continue
                            if tool.name in self.remote_tools_dict:
                                print(f"{Fore.YELLOW}Warning: Remote MCP tool '{tool.name}' conflicts with another remote tool. Skipping duplicate.{Fore.RESET}")
                                continue

                            self.remote_tools_dict[tool.name] = tool
                            self.remote_tool_server_map[tool.name] = server_name
                            
                            translated_schema = {
                                "type": "function",
                                "function": {
                                    "name": tool.name,
                                    "description": tool.description or "",
                                    "parameters": tool.inputSchema or {"type": "object", "properties": {}} # Handle potentially missing schema
                                }
                            }
                            all_schemas.append(translated_schema)
                        else:
                            print(f"{Fore.YELLOW}Warning: Received non-Tool object from {server_name}: {type(tool)}{Fore.RESET}")

        print(f"{Fore.BLUE}[{self.agent_name}] Total tools available to LLM: {len(all_schemas)}{Fore.RESET}")
        # print(f"Schemas: {json.dumps(all_schemas, indent=2)}") 
        return all_schemas



    async def _execute_single_tool_call(self, tool_call: LLMToolCall) -> Dict[str, Any]:
        """
        Executes a single tool call (local, remote MCP, or vector query),
        handling observation length limits for vector queries.
        """
        tool_call_id = tool_call.id
        tool_name = tool_call.function_name
        result_str = f"Error: Processing failed for tool call '{tool_name}' (id: {tool_call_id})." # Default error message

        try:
            arguments = json.loads(tool_call.function_arguments_json_str)

            
            if tool_name == VECTOR_QUERY_TOOL_SCHEMA["function"]["name"]:
                if not self.vector_store:
                    print(f"{Fore.RED}Error: Agent {self.agent_name} received call for '{tool_name}' but has no vector store configured.{Fore.RESET}")
                    result_str = f"Error: Vector store not available for agent {self.agent_name}."
                else:
                    print(f"{Fore.CYAN}\n[{self.agent_name}] Executing Vector Store Query Tool: {tool_name}{Fore.RESET}")
                    print(f"Tool call ID: {tool_call_id}") 
                    print(f"Arguments: {arguments}")     

                    query_text = arguments.get("query")
                    
                    top_k_value_from_llm = arguments.get("top_k")
                    default_top_k_from_schema = VECTOR_QUERY_TOOL_SCHEMA["function"]["parameters"]["properties"]["top_k"].get("default", 3)
                    top_k = default_top_k_from_schema 

                    if top_k_value_from_llm is not None:
                        try:
                            top_k = int(top_k_value_from_llm)
                        except (ValueError, TypeError):
                            print(f"{Fore.YELLOW}Warning: LLM provided top_k '{top_k_value_from_llm}' is not a valid integer. Using schema default: {default_top_k_from_schema}.{Fore.RESET}")
                            


                    if not query_text:
                         result_str = "Error: 'query' argument is required for vector_query tool."
                    else:
                        query_results: QueryResult = await self.vector_store.aquery(
                            query_texts=[query_text],
                            n_results=top_k, 
                            include=["documents", "metadatas", "distances"]
                        )

                        formatted_chunks_for_llm = []
                        current_length = 0
                        max_obs_len = 4000 

                        
                        retrieved_docs = query_results.get("documents")
                        retrieved_ids = query_results.get("ids")

                        if retrieved_docs and isinstance(retrieved_docs, list) and len(retrieved_docs) > 0 and \
                           retrieved_ids and isinstance(retrieved_ids, list) and len(retrieved_ids) > 0:

                            docs_for_query = retrieved_docs[0]
                            ids_for_query = retrieved_ids[0]
                            
                            metas_for_query = []
                            if query_results.get("metadatas") and isinstance(query_results["metadatas"], list) and len(query_results["metadatas"]) > 0:
                                metas_for_query = query_results["metadatas"][0] 
                            else:
                                metas_for_query = [None] * len(docs_for_query)

                            distances_for_query = []
                            if query_results.get("distances") and isinstance(query_results["distances"], list) and len(query_results["distances"]) > 0:
                                distances_for_query = query_results["distances"][0] 
                            else:
                                distances_for_query = [None] * len(docs_for_query)


                            for i, doc_content_item in enumerate(docs_for_query): 
                                current_meta = metas_for_query[i] if i < len(metas_for_query) else None
                                current_id = str(ids_for_query[i]) if i < len(ids_for_query) else "N/A"
                                current_distance = distances_for_query[i] if i < len(distances_for_query) and distances_for_query[i] is not None else float('nan')

                                meta_str = json.dumps(current_meta, ensure_ascii=False) if current_meta else "{}"
                                
                                
                                current_chunk_formatted = (
                                    f"--- Retrieved Chunk {str(i+1)} (ID: {current_id}, Distance: {current_distance:.4f}) ---\n"
                                    f"Metadata: {meta_str}\n" 
                                    f"Content: {str(doc_content_item)}\n\n" 
                                )
                                
                                chunk_len = len(current_chunk_formatted)

                                if current_length + chunk_len <= max_obs_len:
                                    formatted_chunks_for_llm.append(current_chunk_formatted)
                                    current_length += chunk_len
                                else:
                                    print(f"{Fore.YELLOW}[{self.agent_name}] Observation limit ({max_obs_len} chars) reached. Included {len(formatted_chunks_for_llm)} full chunks out of {len(docs_for_query)} retrieved.{Fore.RESET}")
                                    break
                            
                            if formatted_chunks_for_llm:
                                header = f"Retrieved {len(formatted_chunks_for_llm)} relevant document chunks (out of {len(docs_for_query)} found for the query):\n\n"
                                result_str = header + "".join(formatted_chunks_for_llm).strip()
                            else:
                                result_str = "No relevant documents found (or all retrieved documents were too long to fit context limit)."
                        else: 
                             result_str = "No relevant documents found in vector store for the query."

        
            elif tool_name in self.local_tools_dict:
                tool = self.local_tools_dict[tool_name]
                print(f"{Fore.GREEN}\n[{self.agent_name}] Executing Local Tool: {tool_name}{Fore.RESET}")
                print(f"Tool call ID: {tool_call_id}")
                print(f"Arguments: {arguments}")
                result = await tool.run(**arguments)
               
                if not isinstance(result, str):
                    try:
                        result_str = json.dumps(result, ensure_ascii=False)
                    except TypeError: 
                        result_str = str(result)
                else:
                    result_str = result

           
            elif tool_name in self.remote_tool_server_map and self.mcp_manager:
                server_name = self.remote_tool_server_map[tool_name]
                print(f"{Fore.CYAN}\n[{self.agent_name}] Executing Remote MCP Tool: {tool_name} on {server_name}{Fore.RESET}")
                print(f"Tool call ID: {tool_call_id}")
                print(f"Arguments: {arguments}")
                
                result_str = await self.mcp_manager.call_remote_tool(server_name, tool_name, arguments)

            
            else:
                print(f"{Fore.RED}Error: Tool '{tool_name}' not found locally, remotely, or as vector query.{Fore.RESET}")
                result_str = f"Error: Tool '{tool_name}' is not available."

            
            print(f"{Fore.GREEN}Tool '{tool_name}' observation prepared: {result_str[:150]}...{Fore.RESET}")


        except json.JSONDecodeError:
            print(f"{Fore.RED}Error decoding arguments for {tool_name}: {tool_call.function_arguments_json_str}{Fore.RESET}")
            result_str = f"Error: Invalid arguments JSON provided for {tool_name}."
        except Exception as e:
            print(f"{Fore.RED}Error executing/processing tool {tool_name} (id: {tool_call_id}): {e}{Fore.RESET}")
          
            result_str = f"Error during execution of tool {tool_name}: {e}"

        
        return {tool_call_id: result_str}




    
    async def process_tool_calls(self, tool_calls: List[LLMToolCall]) -> Dict[str, Any]:
        """Processes multiple tool calls concurrently."""
        observations = {}
        if not isinstance(tool_calls, list):
            print(f"{Fore.RED}Error: Expected a list of LLMToolCall, got {type(tool_calls)}{Fore.RESET}")
            return observations 

        tasks = [self._execute_single_tool_call(tc) for tc in tool_calls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, dict) and len(result) == 1:
                observations.update(result)
            elif isinstance(result, Exception):
                 
                 print(f"{Fore.RED}Error during concurrent tool execution gather: {result}{Fore.RESET}")
                 
                 # observations[f"error_{len(observations)}"] = f"Tool execution failed: {result}"
            else:
                 print(f"{Fore.RED}Error: Unexpected item in tool execution results: {result}{Fore.RESET}")

        return observations


    async def run(
        self,
        user_msg: str,
        max_rounds: int = 5,
    ) -> str:
        """Runs the ReAct loop for the agent."""
        print(f"--- [{self.agent_name}] Starting ReAct Loop ---")
        combined_tool_schemas = await self._get_combined_tool_schemas()

        initial_user_message = build_prompt_structure(role="user", content=user_msg)
        chat_history = ChatHistory(
            [
                build_prompt_structure(role="system", content=self.system_prompt),
                initial_user_message,
            ]
        )

        final_response = f"Agent {self.agent_name} failed to produce a final response."

        for round_num in range(max_rounds):
            print(Fore.CYAN + f"\n--- [{self.agent_name}] Round {round_num + 1}/{max_rounds} ---")

            
            current_tools = combined_tool_schemas if combined_tool_schemas else None
            current_tool_choice = "auto" if current_tools else "none"

            print(f"[{self.agent_name}] Calling LLM...")
            llm_response: StandardizedLLMResponse = await self.llm_service.get_llm_response(
                model=self.model,
                messages=list(chat_history),
                tools=current_tools,
                tool_choice=current_tool_choice
            )

            assistant_content = llm_response.text_content
            llm_tool_calls = llm_response.tool_calls 

            extracted_thought = None
            potential_final_response = None

            
            if assistant_content is not None:
                lines = assistant_content.strip().split('\n')
                thought_lines = []
                response_lines = []
                in_thought = False
                in_response = False
                for line in lines:
                    stripped_line = line.strip()
                    if stripped_line.startswith("Thought:"):
                        in_thought = True; in_response = False
                        thought_content = stripped_line[len("Thought:"):].strip()
                        if thought_content: thought_lines.append(thought_content)
                    elif stripped_line.startswith("Final Response:"):
                        in_response = True; in_thought = False
                        response_content = stripped_line[len("Final Response:"):].strip()
                        if response_content: response_lines.append(response_content)
                    elif in_thought:
                        
                        thought_lines.append(line) 
                    elif in_response:
                        response_lines.append(line) 

                if thought_lines:
                    extracted_thought = "\n".join(thought_lines).strip()
                    print(f"{Fore.MAGENTA}\n[{self.agent_name}] Thought:\n{extracted_thought}{Fore.RESET}")
                else:
                     print(f"{Fore.YELLOW}Warning: No 'Thought:' prefix found in LLM response content.{Fore.RESET}")
                     

                if response_lines:
                    potential_final_response = "\n".join(response_lines).strip()
                    

            
            assistant_msg_dict: Dict[str, Any] = {"role": "assistant"}
            if assistant_content: 
                assistant_msg_dict["content"] = assistant_content
            if llm_tool_calls:
                 
                 assistant_msg_dict["tool_calls"] = [
                     {
                         "id": tc.id,
                         "type": "function", 
                         "function": {
                             "name": tc.function_name,
                             "arguments": tc.function_arguments_json_str,
                         }
                     } for tc in llm_tool_calls
                 ]
            
            update_chat_history(chat_history, assistant_msg_dict)

            
            if llm_tool_calls:
                print(f"{Fore.YELLOW}\n[{self.agent_name}] Assistant requests tool calls:{Fore.RESET}")
                
                observations = await self.process_tool_calls(llm_tool_calls)
                print(f"{Fore.BLUE}\n[{self.agent_name}] Observations generated: {len(observations)} items.{Fore.RESET}")

                if not observations:
                     print(f"{Fore.RED}Error: Tool processing failed to return any observations.{Fore.RESET}")
                     
                     error_message = build_prompt_structure(role="user", content="System Error: Tool execution failed to produce results. Please try again or proceed without tool results.")
                     update_chat_history(chat_history, error_message)
                     continue 


                
                tool_messages_added = 0
                for tool_call in llm_tool_calls:
                    tool_call_id = tool_call.id
                    observation_content = observations.get(tool_call_id)
                    if observation_content is None:
                         print(f"{Fore.RED}Error: Observation missing for tool call ID {tool_call_id}.{Fore.RESET}")
                         observation_content = f"Error: Result for tool call {tool_call_id} was not found."

                    tool_message = build_prompt_structure(
                         role="tool",
                         content=str(observation_content), 
                         tool_call_id=tool_call_id
                    )
                    update_chat_history(chat_history, tool_message)
                    tool_messages_added += 1

                if tool_messages_added == 0:
                     print(f"{Fore.RED}Critical Error: No tool messages were added to history despite tool calls being present.{Fore.RESET}")
                     
                     return f"Error: Agent {self.agent_name} failed during tool observation processing."


            elif potential_final_response is not None:
                
                print(f"{Fore.GREEN}\n[{self.agent_name}] Assistant provides final response:{Fore.RESET}")
                final_response = potential_final_response
                print(f"{Fore.GREEN}{final_response}{Fore.RESET}")
                return final_response

            elif assistant_content is not None and not llm_tool_calls:
                
                print(f"{Fore.YELLOW}\n[{self.agent_name}] Assistant provided content without 'Final Response:' prefix and no tool calls. Treating as final answer.{Fore.RESET}")
                final_response = assistant_content.strip() 
                 
                if final_response.startswith("Thought:"):
                     final_response = final_response[len("Thought:"):].strip()
                print(f"{Fore.GREEN}{final_response}{Fore.RESET}")
                return final_response

            elif not llm_tool_calls and assistant_content is None:
                
                print(f"{Fore.RED}Error: Assistant message has neither content nor tool calls.{Fore.RESET}")
                final_response = f"Error: Agent {self.agent_name} received an empty response from the LLM."
                return final_response
            


        
        print(f"{Fore.YELLOW}\n[{self.agent_name}] Maximum rounds ({max_rounds}) reached.{Fore.RESET}")

        if potential_final_response and not llm_tool_calls:
             final_response = potential_final_response
             print(f"{Fore.GREEN}(Last response from agent {self.agent_name}): {final_response}{Fore.RESET}")
        elif assistant_content and not llm_tool_calls:
             
             final_response = assistant_content.strip()
             if final_response.startswith("Thought:"):
                  final_response = final_response[len("Thought:"):].strip()
             print(f"{Fore.GREEN}(Last raw content from agent {self.agent_name}): {final_response}{Fore.RESET}")
        else:
            final_response = f"Agent {self.agent_name} stopped after maximum rounds without reaching a final answer."
            print(f"{Fore.YELLOW}{final_response}{Fore.RESET}")

        return final_response




================================================
FILE: src/clap/tool_pattern/__init__.py
================================================



================================================
FILE: src/clap/tool_pattern/tool.py
================================================
import json
import inspect
import functools
from typing import Callable, Any, Dict, List 
import anyio
import jsonschema 


def get_fn_signature(fn: Callable) -> dict:
   
    type_mapping = {
        "int": "integer", "str": "string", "bool": "boolean",
        "float": "number", "list": "array", "dict": "object",
    }
    parameters = {"type": "object", "properties": {}, "required": []}
    sig = inspect.signature(fn)
    for name, type_hint in fn.__annotations__.items():
        if name == "return": continue
        param_type_name = getattr(type_hint, "__name__", str(type_hint))
        schema_type = type_mapping.get(param_type_name.lower(), "string") 
        parameters["properties"][name] = {"type": schema_type}
        if sig.parameters[name].default is inspect.Parameter.empty:
            parameters["required"].append(name)
    if not parameters.get("required"): 
        if "required" in parameters: del parameters["required"]
    return {"type": "function", "function": {"name": fn.__name__, "description": fn.__doc__, "parameters": parameters}}


def validate_and_coerce_arguments(tool_call_args: Dict[str, Any], tool_schema: Dict[str, Any]) -> Dict[str, Any]: # Renamed for clarity
    """
    Validates and coerces arguments in the input dictionary based on the tool's JSON schema.
    Then, performs final validation using jsonschema.

    Args:
        tool_call_args (Dict[str, Any]): Arguments from LLM (often strings).
        tool_schema (Dict[str, Any]): Tool's JSON schema.

    Returns:
        Dict[str, Any]: Arguments with values coerced to correct types.

    Raises:
        jsonschema.ValidationError: If final validation fails after coercion.
        ValueError: If coercion fails for a required argument or types are incompatible.
    """
    parameter_schema = tool_schema.get("function", {}).get("parameters", {})
    properties = parameter_schema.get("properties", {})
    required_args = parameter_schema.get("required", [])
    
    coerced_args: Dict[str, Any] = {}

    
    string_coercion_map = {
        "integer": int,
        "number": float, 
        "boolean": lambda x: x.lower() in ['true', '1', 'yes'] if isinstance(x, str) else bool(x),
        "string": str,
        
        "array": lambda x: json.loads(x) if isinstance(x, str) else x,
        "object": lambda x: json.loads(x) if isinstance(x, str) else x,
    }
    
    expected_python_type_map = {
        "integer": int, "number": (int, float), "boolean": bool,
        "string": str, "array": list, "object": dict,
    }

    for arg_name, arg_value in tool_call_args.items():
        prop_schema = properties.get(arg_name)
        if not prop_schema:
           
            coerced_args[arg_name] = arg_value 
            continue

        expected_schema_type = prop_schema.get("type")
        python_type_tuple = expected_python_type_map.get(expected_schema_type) 

        
        if python_type_tuple and isinstance(arg_value, python_type_tuple):
            coerced_args[arg_name] = arg_value
            continue

        
        coercer = string_coercion_map.get(expected_schema_type) 
        if coercer:
            try:
                coerced_value = coercer(arg_value)
                
                if expected_schema_type in ["array", "object"] and python_type_tuple:
                    if not isinstance(coerced_value, python_type_tuple): 
                         raise ValueError(f"Decoded JSON for '{arg_name}' is not expected type '{expected_schema_type}'.")
                coerced_args[arg_name] = coerced_value
            except (ValueError, TypeError, json.JSONDecodeError) as e:
                
                
                if arg_name in required_args:
                    raise ValueError(
                        f"Error coercing required argument '{arg_name}' with value '{arg_value}' "
                        f"to type '{expected_schema_type}': {e}"
                    )
                else:
                    coerced_args[arg_name] = arg_value 
        else:
            
            coerced_args[arg_name] = arg_value

   
    try:
        jsonschema.validate(instance=coerced_args, schema=parameter_schema)
    except jsonschema.ValidationError as e:
       
        raise

    return coerced_args


class Tool:
    def __init__(self, name: str, fn: Callable, fn_schema: dict):
        self.name = name
        self.fn = fn
        self.fn_schema = fn_schema
        # self.fn_signature = json.dumps(fn_schema) 

    def __str__(self):
        return json.dumps(self.fn_schema, indent=2)

    async def run(self, **kwargs: Any) -> Any: 
        """
        Executes the tool, validating and coercing arguments first.
        """
        function_name = self.fn_schema.get("function", {}).get("name", "unknown_tool")
        try:
           
            validated_and_coerced_kwargs = validate_and_coerce_arguments(kwargs, self.fn_schema)
        except (jsonschema.ValidationError, ValueError) as e:
           
            return f"Error: Invalid arguments for tool {function_name} - {str(e)}"
        except Exception as e:
             return f"Error: Unexpected issue with arguments for tool {function_name}."

        
        try:
            if inspect.iscoroutinefunction(self.fn):
                return await self.fn(**validated_and_coerced_kwargs)
            else:
                
                func_with_args = functools.partial(self.fn, **validated_and_coerced_kwargs)
                return await anyio.to_thread.run_sync(func_with_args)
        except Exception as e:
            
             return f"Error executing tool {function_name}: {e}"


def tool(fn: Callable):
    fn_schema = get_fn_signature(fn)
    if not fn_schema or 'function' not in fn_schema or 'name' not in fn_schema['function']:
            raise ValueError(f"Could not generate valid schema for function {fn.__name__}")
    return Tool(name=fn_schema["function"]["name"], fn=fn, fn_schema=fn_schema)




================================================
FILE: src/clap/tool_pattern/tool_agent.py
================================================

import json
import asyncio
from typing import List, Dict, Any, Optional, Union # Added Union

from colorama import Fore
from dotenv import load_dotenv

from clap.tool_pattern.tool import Tool
from clap.mcp_client.client import MCPClientManager
from clap.utils.completions import build_prompt_structure, ChatHistory, update_chat_history
from clap.llm_services.base import LLMServiceInterface, StandardizedLLMResponse, LLMToolCall
from clap.vector_stores.base import VectorStoreInterface, QueryResult

from clap.multiagent_pattern.agent import VECTOR_QUERY_TOOL_SCHEMA



try:
    from mcp import types as mcp_types
except ImportError:
    mcp_types = None

load_dotenv()

NATIVE_TOOL_SYSTEM_PROMPT = """
You are a helpful assistant. Use the available tools (local functions, remote MCP tools, or vector_query for document retrieval) if necessary to answer the user's request.
If you use a tool, you will be given the results, and then you should provide the final response to the user based on those results.
If no tool is needed, answer directly.
When using vector_query, the 'query' argument should be the user's main question.
"""

class ToolAgent:
    """
    A simple agent that uses LLM native tool calling asynchronously.
    Supports local, remote MCP tools, and RAG via vector_query tool, using an LLMServiceInterface.
    Makes one attempt to call tools if needed, processes results,
    and then generates a final response.
    """

    def __init__(
        self,
        llm_service: LLMServiceInterface,
        model: str,
        tools: Optional[Union[Tool, List[Tool]]] = None,
        mcp_manager: Optional[MCPClientManager] = None,
        mcp_server_names: Optional[List[str]] = None,
        vector_store: Optional[VectorStoreInterface] = None, 
        system_prompt: str = NATIVE_TOOL_SYSTEM_PROMPT,
    ) -> None:
        if not isinstance(llm_service, LLMServiceInterface):
            raise TypeError("llm_service must be an instance of LLMServiceInterface.")
        if not model or not isinstance(model, str):
            raise ValueError("A valid model name (string) is required.")

        self.llm_service = llm_service
        self.model = model
        self.system_prompt = system_prompt

        if tools is None:
            self.local_tools = []
        elif isinstance(tools, list):
            self.local_tools = tools
        else: 
            self.local_tools = [tools]

        self.local_tools_dict = {tool.name: tool for tool in self.local_tools}
        self.local_tool_schemas = [tool.fn_schema for tool in self.local_tools]

        self.mcp_manager = mcp_manager
        self.mcp_server_names = mcp_server_names or []
        self.remote_tools_dict: Dict[str, Any] = {}
        self.remote_tool_server_map: Dict[str, str] = {}

        self.vector_store = vector_store 

    async def _get_combined_tool_schemas(self) -> List[Dict[str, Any]]:
        all_schemas = list(self.local_tool_schemas)
        
        self.remote_tools_dict = {}
        self.remote_tool_server_map = {}
        if self.mcp_manager and self.mcp_server_names and mcp_types:
            fetch_tasks = [self.mcp_manager.list_remote_tools(name) for name in self.mcp_server_names]
            results = await asyncio.gather(*fetch_tasks, return_exceptions=True)
            for server_name, result in zip(self.mcp_server_names, results):
                if isinstance(result, Exception): print(f"{Fore.RED}ToolAgent: Error listing MCP tools '{server_name}': {result}{Fore.RESET}"); continue
                if isinstance(result, list):
                    for tool_obj in result: 
                        if isinstance(tool_obj, mcp_types.Tool):
                           if tool_obj.name in self.local_tools_dict: print(f"{Fore.YELLOW}ToolAgent Warning: MCP tool '{tool_obj.name}' conflicts with local. Skipping.{Fore.RESET}"); continue
                           
                           if self.vector_store and tool_obj.name == VECTOR_QUERY_TOOL_SCHEMA["function"]["name"]:
                               print(f"{Fore.YELLOW}ToolAgent Warning: MCP tool '{tool_obj.name}' conflicts with built-in vector_query tool. Skipping MCP version.{Fore.RESET}"); continue
                           if tool_obj.name in self.remote_tools_dict: print(f"{Fore.YELLOW}ToolAgent Warning: MCP tool '{tool_obj.name}' conflicts with another remote. Skipping.{Fore.RESET}"); continue
                           self.remote_tools_dict[tool_obj.name] = tool_obj
                           self.remote_tool_server_map[tool_obj.name] = server_name
                           translated_schema = {"type": "function", "function": {"name": tool_obj.name, "description": tool_obj.description or "", "parameters": tool_obj.inputSchema or {"type": "object", "properties": {}}}}
                           all_schemas.append(translated_schema)
                        else: print(f"{Fore.YELLOW}ToolAgent Warning: Non-Tool object from {server_name}: {type(tool_obj)}{Fore.RESET}")

        
        if self.vector_store:
            
            if not any(schema["function"]["name"] == VECTOR_QUERY_TOOL_SCHEMA["function"]["name"] for schema in all_schemas):
                all_schemas.append(VECTOR_QUERY_TOOL_SCHEMA)
                print(f"{Fore.BLUE}ToolAgent: Vector query tool is available.{Fore.RESET}")
        
        print(f"{Fore.BLUE}ToolAgent: Total tools available to LLM: {len(all_schemas)}{Fore.RESET}")
        return all_schemas

    async def _execute_single_tool_call(self, tool_call: LLMToolCall) -> Dict[str, Any]:
        tool_call_id = tool_call.id
        tool_name = tool_call.function_name
        result_str = f"Error: Processing tool call '{tool_name}' (id: {tool_call_id})."
        try:
            arguments = json.loads(tool_call.function_arguments_json_str)

            
            if self.vector_store and tool_name == VECTOR_QUERY_TOOL_SCHEMA["function"]["name"]:
                print(f"{Fore.CYAN}\nToolAgent: Executing Vector Store Query Tool: {tool_name} (ID: {tool_call_id}) Args: {arguments}{Fore.RESET}")
                query_text = arguments.get("query")
                
                top_k_value_from_llm = arguments.get("top_k")
                default_top_k_from_schema = VECTOR_QUERY_TOOL_SCHEMA["function"]["parameters"]["properties"]["top_k"].get("default", 3)
                top_k = default_top_k_from_schema
                if top_k_value_from_llm is not None:
                    try: top_k = int(top_k_value_from_llm)
                    except (ValueError, TypeError):
                        print(f"{Fore.YELLOW}ToolAgent Warning: LLM provided top_k '{top_k_value_from_llm}' is invalid. Using default: {default_top_k_from_schema}.{Fore.RESET}")

                if not query_text:
                    result_str = "Error: 'query' argument required for vector_query tool."
                else:
                    query_results: QueryResult = await self.vector_store.aquery(
                        query_texts=[query_text], n_results=top_k,
                        include=["documents", "metadatas", "distances"]
                    )
                    
                    formatted_chunks_for_llm = []
                    current_length = 0
                    max_obs_len = 4000 
                    retrieved_docs = query_results.get("documents")
                    retrieved_ids = query_results.get("ids")
                    if retrieved_docs and isinstance(retrieved_docs, list) and len(retrieved_docs) > 0 and \
                       retrieved_ids and isinstance(retrieved_ids, list) and len(retrieved_ids) > 0:
                        docs_for_query, ids_for_query = retrieved_docs[0], retrieved_ids[0]
                        metas_list, distances_list = query_results.get("metadatas"), query_results.get("distances")
                        metas_for_query = metas_list[0] if metas_list and len(metas_list) > 0 else [None] * len(docs_for_query)
                        distances_for_query = distances_list[0] if distances_list and len(distances_list) > 0 else [None] * len(docs_for_query)
                        for i, doc_content_item in enumerate(docs_for_query):
                            current_meta = metas_for_query[i] if i < len(metas_for_query) else None
                            current_id = str(ids_for_query[i]) if i < len(ids_for_query) else "N/A"
                            current_distance = distances_for_query[i] if i < len(distances_for_query) and distances_for_query[i] is not None else float('nan')
                            meta_str = json.dumps(current_meta, ensure_ascii=False) if current_meta else "{}"
                            current_chunk_formatted = (
                                f"--- Retrieved Chunk {str(i+1)} (ID: {current_id}, Distance: {current_distance:.4f}) ---\n"
                                f"Metadata: {meta_str}\nContent: {str(doc_content_item)}\n\n")
                            chunk_len = len(current_chunk_formatted)
                            if current_length + chunk_len <= max_obs_len:
                                formatted_chunks_for_llm.append(current_chunk_formatted)
                                current_length += chunk_len
                            else:
                                print(f"{Fore.YELLOW}ToolAgent: Obs limit ({max_obs_len}) reached. Included {len(formatted_chunks_for_llm)} chunks.{Fore.RESET}"); break
                        if formatted_chunks_for_llm:
                            header = f"Retrieved {len(formatted_chunks_for_llm)} relevant document chunks (out of {len(docs_for_query)} found):\n\n"
                            result_str = header + "".join(formatted_chunks_for_llm).strip()
                        else: result_str = "No relevant documents found (or chunks too long for limit)."
                    else: result_str = "No relevant documents found in vector store for query."
           

            elif tool_name in self.local_tools_dict:
                tool_instance = self.local_tools_dict[tool_name]
                result = await tool_instance.run(**arguments)
                if not isinstance(result, str): result_str = json.dumps(result, ensure_ascii=False)
                else: result_str = result
            elif tool_name in self.remote_tool_server_map and self.mcp_manager:
                server_name = self.remote_tool_server_map[tool_name]
                result_str = await self.mcp_manager.call_remote_tool(server_name, tool_name, arguments)
            else:
                result_str = f"Error: Tool '{tool_name}' not available."
                print(f"{Fore.RED}ToolAgent: {result_str}{Fore.RESET}")
                return {tool_call_id: result_str}
            
            print(f"{Fore.GREEN}ToolAgent: Tool '{tool_name}' observation: {result_str[:150]}...{Fore.RESET}")
        except json.JSONDecodeError:
            result_str = f"Error: Invalid arguments JSON for {tool_name}."
            print(f"{Fore.RED}ToolAgent: {result_str} Data: {tool_call.function_arguments_json_str}{Fore.RESET}")
        except Exception as e:
            result_str = f"Error executing tool {tool_name}: {e}"
            print(f"{Fore.RED}ToolAgent: Error for tool {tool_name} (ID: {tool_call_id}): {e}{Fore.RESET}")
        return {tool_call_id: result_str}

    
    async def process_tool_calls(self, tool_calls: List[LLMToolCall]) -> List[Dict[str, Any]]:
        observation_messages = []
        if not isinstance(tool_calls, list): return observation_messages
        tasks = [self._execute_single_tool_call(tc) for tc in tool_calls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for result in results:
             if isinstance(result, dict) and len(result) == 1:
                  tool_call_id, result_str = list(result.items())[0]
                  observation_messages.append(build_prompt_structure(role="tool", content=result_str, tool_call_id=tool_call_id))
             elif isinstance(result, Exception): print(f"{Fore.RED}ToolAgent: Error in concurrent tool execution: {result}{Fore.RESET}")
             else: print(f"{Fore.RED}ToolAgent: Unexpected item in tool results: {result}{Fore.RESET}")
        return observation_messages

    async def run(self, user_msg: str) -> str:
        combined_tool_schemas = await self._get_combined_tool_schemas()
        initial_user_message = build_prompt_structure(role="user", content=user_msg)
        chat_history = ChatHistory(
            [build_prompt_structure(role="system", content=self.system_prompt), initial_user_message]
        )
        llm_response_1: StandardizedLLMResponse = await self.llm_service.get_llm_response(
            model=self.model, messages=list(chat_history),
            tools=combined_tool_schemas if combined_tool_schemas else None,
            tool_choice="auto" if combined_tool_schemas else "none"
        )
        assistant_msg_1_dict: Dict[str, Any] = {"role": "assistant"}
        if llm_response_1.text_content: assistant_msg_1_dict["content"] = llm_response_1.text_content
        if llm_response_1.tool_calls:
            assistant_msg_1_dict["tool_calls"] = [
                {"id": tc.id, "type": "function", "function": {"name": tc.function_name, "arguments": tc.function_arguments_json_str}}
                for tc in llm_response_1.tool_calls
            ]
        if "content" in assistant_msg_1_dict or "tool_calls" in assistant_msg_1_dict:
            update_chat_history(chat_history, assistant_msg_1_dict)

        final_response = "ToolAgent encountered an issue."
        if llm_response_1.tool_calls:
            observation_messages = await self.process_tool_calls(llm_response_1.tool_calls)
            for obs_msg in observation_messages: update_chat_history(chat_history, obs_msg)
            llm_response_2: StandardizedLLMResponse = await self.llm_service.get_llm_response(
                model=self.model, messages=list(chat_history)
            )
            final_response = llm_response_2.text_content if llm_response_2.text_content else "Agent provided no final response after using tools."
        elif llm_response_1.text_content is not None:
            final_response = llm_response_1.text_content
        else:
            print(f"{Fore.RED}ToolAgent Error: LLM message has neither content nor tool_calls.{Fore.RESET}")
            final_response = "Error: ToolAgent received an unexpected empty response from the LLM."
        return final_response



================================================
FILE: src/clap/tools/__init__.py
================================================

from .web_search import duckduckgo_search
from .web_crawler import scrape_url, extract_text_by_query 
from .email_tools import send_email, fetch_recent_emails

__all__ = [
    "duckduckgo_search",
    "scrape_url",
    "extract_text_by_query",
    "send_email",
    "fetch_recent_emails",
]




================================================
FILE: src/clap/tools/email_tools.py
================================================

import os
import smtplib
import imaplib
import email
import json 
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders
from email.header import decode_header
from dotenv import load_dotenv
import anyio 
import functools 
import requests
from typing import Optional

from clap.tool_pattern.tool import tool

load_dotenv()

SMTP_HOST = "smtp.gmail.com"
SMTP_PORT = 587
IMAP_HOST = "imap.gmail.com"
SMTP_USERNAME = os.getenv("SMTP_USERNAME")
SMTP_PASSWORD = os.getenv("SMTP_PASSWORD")


def _send_email_sync(recipient: str, subject: str, body: str, attachment_path: Optional[str] = None) -> str:
    """Synchronous helper to send email."""
    if not SMTP_USERNAME or not SMTP_PASSWORD:
        return "Error: SMTP username or password not configured in environment."
    try:
        msg = MIMEMultipart()
        msg["From"] = SMTP_USERNAME
        msg["To"] = recipient
        msg["Subject"] = subject
        msg.attach(MIMEText(body, "plain"))
        if attachment_path and os.path.exists(attachment_path):
            with open(attachment_path, "rb") as attachment:
                part = MIMEBase("application", "octet-stream")
                part.set_payload(attachment.read())
            encoders.encode_base64(part)
            part.add_header("Content-Disposition", f"attachment; filename={os.path.basename(attachment_path)}")
            msg.attach(part)
        
        with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
            server.starttls()
            server.login(SMTP_USERNAME, SMTP_PASSWORD)
            server.sendmail(SMTP_USERNAME, recipient, msg.as_string())
        
        if attachment_path and attachment_path.startswith("temp_attachments"):
            try: os.remove(attachment_path)
            except OSError: pass 
        return "Email sent successfully."
    except Exception as e:
        return f"Failed to send email: {e}"

def _download_attachment_sync(attachment_url: str, attachment_filename: str) -> str:
    """Synchronous helper to download an attachment."""
    temp_dir = "temp_attachments" 
    os.makedirs(temp_dir, exist_ok=True)
    file_path = os.path.join(temp_dir, attachment_filename)
    
    with requests.get(attachment_url, stream=True) as r:
        r.raise_for_status()
        with open(file_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
    return file_path

def _get_pre_staged_attachment_sync(attachment_name: str) -> Optional[str]:
    """Synchronous helper to get a pre-staged attachment."""
    attachment_dir = "available_attachments" 
    file_path = os.path.join(attachment_dir, attachment_name)
    return file_path if os.path.exists(file_path) else None

def _fetch_emails_sync(folder: str, limit: int) -> str:
    """Synchronous helper to fetch emails."""
    if not SMTP_USERNAME or not SMTP_PASSWORD:
        return "Error: Email username or password not configured in environment."
    emails_data = []
    try:
        mail = imaplib.IMAP4_SSL(IMAP_HOST)
        mail.login(SMTP_USERNAME, SMTP_PASSWORD)
        status, messages = mail.select(folder)
        if status != 'OK':
            mail.logout()
            return f"Error selecting folder '{folder}': {messages}"

        result, data = mail.search(None, "ALL")
        if status != 'OK' or not data or not data[0]:
            mail.logout()
            return f"No emails found in folder '{folder}'."

        email_ids = data[0].split()
        
        ids_to_fetch = email_ids[-(limit):]

        for email_id_bytes in reversed(ids_to_fetch):
            status, msg_data = mail.fetch(email_id_bytes, "(RFC822)")
            if status == 'OK':
                for response_part in msg_data:
                    if isinstance(response_part, tuple):
                        msg = email.message_from_bytes(response_part[1])
                        subject, encoding = decode_header(msg["Subject"])[0]
                        if isinstance(subject, bytes):
                            subject = subject.decode(encoding or "utf-8")
                        from_ = msg.get("From", "")
                        date_ = msg.get("Date", "")
                        
                        snippet = ""
                        if msg.is_multipart():
                            for part in msg.walk():
                                ctype = part.get_content_type()
                                cdisp = str(part.get("Content-Disposition"))
                                if ctype == "text/plain" and "attachment" not in cdisp:
                                    try:
                                        body = part.get_payload(decode=True)
                                        snippet = body.decode(part.get_content_charset() or 'utf-8')
                                        snippet = " ".join(snippet.splitlines()) 
                                        snippet = snippet[:150] + "..." 
                                        break
                                    except Exception:
                                        snippet = "[Could not decode body]"
                                        break
                        else:
                            try:
                                body = msg.get_payload(decode=True)
                                snippet = body.decode(msg.get_content_charset() or 'utf-8')
                                snippet = " ".join(snippet.splitlines())
                                snippet = snippet[:150] + "..."
                            except Exception:
                                snippet = "[Could not decode body]"

                        emails_data.append({
                            "id": email_id_bytes.decode(),
                            "from": from_,
                            "subject": subject,
                            "date": date_,
                            "snippet": snippet
                        })
            if len(emails_data) >= limit: 
                 break

        mail.logout()

        if not emails_data:
            return f"No emails found in folder '{folder}'."

        
        result_text = f"Recent emails from {folder} (up to {limit}):\n\n"
        for i, email_data in enumerate(emails_data, 1):
            result_text += f"{i}. From: {email_data['from']}\n"
            result_text += f"   Subject: {email_data['subject']}\n"
            result_text += f"   Date: {email_data['date']}\n"
            result_text += f"   Snippet: {email_data['snippet']}\n\n"
            # result_text += f"   ID: {email_data['id']}\n\n" # ID might not be useful for LLM
        return result_text.strip()
    except Exception as e:
        return f"Failed to fetch emails: {e}"



@tool
async def send_email(recipient: str, subject: str, body: str,
                     attachment_path: Optional[str] = None,
                     attachment_url: Optional[str] = None,
                     attachment_name: Optional[str] = None) -> str:
    """
    Sends an email using configured Gmail account. Can handle attachments via local path, URL, or pre-staged name.

    Args:
        recipient: The email address to send the email to.
        subject: The email subject.
        body: The email body text.
        attachment_path: Optional direct file path for an attachment.
        attachment_url: Optional URL from which to download an attachment (requires attachment_name).
        attachment_name: Optional filename for the attachment (used with URL or pre-staged).

    Returns:
        Success or error message string.
    """
    final_attachment_path = attachment_path
    if attachment_url and attachment_name:
        try:
            # Run synchronous download in thread
            print(f"[Email Tool] Downloading attachment from {attachment_url}...")
            final_attachment_path = await anyio.to_thread.run_sync(
                _download_attachment_sync, attachment_url, attachment_name
            )
            print(f"[Email Tool] Attachment downloaded to {final_attachment_path}")
        except Exception as e:
            return f"Failed to download attachment from URL: {e}"
    elif attachment_name:
        try:
            
            print(f"[Email Tool] Checking for pre-staged attachment: {attachment_name}...")
            final_attachment_path = await anyio.to_thread.run_sync(
                _get_pre_staged_attachment_sync, attachment_name
            )
            if not final_attachment_path:
                return f"Error: Attachment '{attachment_name}' not found in pre-staged directory 'available_attachments'."
            print(f"[Email Tool] Using pre-staged attachment: {final_attachment_path}")
        except Exception as e:
             return f"Error accessing pre-staged attachment: {e}"

    
    print(f"[Email Tool] Sending email to {recipient}...")
    return await anyio.to_thread.run_sync(
        _send_email_sync, recipient, subject, body, final_attachment_path
    )

@tool
async def fetch_recent_emails(folder: str = "INBOX", limit: int = 5) -> str:
    """
    Fetches subject, sender, date, and a snippet of recent emails (up to limit) from a specified folder.

    Args:
        folder: The email folder to fetch from (default: "INBOX"). Common options: "INBOX", "Sent", "Drafts", "[Gmail]/Spam", "[Gmail]/Trash".
        limit: Maximum number of emails to fetch (default: 5).

    Returns:
        A formatted string containing details of the recent emails or an error message.
    """
    print(f"[Email Tool] Fetching up to {limit} emails from folder '{folder}'...")
    return await anyio.to_thread.run_sync(_fetch_emails_sync, folder, limit)



================================================
FILE: src/clap/tools/web_crawler.py
================================================
import asyncio
import json
import os
from dotenv import load_dotenv
from typing import Any 

from clap.tool_pattern.tool import tool 

_CRAWL4AI_AVAILABLE = False
_AsyncWebCrawler_Placeholder_Type = Any 

try:
    from crawl4ai import AsyncWebCrawler as ImportedAsyncWebCrawler
    _AsyncWebCrawler_Placeholder_Type = ImportedAsyncWebCrawler
    _CRAWL4AI_AVAILABLE = True
except ImportError:
    pass

load_dotenv() 

@tool
async def scrape_url(url: str) -> str:
    """
    Scrape a webpage and return its raw markdown content.

    Args:
        url: The URL of the webpage to scrape.

    Returns:
        The webpage content in markdown format or an error message.
    """
    if not _CRAWL4AI_AVAILABLE:
        raise ImportError("The 'crawl4ai' library is required for scrape_url. Install with 'pip install \"clap-agents[standard_tools]\"' or 'pip install crawl4ai'.")
    
    try:
        crawler: _AsyncWebCrawler_Placeholder_Type = _AsyncWebCrawler_Placeholder_Type() # type: ignore
        
        if not hasattr(crawler, 'arun') or not hasattr(crawler, 'close'): # Basic check
             raise RuntimeError("AsyncWebCrawler from crawl4ai is not correctly initialized (likely due to missing dependency).")

        async with crawler: # type: ignore
            result = await crawler.arun(url=url) # type: ignore
            return result.markdown.raw_markdown if result.markdown else "No content found"
    except Exception as e:
        return f"Error scraping URL '{url}': {str(e)}"

@tool
async def extract_text_by_query(url: str, query: str, context_size: int = 300) -> str:
    """
    Extract relevant text snippets containing a query from a webpage's markdown content.

    Args:
        url: The URL of the webpage to search.
        query: The search query (case-insensitive) to look for.
        context_size: Number of characters around the match to include.

    Returns:
        Relevant text snippets containing the query or a message indicating no matches/content.
    """
    if not _CRAWL4AI_AVAILABLE:
        raise ImportError("The 'crawl4ai' library is required for extract_text_by_query. Install with 'pip install \"clap-agents[standard_tools]\"' or 'pip install crawl4ai'.")
    
    try:
        markdown_content = await scrape_url(url=url) 

        if not markdown_content or markdown_content == "No content found" or markdown_content.startswith("Error"):
            return markdown_content

        lower_query = query.lower()
        lower_content = markdown_content.lower()
        matches = []
        start_index = 0
        while len(matches) < 5: # Limit matches
            pos = lower_content.find(lower_query, start_index)
            if pos == -1: break
            start = max(0, pos - context_size)
            end = min(len(markdown_content), pos + len(lower_query) + context_size)
            context_snippet = markdown_content[start:end]
            prefix = "..." if start > 0 else ""
            suffix = "..." if end < len(markdown_content) else ""
            matches.append(f"{prefix}{context_snippet}{suffix}")
            start_index = pos + len(lower_query)
        if matches:
            result_text = "\n\n---\n\n".join([f"Match {i+1}:\n{match}" for i, match in enumerate(matches)])
            return f"Found {len(matches)} matches for '{query}' on the page:\n\n{result_text}"
        else:
            return f"No matches found for '{query}' on the page."

    except Exception as e:
        return f"Error processing content from '{url}' for query '{query}': {str(e)}"



================================================
FILE: src/clap/tools/web_search.py
================================================
import os
from duckduckgo_search import DDGS
from ..tool_pattern.tool import tool



@tool
def duckduckgo_search(query: str, num_results: int = 5) -> str:
    """Performs a web search using the DuckDuckGo Search API."""
    try:
        with DDGS() as ddgs:
            results = ddgs.text(keywords=query, max_results=num_results)
            if results:
                results_str = f"DuckDuckGo search results for '{query}':\n"
                for i, result in enumerate(results):
                    title = result.get('title', 'No Title')
                    snippet = result.get('body', 'No Snippet')
                    url = result.get('href', 'No URL')
                    results_str += f"{i+1}. {title}\n   URL: {url}\n   Snippet: {snippet}\n\n"
                return results_str.strip()
            else:
                return f"No DuckDuckGo results found for '{query}'."
    except Exception as e:
        return f"Error during DuckDuckGo web search for '{query}': {e}"


================================================
FILE: src/clap/utils/__init__.py
================================================



================================================
FILE: src/clap/utils/completions.py
================================================


import asyncio 
from typing import Optional, List, Dict, Any 
# from groq import Groq 
# from groq.types.chat.chat_completion import ChatCompletion 
# from groq.types.chat.chat_completion_message import ChatCompletionMessage 
from groq import AsyncGroq


GroqClient = Any
ChatCompletionMessage = Any


async def completions_create(
    client: AsyncGroq,
    messages: List[Dict[str, Any]], 
    model: str,
    tools: Optional[List[Dict[str, Any]]] = None, 
    tool_choice: str = "auto" 
) -> ChatCompletionMessage: 
    """
    Sends an asynchronous request to the client's completions endpoint, supporting tool use.

    Args:
        client: The API client object (e.g., Groq) supporting async operations.
        messages: A list of message objects for the chat history.
        model: The model to use.
        tools: A list of tool schemas the model can use.
        tool_choice: Controls how the model uses tools.

    Returns:
        The message object from the API response, which might contain content or tool calls.
    """
    try:
        
        api_kwargs = {
            "messages": messages,
            "model": model,
        }
        if tools:
            api_kwargs["tools"] = tools
            api_kwargs["tool_choice"] = tool_choice

        
        response = await client.chat.completions.create(**api_kwargs)
        
        return response.choices[0].message
    except Exception as e:
        
        print(f"Error calling LLM API asynchronously: {e}")
        
        
        class ErrorMessage:
             content = f"Error communicating with LLM: {e}"
             tool_calls = None
             role = "assistant"
        return ErrorMessage()


def build_prompt_structure(
    role: str,
    content: Optional[str] = None, 
    tag: str = "",
    tool_calls: Optional[List[Dict[str, Any]]] = None,
    tool_call_id: Optional[str] = None 
) -> dict:
    """
    Builds a structured message dictionary for the chat API.

    Args:
        role: The role ('system', 'user', 'assistant', 'tool').
        content: The text content of the message (required for user, system, tool roles).
        tag: An optional tag to wrap the content (legacy, consider removing).
        tool_calls: A list of tool calls requested by the assistant.
        tool_call_id: The ID of the tool call this message is a response to (for role 'tool').

    Returns:
        A dictionary representing the structured message.
    """
    message: Dict[str, Any] = {"role": role}
    if content is not None:
        if tag: # Apply legacy tag if provided
             content = f"<{tag}>{content}</{tag}>"
        message["content"] = content

    
    if role == "assistant" and tool_calls:
        message["tool_calls"] = tool_calls

   
    if role == "tool" and tool_call_id:
        message["tool_call_id"] = tool_call_id
        if content is None: 
             raise ValueError("Content is required for role 'tool'.")

    
    if role == "tool" and not tool_call_id:
         raise ValueError("tool_call_id is required for role 'tool'.")
    if role != "assistant" and tool_calls:
         raise ValueError("tool_calls can only be added to 'assistant' role messages.")

    return message


def update_chat_history(
    history: list,
    message: ChatCompletionMessage | Dict[str, Any] 
    ):
    """
    Updates the chat history by appending a message object or a manually created message dict.

    Args:
        history (list): The list representing the current chat history.
        message: The message object from the API response or a dict created by build_prompt_structure.
    """
    
    if hasattr(message, "role"): # Basic check if it looks like an API message object
        msg_dict = {"role": message.role}
        if hasattr(message, "content") and message.content is not None:
            msg_dict["content"] = message.content
        if hasattr(message, "tool_calls") and message.tool_calls:
             
            msg_dict["tool_calls"] = message.tool_calls
        
        history.append(msg_dict)
    elif isinstance(message, dict) and "role" in message:
        
        history.append(message)
    else:
        raise TypeError("Invalid message type provided to update_chat_history.")


class ChatHistory(list):
    def __init__(self, messages: Optional[List[Dict[str, Any]]] = None, total_length: int = -1): 
        if messages is None:
            messages = []
        super().__init__(messages)
        self.total_length = total_length 

    def append(self, msg: Dict[str, Any]): 
        if not isinstance(msg, dict) or "role" not in msg:
            raise TypeError("ChatHistory can only append message dictionaries with a 'role'.")

        
        if self.total_length > 0 and len(self) == self.total_length:
            self.pop(0) 
        super().append(msg)


class FixedFirstChatHistory(ChatHistory):
    def __init__(self, messages: Optional[List[Dict[str, Any]]] = None, total_length: int = -1):
        super().__init__(messages, total_length)

    def append(self, msg: Dict[str, Any]):
        if not isinstance(msg, dict) or "role" not in msg:
            raise TypeError("ChatHistory can only append message dictionaries with a 'role'.")

        
        if self.total_length > 0 and len(self) == self.total_length:
            if len(self) > 1:
                 self.pop(1) 
            else:
                 
                 print("Warning: Cannot append to FixedFirstChatHistory of size 1.")
                 return
        
        if self.total_length <= 0 or len(self) < self.total_length:
             super().append(msg)





================================================
FILE: src/clap/utils/extraction.py
================================================
import re
from dataclasses import dataclass


@dataclass
class TagContentResult:
    """
    A data class to represent the result of extracting tag content.

    Attributes:
        content (List[str]): A list of strings containing the content found between the specified tags.
        found (bool): A flag indicating whether any content was found for the given tag.
    """

    content: list[str]
    found: bool


def extract_tag_content(text: str, tag: str) -> TagContentResult:
    """
    Extracts all content enclosed by specified tags (e.g., <thought>, <response>, etc.).

    Parameters:
        text (str): The input string containing multiple potential tags.
        tag (str): The name of the tag to search for (e.g., 'thought', 'response').

    Returns:
        dict: A dictionary with the following keys:
            - 'content' (list): A list of strings containing the content found between the specified tags.
            - 'found' (bool): A flag indicating whether any content was found for the given tag.
    """
    
    tag_pattern = rf"<{tag}>(.*?)</{tag}>"

    
    matched_contents = re.findall(tag_pattern, text, re.DOTALL)

    
    return TagContentResult(
        content=[content.strip() for content in matched_contents],
        found=bool(matched_contents),
    )


================================================
FILE: src/clap/utils/logging.py
================================================
import time

from colorama import Fore
from colorama import Style


def fancy_print(message: str) -> None:
    """
    Displays a fancy print message.

    Args:
        message (str): The message to display.
    """
    print(Style.BRIGHT + Fore.CYAN + f"\n{'=' * 50}")
    print(Fore.MAGENTA + f"{message}")
    print(Style.BRIGHT + Fore.CYAN + f"{'=' * 50}\n")
    time.sleep(0.5)


def fancy_step_tracker(step: int, total_steps: int) -> None:
    """
    Displays a fancy step tracker for each iteration of the generation-reflection loop.

    Args:
        step (int): The current step in the loop.
        total_steps (int): The total number of steps in the loop.
    """
    fancy_print(f"STEP {step + 1}/{total_steps}")


================================================
FILE: src/clap/utils/rag_utils.py
================================================

import csv 
from typing import List, Dict, Any, Tuple, Optional, Union


try:
    import pypdf
except ImportError:
    raise ImportError(
        "pypdf not found. Please install it for PDF loading: pip install pypdf"
    )



def load_text_file(file_path: str) -> str:
    """Loads text content from a file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"Error loading text file {file_path}: {e}")
        return ""

def load_pdf_file(file_path: str) -> str:
    """Loads text content from a PDF file."""
    text = ""
    try:
        with open(file_path, 'rb') as f:
            reader = pypdf.PdfReader(f)
            print(f"Loading PDF '{file_path}' with {len(reader.pages)} pages...")
            for i, page in enumerate(reader.pages):
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n" 
                else:
                    print(f"Warning: No text extracted from page {i+1} of {file_path}")
        print(f"Finished loading PDF '{file_path}'.")
        return text.strip()
    except FileNotFoundError:
        print(f"Error: PDF file not found at {file_path}")
        return ""
    except Exception as e:
        print(f"Error loading PDF file {file_path}: {e}")
        return ""


def load_csv_file(
    file_path: str,
    content_column: Union[str, int],
    metadata_columns: Optional[List[Union[str, int]]] = None,
    delimiter: str = ',',
    encoding: str = 'utf-8'
) -> List[Tuple[str, Dict[str, Any]]]:
    """
    Loads data from a CSV file, extracting content and metadata.

    Each row is treated as a potential document/chunk.

    Args:
        file_path: Path to the CSV file.
        content_column: The name (string) or index (int) of the column containing the main text content.
        metadata_columns: Optional list of column names (string) or indices (int)
                          to include as metadata for each row.
        delimiter: CSV delimiter (default ',').
        encoding: File encoding (default 'utf-8').

    Returns:
        A list of tuples, where each tuple contains:
        (document_text: str, metadata: dict)
    """
    data = []
    metadata_columns = metadata_columns or []
    try:
        with open(file_path, mode='r', encoding=encoding, newline='') as f:
            
            has_header = isinstance(content_column, str) or any(isinstance(mc, str) for mc in metadata_columns)

            if has_header:
                reader = csv.DictReader(f, delimiter=delimiter)
                headers = reader.fieldnames
                if headers is None:
                     print(f"Error: Could not read headers from CSV {file_path}")
                     return []

                
                if isinstance(content_column, str) and content_column not in headers:
                    raise ValueError(f"Content column '{content_column}' not found in CSV headers: {headers}")
                for mc in metadata_columns:
                    if isinstance(mc, str) and mc not in headers:
                        raise ValueError(f"Metadata column '{mc}' not found in CSV headers: {headers}")

                content_key = content_column
                meta_keys = [mc for mc in metadata_columns if isinstance(mc, str)]

            else:
                
                reader = csv.reader(f, delimiter=delimiter)
                
                content_key = int(content_column)
                meta_keys = [int(mc) for mc in metadata_columns]


            print(f"Loading CSV '{file_path}'...")
            for i, row in enumerate(reader):
                try:
                    if has_header:
                        
                        doc_text = row.get(content_key, "").strip()
                        metadata = {key: row.get(key, "") for key in meta_keys}
                    else:
                        
                        if content_key >= len(row): continue 
                        doc_text = row[content_key].strip()
                        metadata = {}
                        for key_index in meta_keys:
                            if key_index < len(row):
                               
                                metadata[f"column_{key_index}"] = row[key_index]


                    if doc_text: 
                        metadata["source_row"] = i + (1 if has_header else 0)
                        data.append((doc_text, metadata))
                except IndexError:
                     print(f"Warning: Skipping row {i} due to index out of bounds (check column indices).")
                except Exception as row_e:
                     print(f"Warning: Skipping row {i} due to error: {row_e}")


        print(f"Finished loading CSV '{file_path}', processed {len(data)} rows with content.")
        return data

    except FileNotFoundError:
        print(f"Error: CSV file not found at {file_path}")
        return []
    except ValueError as ve: 
        print(f"Error processing CSV header/indices for {file_path}: {ve}")
        return []
    except Exception as e:
        print(f"Error loading CSV file {file_path}: {e}")
        return []




def chunk_text_by_fixed_size(
    text: str, chunk_size: int, chunk_overlap: int = 0
) -> List[str]:
    """Chunks text into fixed size blocks with optional overlap."""
    if not isinstance(text, str):
        print(f"Warning: chunk_text_by_fixed_size expected string, got {type(text)}. Skipping.")
        return []
    if chunk_overlap >= chunk_size:
        
        raise ValueError("chunk_overlap must be smaller than chunk_size")
    if chunk_size <= 0:
         raise ValueError("chunk_size must be positive")

    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        
        step = chunk_size - chunk_overlap
        if step <= 0:
             
             step = 1 

        start += step

    return [chunk for chunk in chunks if chunk.strip()] 


def chunk_text_by_separator(text: str, separator: str = "\n\n") -> List[str]:
    """Chunks text based on a specified separator."""
    if not isinstance(text, str):
        print(f"Warning: chunk_text_by_separator expected string, got {type(text)}. Skipping.")
        return []
    chunks = text.split(separator)
    return [chunk for chunk in chunks if chunk.strip()] 





================================================
FILE: src/clap/vector_stores/__init__.py
================================================
from .base import VectorStoreInterface, QueryResult, Document, Embedding, ID, Metadata

__all__ = ["VectorStoreInterface", "QueryResult", "Document", "Embedding", "ID", "Metadata"]

try:
    from .chroma_store import ChromaStore
    __all__.append("ChromaStore")
except ImportError:
    pass

try:
    from .qdrant_store import QdrantStore
    __all__.append("QdrantStore")
except ImportError:
    pass




================================================
FILE: src/clap/vector_stores/base.py
================================================

import abc
from typing import Any, Dict, List, Optional, TypedDict, Union

Document = str
Embedding = List[float]
ID = str
Metadata = Dict[str, Any]

class QueryResult(TypedDict):
    ids: List[List[ID]]
    embeddings: Optional[List[List[Embedding]]]
    documents: Optional[List[List[Document]]]
    metadatas: Optional[List[List[Metadata]]]
    distances: Optional[List[List[float]]]

class VectorStoreInterface(abc.ABC):
    """Abstract Base Class for Vector Store interactions."""

    @abc.abstractmethod
    async def add_documents(
        self,
        documents: List[Document],
        ids: List[ID],
        metadatas: Optional[List[Metadata]] = None,
        embeddings: Optional[List[Embedding]] = None,
    ) -> None:
        """
        Add documents and their embeddings to the store.
        If embeddings are not provided, the implementation should handle embedding generation.

        Args:
            documents: List of document texts.
            ids: List of unique IDs for each document.
            metadatas: Optional list of metadata dictionaries for each document.
            embeddings: Optional list of pre-computed embeddings.
        """
        pass

    @abc.abstractmethod
    async def aquery(
        self,
        query_texts: Optional[List[Document]] = None,
        query_embeddings: Optional[List[Embedding]] = None,
        n_results: int = 5,
        where: Optional[Dict[str, Any]] = None,
        where_document: Optional[Dict[str, Any]] = None,
        include: List[str] = ["metadatas", "documents", "distances"],
    ) -> QueryResult:
        """
        Query the vector store for similar documents.
        Provide either query_texts or query_embeddings.

        Args:
            query_texts: List of query texts. Embeddings will be generated.
            query_embeddings: List of query embeddings.
            n_results: Number of results to return for each query.
            where: Optional metadata filter (syntax depends on implementation).
            where_document: Optional document content filter (syntax depends on implementation).
            include: List of fields to include in the results (e.g., "documents", "metadatas", "distances", "embeddings").

        Returns:
            A QueryResult dictionary containing the search results.
        """
        pass

    @abc.abstractmethod
    async def adelete(
        self,
        ids: Optional[List[ID]] = None,
        where: Optional[Dict[str, Any]] = None,
        where_document: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        Delete documents from the store by ID or filter.

        Args:
            ids: Optional list of IDs to delete.
            where: Optional metadata filter for deletion.
            where_document: Optional document content filter for deletion.
        """
        pass

   




================================================
FILE: src/clap/vector_stores/chroma_store.py
================================================
import json
import functools
from typing import Any, Dict, List, Optional, cast, Callable, Coroutine
import asyncio 
import anyio

_CHROMADB_LIB_AVAILABLE = False
_ChromaDB_Client_Placeholder_Type = Any
_ChromaDB_Collection_Placeholder_Type = Any
_ChromaDB_Settings_Placeholder_Type = Any
_ChromaDB_EmbeddingFunction_Placeholder_Type = Any
_ChromaDB_DefaultEmbeddingFunction_Placeholder_Type = Any

try:
    import chromadb
    from chromadb import Collection as ImportedChromaCollection
    from chromadb.config import Settings as ImportedChromaSettings
    from chromadb.utils.embedding_functions import (
        EmbeddingFunction as ImportedChromaEF, 
        DefaultEmbeddingFunction as ImportedChromaDefaultEF
    )
    from chromadb.api.types import Documents, Embeddings 
    _CHROMADB_LIB_AVAILABLE = True
    _ChromaDB_Client_Placeholder_Type = chromadb.Client
    _ChromaDB_Collection_Placeholder_Type = ImportedChromaCollection
    _ChromaDB_Settings_Placeholder_Type = ImportedChromaSettings
    _ChromaDB_EmbeddingFunction_Placeholder_Type = ImportedChromaEF
    _ChromaDB_DefaultEmbeddingFunction_Placeholder_Type = ImportedChromaDefaultEF
except ImportError:
    class Documents: pass 
    class Embeddings: pass 
    pass


from .base import ( Document as ClapDocument, Embedding as ClapEmbedding, ID, Metadata, QueryResult, VectorStoreInterface,) # Aliased Document
from clap.embedding.base_embedding import EmbeddingFunctionInterface as ClapEFInterface


class _AsyncEFWrapperForChroma:
    """Wraps an async CLAP EmbeddingFunctionInterface to be callable synchronously by Chroma."""
    def __init__(self, async_ef_call: Callable[..., Coroutine[Any, Any, Embeddings]], loop: asyncio.AbstractEventLoop):
        self._async_ef_call = async_ef_call
        self._loop = loop

    def __call__(self, input: Documents) -> Embeddings: 
        
        if self._loop.is_running():
            
            future = asyncio.run_coroutine_threadsafe(self._async_ef_call(input), self._loop)
            return future.result() 
        else:
            return self._loop.run_until_complete(self._async_ef_call(input))


class ChromaStore(VectorStoreInterface):
    _client: _ChromaDB_Client_Placeholder_Type
    _collection: _ChromaDB_Collection_Placeholder_Type
    _clap_ef_is_async: bool = False 

    def __init__(
        self,
        collection_name: str,
        embedding_function: Optional[ClapEFInterface] = None, # CLAP's interface
        path: Optional[str] = None, host: Optional[str] = None, port: Optional[int] = None,
        client_settings: Optional[_ChromaDB_Settings_Placeholder_Type] = None,
    ):
        if not _CHROMADB_LIB_AVAILABLE:
            raise ImportError("The 'chromadb' library is required to use ChromaStore.")

        self.collection_name = collection_name
        
        if path: self._client = chromadb.PersistentClient(path=path, settings=client_settings)
        elif host and port: self._client = chromadb.HttpClient(host=host, port=port, settings=client_settings)
        else: self._client = chromadb.EphemeralClient(settings=client_settings)
       

        chroma_ef_to_pass_to_chroma: Optional[_ChromaDB_EmbeddingFunction_Placeholder_Type]

        if embedding_function is None:
            chroma_ef_to_pass_to_chroma = _ChromaDB_DefaultEmbeddingFunction_Placeholder_Type()
            self._clap_ef_is_async = False 
            print(f"ChromaStore: Using ChromaDB's DefaultEmbeddingFunction for '{self.collection_name}'.")
        
        elif isinstance(embedding_function, _ChromaDB_DefaultEmbeddingFunction_Placeholder_Type): 
            chroma_ef_to_pass_to_chroma = embedding_function
            self._clap_ef_is_async = False 
            print(f"ChromaStore: Using provided native Chroma EmbeddingFunction for '{self.collection_name}'.")
        else:
            
            ef_call_method = getattr(embedding_function, "__call__", None)
            if ef_call_method and asyncio.iscoroutinefunction(ef_call_method):
                self._clap_ef_is_async = True
                print(f"ChromaStore: Wrapping async CLAP EmbeddingFunction for Chroma compatibility for '{self.collection_name}'.")
                
                try:
                    loop = asyncio.get_running_loop()
                except RuntimeError: 
                    
                    print("ChromaStore WARNING: No running asyncio loop to wrap async EF for Chroma. This might fail.")
                    
                    loop = asyncio.new_event_loop() 
                                                  

                chroma_ef_to_pass_to_chroma = _AsyncEFWrapperForChroma(ef_call_method, loop) 
            else: 
                self._clap_ef_is_async = False
                print(f"ChromaStore: Using synchronous CLAP EmbeddingFunction for '{self.collection_name}'.")
                chroma_ef_to_pass_to_chroma = embedding_function # type: ignore

        self._collection = self._client.get_or_create_collection(
            name=self.collection_name,
            embedding_function=chroma_ef_to_pass_to_chroma
        )
        # print(f"ChromaStore: Collection '{self.collection_name}' ready.")

    
    async def _run_sync(self, func, *args, **kwargs):
        bound_func = functools.partial(func, *args, **kwargs)
        return await anyio.to_thread.run_sync(bound_func)

    async def add_documents(self, documents: List[ClapDocument], ids: List[ID], metadatas: Optional[List[Metadata]] = None, embeddings: Optional[List[ClapEmbedding]] = None) -> None:
        
        # print(f"ChromaStore: Adding {len(ids)} documents to '{self.collection_name}'...")
        await self._run_sync(self._collection.add, ids=ids, embeddings=embeddings, metadatas=metadatas, documents=documents)
        # print(f"ChromaStore: Add/Update completed for {len(ids)} documents.")

    async def aquery(self, query_texts: Optional[List[ClapDocument]] = None, query_embeddings: Optional[List[ClapEmbedding]] = None, n_results: int = 5, where: Optional[Dict[str, Any]] = None, where_document: Optional[Dict[str, Any]] = None, include: List[str] = ["metadatas", "documents", "distances"]) -> QueryResult:
        if not query_texts and not query_embeddings: raise ValueError("Requires query_texts or query_embeddings.")
        if query_texts and query_embeddings: query_texts = None
        
       
        # print(f"ChromaStore: Querying collection '{self.collection_name}'...")
        results = await self._run_sync(self._collection.query, query_embeddings=query_embeddings, query_texts=query_texts, n_results=n_results, where=where, where_document=where_document, include=include)
        
        num_queries = len(query_texts or query_embeddings or [])
       
        return QueryResult(
            ids=results.get("ids") or ([[]] * num_queries), embeddings=results.get("embeddings"),
            documents=results.get("documents"), metadatas=results.get("metadatas"), distances=results.get("distances") )

    async def adelete(self, ids: Optional[List[ID]] = None, where: Optional[Dict[str, Any]] = None, where_document: Optional[Dict[str, Any]] = None) -> None:
        await self._run_sync(self._collection.delete, ids=ids, where=where, where_document=where_document)



================================================
FILE: src/clap/vector_stores/qdrant_store.py
================================================
import asyncio
import json
import functools
import os
from typing import Any, Dict, List, Optional, cast, Type

import anyio

_QDRANT_LIB_AVAILABLE = False
_AsyncQdrantClient_Placeholder_Type = Any
_QdrantClient_Placeholder_Type = Any 
_qdrant_models_Placeholder_Type = Any
_QdrantUnexpectedResponse_Placeholder_Type = type(Exception)

try:
    from qdrant_client import AsyncQdrantClient as ImportedAsyncQC, QdrantClient as ImportedSyncQC, models as ImportedModels
    from qdrant_client.http.exceptions import UnexpectedResponse as ImportedQdrantUnexpectedResponse
    _QDRANT_LIB_AVAILABLE = True
    _AsyncQdrantClient_Placeholder_Type = ImportedAsyncQC
    _QdrantClient_Placeholder_Type = ImportedSyncQC 
    _qdrant_models_Placeholder_Type = ImportedModels
    _QdrantUnexpectedResponse_Placeholder_Type = ImportedQdrantUnexpectedResponse
except ImportError:
    pass


from .base import ( Document, Embedding, ID, Metadata, QueryResult, VectorStoreInterface,)
from clap.embedding.base_embedding import EmbeddingFunctionInterface


class QdrantStore(VectorStoreInterface):
    _async_client: _AsyncQdrantClient_Placeholder_Type
    models: Any # For qdrant_client.models

    def __init__(self):
        if not hasattr(self, "_initialized_via_factory"):
             raise RuntimeError("Use QdrantStore.create(...) async factory method.")

    @classmethod
    async def create(
        cls: Type['QdrantStore'],
        collection_name: str, embedding_function: EmbeddingFunctionInterface,
        path: Optional[str] = None, distance_metric: Any = None, 
        recreate_collection_if_exists: bool = False, **qdrant_client_kwargs: Any
    ) -> 'QdrantStore':
        if not _QDRANT_LIB_AVAILABLE:
            raise ImportError("The 'qdrant-client' library is required. Install with 'pip install \"clap-agents[qdrant]\"'")
        if not embedding_function: raise ValueError("embedding_function is required.")

        instance = cls.__new__(cls)
        instance._initialized_via_factory = True
        instance.collection_name = collection_name
        instance.models = _qdrant_models_Placeholder_Type 
        instance._embedding_function = embedding_function
        instance.distance_metric = distance_metric if distance_metric else instance.models.Distance.COSINE
        instance.vector_size = instance._embedding_function.get_embedding_dimension()

        client_location_for_log = path if path else ":memory:"
        # print(f"QdrantStore (Async): Initializing client for '{instance.collection_name}' at '{client_location_for_log}'.")
        if path: instance._async_client = _AsyncQdrantClient_Placeholder_Type(path=path, **qdrant_client_kwargs)
        else: instance._async_client = _AsyncQdrantClient_Placeholder_Type(location=":memory:", **qdrant_client_kwargs)
        
        await instance._setup_collection_async(recreate_collection_if_exists)
        return instance

    async def _setup_collection_async(self, recreate_if_exists: bool):
        try:
            if recreate_if_exists:
                await self._async_client.delete_collection(collection_name=self.collection_name)
                await self._async_client.create_collection(
                     collection_name=self.collection_name,
                     vectors_config=self.models.VectorParams(size=self.vector_size, distance=self.distance_metric))
                return
            try:
                info = await self._async_client.get_collection(collection_name=self.collection_name)
                if info.config.params.size != self.vector_size or info.config.params.distance.lower() != self.distance_metric.lower(): # type: ignore
                    raise ValueError("Existing Qdrant collection has incompatible config.")
            except (_QdrantUnexpectedResponse_Placeholder_Type, ValueError) as e: # type: ignore
                 if isinstance(e, _QdrantUnexpectedResponse_Placeholder_Type) and e.status_code == 404 or "not found" in str(e).lower(): # type: ignore
                      await self._async_client.create_collection(
                          collection_name=self.collection_name,
                          vectors_config=self.models.VectorParams(size=self.vector_size, distance=self.distance_metric))
                 else: raise
        except Exception as e: print(f"QdrantStore: Error during collection setup: {e}"); raise


    async def _embed_texts_via_interface(self, texts: List[Document]) -> List[Embedding]:
        if not self._embedding_function: raise RuntimeError("EF missing.")
        ef_call = self._embedding_function.__call__
        if asyncio.iscoroutinefunction(ef_call): return await ef_call(texts) 
        return await anyio.to_thread.run_sync(functools.partial(ef_call, texts)) 

    async def add_documents(self, documents: List[Document], ids: List[ID], metadatas: Optional[List[Metadata]] = None, embeddings: Optional[List[Embedding]] = None) -> None:
        if not documents and not embeddings: raise ValueError("Requires docs or embeddings.")
        num_items = len(documents) if documents else (len(embeddings) if embeddings else 0)
        if num_items == 0: return
        if len(ids) != num_items: raise ValueError("'ids' length mismatch.")
        if metadatas and len(metadatas) != num_items: raise ValueError("'metadatas' length mismatch.")
        if not embeddings and documents: embeddings = await self._embed_texts_via_interface(documents)
        if not embeddings: return
        points: List[Any] = [] # Use Any for models.PointStruct if models is Any
        for i, item_id in enumerate(ids):
            payload = metadatas[i].copy() if metadatas and i < len(metadatas) else {}
            if documents and i < len(documents): payload["_clap_document_content_"] = documents[i]
            points.append(self.models.PointStruct(id=str(item_id), vector=embeddings[i], payload=payload))
        if points: await self._async_client.upsert(collection_name=self.collection_name, points=points, wait=True)

    async def aquery(self, query_texts: Optional[List[Document]] = None, query_embeddings: Optional[List[Embedding]] = None, n_results: int = 5, where: Optional[Dict[str, Any]] = None, include: List[str] = ["metadatas", "documents", "distances"], **kwargs) -> QueryResult:
        if not query_texts and not query_embeddings: raise ValueError("Requires query_texts or query_embeddings.")
        if query_texts and query_embeddings: query_texts = None
        q_filter = self._translate_clap_filter(where) 
        if query_texts: q_vectors = await self._embed_texts_via_interface(query_texts)
        elif query_embeddings: q_vectors = query_embeddings
        else: return QueryResult(ids=[[]], embeddings=None, documents=None, metadatas=None, distances=None)
        raw_results: List[List[Any]] = [] 
        for qv in q_vectors:
            hits = await self._async_client.search(collection_name=self.collection_name, query_vector=qv, query_filter=q_filter, limit=n_results, with_payload=True, with_vectors="embeddings" in include, **kwargs)
            raw_results.append(hits)
        return self._format_qdrant_results(raw_results, include) # Keep using self.models

    def _translate_clap_filter(self, clap_where_filter: Optional[Dict[str, Any]]) -> Optional[Any]: # Return Any
        if not clap_where_filter or not _QDRANT_LIB_AVAILABLE: return None
        must = []
        for k, v in clap_where_filter.items():
            if isinstance(v, dict) and "$eq" in v: must.append(self.models.FieldCondition(key=k, match=self.models.MatchValue(value=v["$eq"])))
            elif isinstance(v, (str,int,float,bool)): must.append(self.models.FieldCondition(key=k, match=self.models.MatchValue(value=v)))
        return self.models.Filter(must=must) if must else None

    def _format_qdrant_results(self, raw_results: List[List[Any]], include: List[str]) -> QueryResult: 
        ids, embs, docs, metas, dists_final = [], [], [], [], []
        for hits_list in raw_results:
            current_ids, current_embs, current_docs, current_metas, current_dists = [],[],[],[],[]
            for hit in hits_list: 
                current_ids.append(str(hit.id))
                payload = hit.payload if hit.payload else {}
                if "distances" in include and hit.score is not None: current_dists.append(hit.score)
                if "embeddings" in include and hit.vector : current_embs.append(cast(List[float], hit.vector))
                if "documents" in include : current_docs.append(payload.get("_clap_document_content_", ""))
                if "metadatas" in include : current_metas.append({k:v for k,v in payload.items() if k != "_clap_document_content_"})
            ids.append(current_ids); embs.append(current_embs); docs.append(current_docs); metas.append(current_metas); dists_final.append(current_dists)
        return QueryResult(ids=ids, embeddings=embs if "embeddings" in include else None, documents=docs if "documents" in include else None, metadatas=metas if "metadatas" in include else None, distances=dists_final if "distances" in include else None)


    async def adelete(self, ids: Optional[List[ID]] = None, where: Optional[Dict[str, Any]] = None, **kwargs ) -> None:
        if not ids and not where: return
        q_filter = self._translate_clap_filter(where)
        selector: Any = None
        if ids and q_filter: selector = self.models.FilterSelector(filter=self.models.Filter(must=[q_filter, self.models.HasIdCondition(has_id=[str(i) for i in ids])]))
        elif ids: selector = self.models.PointIdsList(points=[str(i) for i in ids])
        elif q_filter: selector = self.models.FilterSelector(filter=q_filter)
        if selector: await self._async_client.delete(collection_name=self.collection_name, points_selector=selector, wait=True)

    async def close(self):
        if hasattr(self, '_async_client') and self._async_client:
            await self._async_client.close(timeout=5)


